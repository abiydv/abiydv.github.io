<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>K8s Kubernetes | @abiydv</title>
<meta name=keywords content><meta name=description content="Architecture graph LR; c1(Cluster) --> n1(node1) --> p11[pod 1] n1 --> p12[pod 2] c1 --> n2(node2) --> p21[pod 1] API docs - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/
Insert diagram about control nodes + worker nodes + api servers etc.
#refine https://kubernetes.io/docs/concepts/architecture/
Components Metric Server Use HostNetwork
Helps monitor pods Can run kubectl top pod to check resource usage for pods kubectl top nodes
CNCF Project status graph LR; a(Sandbox
New) --> b(Incubating
More wide-spread adoption, active development) --> c[Graduated"><meta name=author content><link rel=canonical href=https://abiydv.github.io/notes/k8s-main/><link crossorigin=anonymous href=/assets/css/stylesheet.c5edd088d0c984c192170cfbab7d2de030010ee0b7d771bd6bca049dd2332874.css integrity="sha256-xe3QiNDJhMGSFwz7q30t4DABDuC313G9a8oEndIzKHQ=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://abiydv.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://abiydv.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://abiydv.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://abiydv.github.io/apple-touch-icon.png><link rel=mask-icon href=https://abiydv.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://abiydv.github.io/notes/k8s-main/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="K8s Kubernetes"><meta property="og:description" content="Architecture graph LR; c1(Cluster) --> n1(node1) --> p11[pod 1] n1 --> p12[pod 2] c1 --> n2(node2) --> p21[pod 1] API docs - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/
Insert diagram about control nodes + worker nodes + api servers etc.
#refine https://kubernetes.io/docs/concepts/architecture/
Components Metric Server Use HostNetwork
Helps monitor pods Can run kubectl top pod to check resource usage for pods kubectl top nodes
CNCF Project status graph LR; a(Sandbox
New) --> b(Incubating
More wide-spread adoption, active development) --> c[Graduated"><meta property="og:type" content="article"><meta property="og:url" content="https://abiydv.github.io/notes/k8s-main/"><meta property="article:section" content="notes"><meta property="article:published_time" content="2024-04-15T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-04T12:48:24+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="K8s Kubernetes"><meta name=twitter:description content="Architecture graph LR; c1(Cluster) --> n1(node1) --> p11[pod 1] n1 --> p12[pod 2] c1 --> n2(node2) --> p21[pod 1] API docs - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/
Insert diagram about control nodes + worker nodes + api servers etc.
#refine https://kubernetes.io/docs/concepts/architecture/
Components Metric Server Use HostNetwork
Helps monitor pods Can run kubectl top pod to check resource usage for pods kubectl top nodes
CNCF Project status graph LR; a(Sandbox
New) --> b(Incubating
More wide-spread adoption, active development) --> c[Graduated"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://abiydv.github.io/notes/"},{"@type":"ListItem","position":2,"name":"K8s Kubernetes","item":"https://abiydv.github.io/notes/k8s-main/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"K8s Kubernetes","name":"K8s Kubernetes","description":"Architecture graph LR; c1(Cluster) --\u003e n1(node1) --\u003e p11[pod 1] n1 --\u003e p12[pod 2] c1 --\u003e n2(node2) --\u003e p21[pod 1] API docs - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/ Insert diagram about control nodes + worker nodes + api servers etc. #refine https://kubernetes.io/docs/concepts/architecture/ Components Metric Server Use HostNetwork Helps monitor pods Can run kubectl top pod to check resource usage for pods kubectl top nodes CNCF Project status graph LR; a(Sandbox New) --\u003e b(Incubating More wide-spread adoption, active development) --\u003e c[Graduated","keywords":[],"articleBody":"Architecture graph LR; c1(Cluster) --\u003e n1(node1) --\u003e p11[pod 1] n1 --\u003e p12[pod 2] c1 --\u003e n2(node2) --\u003e p21[pod 1] API docs - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/\nInsert diagram about control nodes + worker nodes + api servers etc.\n#refine https://kubernetes.io/docs/concepts/architecture/\nComponents Metric Server Use HostNetwork\nHelps monitor pods Can run kubectl top pod to check resource usage for pods kubectl top nodes\nCNCF Project status graph LR; a(Sandbox\nNew) --\u003e b(Incubating\nMore wide-spread adoption, active development) --\u003e c[Graduated\nmature, stable part of k8s core] https://www.cncf.io/projects\nKubernetes Plugins? CRIO Kubernetes container runtime #readmore\nCNI Common network interface\nJaeger Kubernetes Operator, manages packaging, deploying and managing applications\nRook Storage Orchestrator\nCluster Autoscaler https://github.com/kubernetes/autoscaler\nKubernetes Distributions Rancher Red Hat OpenShift SUSE Containers as a Service Kubernetes Managed Services AWS Elastic Kubernetes Service Azure Kubernetes Service Google Kubernetes Engine Certifications CKAD Developers\nCKA Admins\nKubernetes Dashboard Link\nKubernetes Database etcd essentially a key value pair\nall k8s resources are stored in etcd in json format\njson is not very human friendly, so yaml is the de-facto choice for k8s config files, which are called manifests.\nA manifest file broadly contains -\napiVersion: # v1, v1beta1, v1beta2 etc. kind: # pod, deployment, secret, configmap etc. metadata: annotations: # used for configurations sometimes labels: selector: name: # name of the object resourceVersion: # value changes with each update data: # found in secret, and configmap objects spec: # configs, varies by object, absent for some like secret, configmaps kubectl Config file ~/.kube/config Structure of the config file, and the values that need to be specified -\napiVersion: v1 clusters: - cluster: certificate-authority-data: xxxx server: https://172.22.28.5:6443 name: kubernetes contexts: # combination of cluster, username and namespace - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: xxxx client-key-data: xxxx If this file is not present or has invalid details of a cluster, you might see an error like\n$ kubectl get all E1223 11:43:00.538822 14558 memcache.go:265] couldn't get current server API group list: Get \"http://localhost:8080/api?timeout=32s\": dial tcp 127.0.0.1:8080: connect: connection refused This file is usually generated with help of /etc/kubernetes/admin.conf file from the control node. This file‚Äôs user is kube admin\nCommands kubectl create deployment can‚Äôt specify replicas! incorrect! see below kubectl create deployment my-dep --image=nginx --replicas=3\nkubectl explain pod shows all the fields that are necessary to configure a pod. To deep dive into a particular property, use kubectl explain pod.Spec\nTo find out specific fields to specify for configuring [[K8S Scheduling#Node Affinity|nodeAffinity]], use kubectl explain pod.spec.affinity.nodeAffinity\nGenerate yaml from existing resources, use kubectl get -o yaml\nRemember to cleanup the output (metadata, and status) as these should be added automatically when the resource is created\nkubectl delete pod/podname --grace-period=0 --force to delete pod immediately\nTroubleshooting and Debugging When creating a pod, kubernetes first adds it to the etcd store.\nkubectl describe can highlight problems if there is an issue during this initial step. Once the pod is added to etcd, it‚Äôs then started up.\nkubectl -n namespace describe pod Containers[].State shows the current state of containers in the pod\nContainers: busybox: Container ID: containerd:// Image: busybox Image ID: docker.io/library/busybox@sha256:ver Port: Host Port: State: Waiting Reason: CrashLoopBackOff Last State: Terminated Reason: Completed # this can hint at a problem where the container has exited after completing its task Exit Code: 0 Started: Sun, 10 Dec 2023 00:06:04 +0000 Finished: Sun, 10 Dec 2023 00:06:04 +0000 Ready: False Restart Count: 7 Environment: Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sfpsw (ro) Events section shows any errors including any errors like CrashLoopBackOff. Latest events are at the bottom.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- ... Warning BackOff 3m24s (x48 over 13m) kubelet Back-off restarting failed container busybox in pod mydep-8677c6d8bd-8c2c6_default(72a66cb8-e29a-4be3-ac99-8f88565327f3) Once the pod is running, in addition to kubectl describe more information can be found out using the following commands.\nkubectl -n namespace get pods - high level view of pods in a namespace\nkubectl -n namespace get pod/pod-id -o yaml another way of getting similar info as kubectl describe, here the interesting field to watch is status.condition, and status.containerStatuses:\nRestart a pod?\nkubectl scale kubectl rollout restart deployment name kubectl delete pod name kunectl replace -f pod/name To find problems when a container/pod is running, use the following commands -\nkubectl -n namespace logs podname --all-containers get logs from all containers in pod podname\nkubectl -n namespace logs deployment/mydep --tail=10 -f follow logs from all pods under dep mydep\nkubectl -n namespace logs podname -c container get logs from container in pod podname\nkubectl -n namespace exec -it podname -- /bin/sh get a session into the container, if the container has a shell\nInspect last 1h events, helpful to find details about pods that don‚Äôt exist anymore kubectl -n namespace get events --sort-by='.lastTimestamp'\nPS: Node level events are displayed under -n default\nkubectl -n namespace get events --field-selector involvedObject.name=podname Find events related to a specific pod\nEven if a container has a shell, you will find many of the regular utilities missing since images are usually optimized for runtime.\nThe [[Linux#Proc|proc]] file system can still help in such a case to find running processes etc.\nHelpful debugging kubectl commands for most objects -\ndescribe Show details of a specific resource or group of resources logs Print the logs for a container in a pod events List events\nattach Attach to a running container exec Execute a command in a container cp Copy files and directories to and from containers port-forward Forward one or more local ports to a pod\nproxy Run a proxy to the Kubernetes API server auth Inspect authorization debug Create debugging sessions for troubleshooting workloads and nodes\nProblems with Nodes\nkubectl get nodes shows which nodes are available and in a ready state\nkubectl cordon - Use to mark node(s) unschedulable, can use selector. Use uncordon once the maintenance is done.\nkubectl drain - Prepare node for maintenance by removing running pods gracefully and marking it unschedulable for new pods.\nThe behaviour differs based on how the pod is started on the node -\nIf controlled by a daemon-set, the pods are ignored! Since the daemonset controller ignores the unschedulable node state. If controlled by deployment, replicat-set, stateful-set, job, replication controller, then drain will either evict the pods (if supported by API server), or delete them. If there are standalone pods, these won‚Äôt be deleted or evicted unless --force flag is specified. If a node is NOT_READY,\nCheck if kubelet is running on a node. Check networking plugin is setup properly and running Q: How to port forward to local, when running kubectl in docker?\nA: start the kubectl container on docker, and expose a port\ndocker run -it --name kubectl -p 8000:8000 kubectl:latest now run port forward as normal, but listen on 0.0.0.0 in addition to localhost.\nkubectl -n workload port-forward svc/workload --address localhost,0.0.0.0 8000:8000 Upgrade checks\nVerify api versions\nOn existing cluster\nkubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --ignore-not-found -o=go-template='{{range .items}}{{.metadata.namespace}}: {{.kind}}: {{.apiVersion}}{{\"\\n\"}}{{end}}' -n namespace \u003e\u003e namespace-all-resources.txt cat namespace-all-resources.txt namespace: Event: events.k8s.io/v1 namespace: Event: events.k8s.io/v1 # lists all resources, so some duplication namespace: ExternalSecret: external-secrets.io/v1beta1 namespace: ExternalSecret: external-secrets.io/v1beta1 namespace: InMemoryChannel: messaging.knative.dev/v1 namespace: Subscription: messaging.knative.dev/v1 # lists crd as well namespace: Subscription: messaging.knative.dev/v1 namespace: PodMetrics: metrics.k8s.io/v1beta1 namespace: ConfigMap: v1 namespace: Endpoints: v1 namespace: PersistentVolumeClaim: v1 namespace: Pod: v1 namespace: Secret: v1 namespace: ServiceAccount: v1 namespace: Deployment: apps/v1 namespace: ReplicaSet: apps/v1 ... ... cat namespace-all-resources.txt | uniq | cut -f3 -d\": \" | uniq \u003e\u003e check-api-list.txt # or, just shorten the initial query to only get apiVersion, and skip this On new cluster\nkubectl api-resources \u003e\u003e available-api-list.txt # search for api which appears in check-api-list.txt but not in available-api-list.txt while read api; do echo -n \"$api \"; grep -cx $api available-api-list.txt; done \u003c check-api-list.txt tool/v1alpha1 0 # not supported tool/v1beta 1 # supported ... Kubernetes Objects Diagram ![[k8s-objects.png]]\nDeployments Adds scalability, high availability, self healing capabilities to a pod by defining replication strategy and update strategy\nkubectl create deployment my-dep --image=nginx --replicas=3\nExample declaration\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # tells deployment to run 2 pods matching the template template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 kubectl rollout history deployments provides recent rollout events including reason for change (scale out/in not included)\nkubectl rollout history deployment/my-app\nRollback a failed deployment to previous version kubectl rollout undo deployment/my-app --to-revision=1\nCheck logs across all pods in a deployment\nkubectl logs deployment/deployment-name -n namespace found x pods Update Strategy Specify rollingUpdate or recreate (can cause temporary )\nDeployment rollingUpdate recreate Note deploys new replicaset, then removes old replicaset Disruption no yes Useful for add examples add examples apiVersion: kind: Deployment spec: strategy: rollingUpdate | recreate ReplicaSet Use labels to monitor pods. If you remove a label from the pod, see another come up within seconds, check the 1st and 2nd pod in output below.\nroot@controlplane:~$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS my-dep-7674c564c-9t2wk 1/1 Running 0 6m39s pod-template-hash=7674c564c,test=worksok my-dep-7674c564c-gxzb7 1/1 Running 0 6m39s app=my-dep,pod-template-hash=7674c564c my-dep-7674c564c-svzgv 1/1 Running 0 4s app=my-dep,pod-template-hash=7674c564c my-dep-7674c564c-v4mnc 1/1 Running 0 6m39s app=my-dep,pod-template-hash=7674c564c DaemonSet StatefulSet Persistence and consistent naming. Restarted pods in a statefulset use the same name.\nPods Usually a group of containers, volume declarations\nSmallest app building block in k8s, replicated across nodes to achieve the app‚Äôs desired availability, scalability, performance, capacity requirements.\nSmallest unit of compute that can be deployed.\nA Pod is similar to a set of containers with shared namespaces and shared filesystem volumes\nOffers similar isolation as [[Containers]] using cgroups, namespaces etc.\nExecute a command in a container contained in the pod - kubectl exec -it -c -- /bin/sh\nThere no ntworking within a pod. Any containers running within a pod use the same IP.\nRun a single stand alone pod, change it‚Äôs default image ‚Äúcommand‚Äù, check the output using kubectl logs kubectl run busybox --image busybox --command -- nslookup kubernetes\nkubectl run busybox --image busybox --command -- sleep 3600 kubectl exec busybox -it -- nslookup kubernetes\nDouble dash -- separates the kubectl command from the command you want to run in the container. Use -n namespace immediately after kubectl to avoid passing this argument to the container command instead.\n#ask can you do this in a single step? run a pod/container, and get the output on command line?\nFind all containers within a pod\nkubectl -n namespace get pods podname -o jsonpath=\"{.spec['containers','initContainers'][*].name}\" Find resource utilization of containers within a pod\nkubectl -n namespace top pod podname --containers Display multiple fields from each container within a pod, ex - name, image and resources\nkubectl -n namespace get pods podname -o jsonpath='{range .spec.containers[*]}{.name}{\"\\t\"}{.image}{\"\\t\"}{.resources}{\"\\n\"}{end}' Labels Add identifying information to an object. This information can then be used to query and select objects. Labels help add information to objects that is relevant to users, so are useful in UI or CLI.\nLabels allow users to map their own org structure on system resources. Things like environment, team etc.\nA label key and value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters each.\nOptionally, the key can begin with a DNS subdomain prefix and a single ‚Äò/‚Äô, like example.com/my-app.\nIt appears under the metadata field\napiVersion: kind: metadata: label: app: myawesomeapp List labels applied to a pod. By default, show-labels=false kubectl get pod/nginx --show-labels\nApply label to a pod kubectl label pod/podid newlabel=value\nRemove an existing label from a pod kubectl label pod/podid newlabel- Note the trailing -\nUpdate an existing label kubectl label pod/podid oldlabel=newvalue --overwrite without --overwrite flag label is not updated\nIf --overwrite is true, then existing labels can be overwritten, otherwise attempting to overwrite a label will result in an error.\nInspect labels applied to all objects kubectl get all --all-namespaces --show-labels\nUse Selector flag to list only resources with a specific label kubectl get all --selector app=my-dep\nSome labels are applied automatically, example on a [[#Namespace]], kubernetes.io/metadata.name=namespacename\nIf --resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used. This resource-version available under metadata.resourceVersion.\nSelector Appears under spec\napiVersion: kind: metadata: spec: selector: matchLabels: app: myawesomeapp Use Selector flag to list only resources with a specific label kubectl get all --selector app=my-dep\nAnnotations Add non-identifying information/metadata to objects. Annotations cannot be used to query and select objects. Information or metadata added as annotation to objects is mostly for use by machines ex - iam role annotations in case of IRSA\nDeployment versions are added as annotations to the metadata field in the manifest yml\napiVersion: apps/v1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" Labels vs Annotations Labels = identifying information, Annotations = non-identifying information Labels can be used to select objects or collection of objects, annotations cannot be used to identify or select objects Annotations can contain characters not allowed by labels Property Labels Annotations Notes Identifying information yes no a Limited characters yes no a Use with selector yes no a User friendly yes no a Namespace Provides isolation for resources Some objects are namespaced scoped while others are cluster wide Objects can have same name across namespaces, but must be unique within a namespace. So? Every service can use names like frontend, backend, cache without worrying about name collisions. Hierarchical namespaces - userguide All namespaces Use --all-namespace and -n flags to work with all, or a specific namespace kubectl [verb] [resource] --all-namespaces kubectl [verb] [resource] -A kubectl [verb] [resource] -n namespace\nExisting namespaces kubectl get ns on a fresh cluster will show these 4 existing namespaces\ndefault Active 48m kube-node-lease Active 48m kube-public Active 48m kube-system Active 48m Namespace Issue When creating a [[#Service]], a corresponding DNS entry like service.namespace.svc.cluster.local is created. Due to this, all namespace names must be valid DNS name.\nTo connect to a service in the same namespace, just specifying service is enough. It will be resolved locally within the same namespace. This is useful to launch multiple environments with the same config without much modifications.\nTo connect to a service in a different namespace, fully qualified name service.othernamespace.svc.cluster.local must be used.\n[!danger] Be careful about namespaces matching public domain names.\nSuppose, a namespace is named com, it contains a service called google. The local DNS name for it will be google.com.svc.cluster.local. If another service, foo in the same namespace tries to reach the public google.com, it will get resolved to the local google service instead.\nRestrict permissions to create namespaces, and use admission controllers to further enforce this.\n[!Test] Launch a service called landing in ai namespace, are other services in that space able to reach the public landing.ai service?\nI wasn‚Äôt able to reproduce this behaviour :(\nUpdate: I don‚Äôt see this happening with the busybox image, BUT this can be seen with the dnsutils image. All properties are exactly the same between both pods, so it might be down to the OS used in each image ü§∑\n$ kubectl -n ai get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE landing ClusterIP 10.97.252.177 80/TCP 21m # this still resolves to the public landing.ai service $ kubectl exec busybox -it -- nslookup landing.ai Server: 10.96.0.10 Address: 10.96.0.10:53 Non-authoritative answer: Name: landing.ai Address: 35.196.113.152 Non-authoritative answer: # this resolves to the private landing.ai service $ kubectl exec busybox -it -- nslookup landing.ai.svc.cluster.local Server: 10.96.0.10 Address: 10.96.0.10:53 Name: landing.ai.svc.cluster.local Address: 10.97.252.177 $ kubectl exec -it busybox -- cat /etc/resolv.conf search default.svc.cluster.local svc.cluster.local cluster.local nameserver 10.96.0.10 options ndots:5 $ kubectl exec -it dnsutils -- nslookup launch.ai Server: 10.96.0.10 Address: 10.96.0.10#53 Name: launch.ai.svc.cluster.local Address: 10.106.142.53 $ kubectl exec -it dnsutils -- nslookup launch Server: 10.96.0.10 Address: 10.96.0.10#53 ** server can\\'t find launch: NXDOMAIN $ kubectl exec -it dnsutils -- cat /etc/resolv.conf search default.svc.cluster.local svc.cluster.local cluster.local nameserver 10.96.0.10 options ndots:5 Service Almost like a virtual load balancer, connected to [[#Deployments]] using [[#Labels]]\nProperties - id address, target port, and endpoints, session affinity?\nIt connects to the nodes which run kube-proxy. kube-proxy uses iptables to connect to the pods running on the nodes.\nThis service object ensures, the traffic is redirected to one of the pods.\nkubectl get svc -A shows all services running in a cluster\nkubectl expose creates a service by looking up a deployment, replica set, replication controller, pod or another service by name and using the selector of the resource.\nkubectl expose deployment nginx --port=80 --target-port=8000\nPort vs Target Port? target port is the port on the pod that the service target, port is the port that the service exposes\nCluster IP default, internal access only\nNodePort ties a port of the node to the node of a pod, accessible from outside the cluster\nLoadBalancer Public cloud load balancers\nExternalName uses DNS names, redirection happens at DNS level\nService without selector use for direct connections based on ip/port, without an endpoint. Useful for databases and within namespaces\n#ask Can I not use a service for resources with no labels?\n#ask What is a headless service?\nIngress Successor [[GatewayApi]]\nProvides a http route from outside the cluster to services running in the cluster. It can also handle ssl termination, load balancing and name based virtual hosting.\nExample ingress sending all traffic to a single service\ngraph LR; client([client])-. Ingress-managed load balancer .-\u003eingress[Ingress]; ingress--\u003e|routing rule|service[Service]; subgraph cluster ingress; service--\u003epod1[Pod]; service--\u003epod2[Pod]; end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class ingress,service,pod1,pod2 k8s; class client plain; class cluster cluster; Example ingress config\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: minimal-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx-example rules: - http: paths: - path: /testpath pathType: Prefix backend: service: name: test port: number: 80 Ingress spec has rules which are matched against all incoming http requests, and the traffic is directed accordingly.\nIngress Annotations are often used to configure certain properties depending on the ingress controller in use.\nIf no host is specified in rules as in the example above, it matches all hosts.\nBackend can also be a resource, but you cannot specify both resource and service for a path. resource backend is useful for directing requests for static assets to an object storage.\npathType can be one of Prefix, Exact, or ImplementationSpecific (upto the IngressClass)\nFor exposing arbitraty protocols and ports, [[NodePort]] or LoadBalancer service type can be used.\nAn ingress resource on its own doesn‚Äôt mean anything, it needs an [[Ingress Controller]] to be present on the cluster to provide the required functionality.\nFor handling TLS, the ingress spec should refer to a secret which provides the cert and secret key. For TLS to work properly, the host values in spec.tls.hosts must match spec.rules.host.\n#find how is the ingress configured in the general eks cluster?\nBlog post\nIngress Controller Various options like nginx, aws alb, istio etc.\nEach ingress controller implements a particular ingress class. For ex, for aws load balancer controller, it is alb. (ref)\nNetowrking Offical docs Design doc\nNode contains pods which is controlled by a deployment, each pod has an IP. But\nService is connected to deployment using label\nIP is a pod property, not container property, kubectl describe pod shows the IP assigned to a pod, or use kubectl get pods -o wide\n4 major problems -\ncontainer to container communication - handled by [[##Pods|pod]], localhost communication pod to pod communication - explained below pod to service communication - handled by [[#Service|services]] external to service communication - handled by [[#Service|services]] Plugin When changing a network plugin - ensure the network cidr stays the same\nDNS # service NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-system kube-dns ClusterIP 10.96.0.10 53/UDP,53/TCP,9153/TCP 33s #pods NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-5dd5756b68-2l28z 1/1 Running 0 33s kube-system coredns-5dd5756b68-t55kw 1/1 Running 0 33 What objects get dns names?\n[[#Service]] service.namespace.svc.cluster.local [[#Pods]] pod-ipv4.namespace.pod.cluster.local Each pod has a dns policy defined under pod.spec.dnsPolicy, value is either of\nDefault, inherits from the node ClusterFirst, any query not matching cluster domain is forwarded to upstream DNS servers. ClusterFirstWithHostNet, for pods running with hostNetwork: true. None, specify dns configs under pod.spec.dnsConfig Note: default is NOT the default dns policy. If no policy is used ClusterFirst is used.\ndo an nsloop on kubernetes\n$ kubectl run busybox --image busybox --command -- nslookup kubernetes pod/busybox created $ kubectl logs pod/busybox Server: 10.96.0.10 Address: 10.96.0.10:53 ** server can't find kubernetes.cluster.local: NXDOMAIN ** server can't find kubernetes.cluster.local: NXDOMAIN Name: kubernetes.default.svc.cluster.local Address: 10.96.0.1 ** server can't find kubernetes.svc.cluster.local: NXDOMAIN ** server can't find kubernetes.svc.cluster.local: NXDOMAIN This provides the ip of the kubernetes service which can be verified using kubectl describe svc/kubernetes\nNote: lookup only works within the namespace. Outside the namespace, you won‚Äôt get the result!\n$ kubectl run dnsnginx --image busybox --command -- nslookup nginx pod/dnsnginx created $ kubectl run dnskube --image busybox --command -- nslookup kube-dns pod/dnskube created $ $ kubectl logs pod/dnsnginx Server: 10.96.0.10 Address: 10.96.0.10:53 ** server can't find nginx.cluster.local: NXDOMAIN ** server can't find nginx.cluster.local: NXDOMAIN ** server can't find nginx.default.svc.cluster.local: NXDOMAIN ** server can't find nginx.default.svc.cluster.local: NXDOMAIN $ kubectl -n nginx run dnsnginx --image busybox --command -- nslookup nginx pod/dnsnginx created root@controlplane:~$ kubectl logs pod/dnsnginx -n nginx Server: 10.96.0.10 Address: 10.96.0.10:53 Name: nginx.nginx.svc.cluster.local Address: 10.99.230.232 ** server can't find nginx.cluster.local: NXDOMAIN ** server can't find nginx.cluster.local: NXDOMAIN Why is that? Check the dns config inserted into a pod -\n$ kubectl -n nginx exec -it dnsnginx -- /bin/sh / # cat /etc/resolv.conf search nginx.svc.cluster.local svc.cluster.local cluster.local nameserver 10.96.0.10 options ndots:5 The name server 10.96.0.10 points to the kube-dns service running in the kube-system namespace\n$ kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 443/TCP 52m kube-system kube-dns ClusterIP 10.96.0.10 53/UDP,53/TCP,9153/TCP 52m nginx nginx ClusterIP 10.99.230.232 80/TCP 43m Q: How to connect to a service running in namespace B if it can‚Äôt be queried from pods in namespace A? A: Service name can be queried using the format servicename.namespace from any namespace in the cluster\nhttps://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/\nStorage kubectl explain pod.spec.volumes shows the different volume types that are available for use.\nVolumes Volumes can be ephermal or persistent.\nTo use a volume within a pod‚Äôs containers, you need to specify spec.volumes and spec.containers[*].volumeMounts. The container so created sees the data contained in the image + any data mounted as a volume.\nSpecified for a pod in spec.volumes, to check all the available configuration options, use kubectl explain pod.spec.volumes\nVolume types were cloud specific which have now been deprecated in favor of 3rd party [[##storage drivers]] instead. The following volume types are still valid -\nSecret (always mounted as RO, don‚Äôt use as subpath to receive updates) ConfigMap (always mounted as RO, don‚Äôt use as subpath to receive updates) Local, Empty Dir, Host Path relate to local filesystems of the node. PVC Projected Downward API - check coredns pods graph LR; subgraph pod subgraph container1 m1[volMount] end subgraph container2 m2[volMount] end subgraph volumes v[vol] end end subgraph storage pv[pv] end subgraph claim pvc[pvc] end pv --bound--\u003e pvc v --\u003e m1 v --\u003e m2 pvc --\u003e v PV Persistent Volumes decouple the storage requirements from pod development. PV use properties like accessModes, capacity, mountOptions, pvreclaimPolicy, volumeMode etc to mount the persistent volume to the pod.\nPV can be created manually (manifest) or dynamically (using a storage class)\nAccess Modes can be one of the following\nReadWriteOnce (RWO) - A single node can mount this volume as read write. Many pods on this node can still use the volume. ReadOnlyMany (ROX) - Many pods can mount the volume as read only. ReadWriteMany (RWX) - Many pods can mount the volume as read, write. ReadWriteOncePod (RWOP) - A single pod can mount the volume as read, write (version v1.22 onwards only). PVC Persistent volume claims are used by pod authors to add storage needs in a declarative way, without worrying about storage specifics.\nPVC use properties like accessModes, volumeMode, storageClassName, resources, selector to provision the storage as per the requirements. kubectl explain pvc.spec to know about all the properties.\nSimple local example\n# pv.yaml kind: PersistentVolume apiVersion: v1 metadata: name: pv-vol # not used anywhere labels: type: local spec: accessModes: - ReadWriteOnce capacity: storage: 2Gi hostPath: path: \"/data\" # this should exist on host # pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pv-claim # used in pod.spec.volumes[].pvc.claimName spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi # \u003c= pv.spec.capacity.storage # pod.yaml kind: Pod apiVersion: v1 metadata: name: pv-pod spec: containers: name: pv-container image: nginx ports: - containerPort: 80 name: nginxhttp volumeMounts: - mountPath: \"/usr/share/nginx/html\" name: cvol # from pod.spec.volumes[].name volumes: - name: cvol persistentVolumeClaim: claimName: pv-claim # from pvc.metadata.name #ask what happens if pvc.spec.requests.storage \u003e pv.spec.capacity.storage ?\nStorage Class can be grouped according to anything - capacity, type, location etc. Uses spec.provisioner to connect to the storage When a PVC does not specify a¬†storageClassName, the default StorageClass is used. The cluster can only have one default StorageClass. If more than one default StorageClass is set, the newest default is used. Example\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: standard provisioner: kubernetes.io/aws-ebs parameters: # provisioner specific parameters type: gp2 reclaimPolicy: Retain allowVolumeExpansion: true mountOptions: - debug volumeBindingMode: Immediate ConfigMap Decouple configuration from application\nexample, notice it uses data instead of the usual spec\napiVersion: v1 kind: ConfigMap metadata: name: nginxcm data: # use in `pod.spec.volumes[].configMap.items[].key` nginx-custom-config.conf: | server { listen 8080; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } } Use it in a pod\napiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx volumeMounts: - name: conf mountPath: /etc/nginx/conf.d/ volumes: - name: conf configMap: name: nginxcm items: # key as in configMap.data.key - key: nginx-custom-config.conf # path within the container path: default.conf Secrets Decouple sensitive variables from application\nexample - notice it uses data instead of the usual spec\napiVersion: v1 kind: Secret metadata: name: secret data: username: encodedusername password: encodedpassword Kubernetes API Collection of [[RESTful APIs]], supports GET, POST, DELETE. It is crucial to identify api version to use.\n#ask why did kubernetes project choose this RESTful API approach?\nTo allow the system to continuously evolve and grow.\nNew features can be easily added without impacting existing clients as alpha, and moved to beta, then stable version as they mature.\nIt also allows the project to maintain compatibility with existing clients by offering both beta and stable version of an API simultaneously (for a length of time).\nVersioning is done at the API level rather than at the resource or field level to ensure that the API presents a clear, consistent view of system resources and behavior, and to enable controlling access to end-of-life and/or experimental APIs.\nThe API server handles the conversion between API versions transparently: all the different versions are actually representations of the same persisted data. The API server may serve the same underlying data through multiple API versions.\nSo, if I create a resource using an API version v1beta1, I can later use v1 version to query or manage it (within the deprecation period). Some fields may need updating due to the API graduating to v1, but, I can still migrate to the newer version of the API without having to destroy and recreate the resource.\nAPI versions cannot be removed in future versions until this issue is fixed.\nAPI access is controlled by the API server.\nIt saves the serialized objects in [[etcd]].\nAPI resources are distinguished by their API group, resource type, namespace (for namespaced resources), and name.\nMonitor deprecated API requests - apiserver_requested_deprecated_apis metric. This can help identify if there are objects in the cluster still using deprecated APIs.\n#ask kube-proxy, where is it hosted, host it works?\ngraph LR subgraph server api[api/etcd] end cr[curl] --\u003e kp[kube-proxy] --\u003e api List available resource APIs, their kind, groups, version, namespaced (bool), version, any shortnames etc, use kubectl api-resources -o wide\nAPI groups can be¬†enabled or disabled using --runtime-config flag on API server\n$ kubectl api-resources NAME SHORTNAMES APIVERSION NAMESPACED KIND .. configmaps cm v1 true ConfigMap ... namespaces ns v1 false Namespace nodes no v1 false Node persistentvolumeclaims pvc v1 true PersistentVolumeClaim persistentvolumes pv v1 false PersistentVolume pods po v1 true Pod ... secrets v1 true Secret serviceaccounts sa v1 true ServiceAccount services svc v1 true Service ... networking.k8s.io/v1 false IngressClass ingresses ing networking.k8s.io/v1 true Ingress networkpolicies netpol ... List versions of available API kubectl api-versions\n$ kubectl api-versions .. apps/v1 authentication.k8s.io/v1 authorization.k8s.io/v1 autoscaling/v1 autoscaling/v2 .. flowcontrol.apiserver.k8s.io/v1beta2 flowcontrol.apiserver.k8s.io/v1beta3 networking.k8s.io/v1 node.k8s.io/v1 policy/v1 rbac.authorization.k8s.io/v1 scheduling.k8s.io/v1 storage.k8s.io/v1 v1 Find properties required for an object, see kubectl commands section above kubectl explain Examples -\nkubectl explain externalsecrets # explanation only for top level properties kubectl explain externalsecrets --recursive # no explanation, prints the complete schema kubectl explain externalsecrets.spec.target # explanation for a specific property Find namespace scoped APIs or cluster wide APIs kubectl api-resources --namespaced=true kubectl api-resources --namespaced=false\nProxy kubectl to access the API more easily using [[Curl]]\n#ask But why would you do this? If an app needs to interact with kubernetes, it can simply use the language specific http library to do this directly instead of going through kubectl\nkubectl proxy --port=8080\n$ curl http://localhost:8080/version { \"major\": \"1\", \"minor\": \"28\", \"gitVersion\": \"v1.28.2\", \"gitCommit\": \"89a4ea3e1e4ddd7f7572286090359983e0387b2f\", \"gitTreeState\": \"clean\", \"buildDate\": \"2023-09-13T09:29:07Z\", \"goVersion\": \"go1.20.8\", \"compiler\": \"gc\", \"platform\": \"linux/amd64\" } Get pods from kube-system namespace (truncated output)\n$ curl http://localhost:8080/api/v1/namespaces/kube-system/pods | les { \"kind\": \"PodList\", \"apiVersion\": \"v1\", \"metadata\": { \"resourceVersion\": \"1555\" \"name\": \"coredns-5dd5756b68-fcz42\", \"generateName\": \"coredns-5dd5756b68-\", \"namespace\": \"kube-system\", \"uid\": \"326ba1b7-31b6-4d6c-9978-1057f6734154\", \"resourceVersion\": \"553\", .. Check the openapi v3 specification (truncated output) on /openapi/v3, and v2 specification on /openapi/v2\n$ curl http://localhost:8080/openapi/v3 { \"paths\": { \".well-known/openid-configuration\": { \"serverRelativeURL\": \"/openapi/v3/.well-known/openid-configuration?hash=4488--\" }, \"api\": { \"serverRelativeURL\": \"/openapi/v3/api?hash=929E--\" }, \"api/v1\": { \"serverRelativeURL\": \"/openapi/v3/api/v1?hash=5133--\" }, \"apis\": { \"serverRelativeURL\": \"/openapi/v3/apis?hash=27E0--\" }, \"apis/admissionregistration.k8s.io\": { \"serverRelativeURL\": \"/openapi/v3/apis/admissionregistration.k8s.io?hash=E8D5..\" } } API Extensions Custom Resources\nThis is a way to make the API server recognize new non-standard Kubernetes objects.\nExample - Prometheus Operator uses a number of CRDs to manage the deployment in a cluster.\nAggregation Layer\nNeeds to be enabled and then runs in-process in the kube-apisever.\nYou first need to create an APIService object, say myawesomeapi, at a path, say apis/myawesomeapi/v1beta1/. The aggregation layer then proxies any requests API server receives for this API to the registered APIService.\nExample - metrics server\nCreate a Cluster Manually! Kubernetes releases before v1.24 included a direct integration with Docker Engine, using a component named¬†dockershim.\nWhat is dockershim?\nTo provide support for multile container runtimes, CRI API/ specification was developed. But since docker was the first container runtime k8s supported, and to maintain backward compatibility, dockershim was developed which allowed kubelet to interact with docker runtime via the CRI API, sort of like a proxy?\ngraph LR; kb[kubelet] \u003c--cri--\u003e ds[dockershim] \u003c--\u003e dc[docker] \u003c--\u003e cd[containerd] --\u003e c1[container 1] cd --\u003e c2[container 2] cd --\u003e cn[container n] graph LR; kb[kubelet] \u003c--cri--\u003e ccd[cri-containerd] \u003c--\u003e cd[containerd] --\u003e c1[container 1] cd --\u003e c2[container 2] cd --\u003e cn[container n] Create a 3 node cluster - 1 control node, and 2 worker nodes.\nOn control node, kubeadm init On control node, Networking On worker node, kubeadm join Control node\nInstall a container runtime like docker, containerd, cri-o. Install kube tools like kubeadm, kubelet, kubectl kubeadm init Ref Setup $HOME/.kube/config Verify all hosts are present under /etc/hosts Install a pod network add-on - any one of calico, cilium, flannel etc. Worker nodes\nInstall a container runtime like docker, containerd, cri-o. Install kube tools like kubeadm, kubelet, kubectl kubeadm join --token xx --discovery-cert xx To create high availability - use 3 controller nodes, each running with etcd, or use a dedicated etcd cluster\nOlder, needs refining Pod Disruption Budgets https://kubernetes.io/docs/tasks/run-application/configure-pdb/ Based on the value of maxUnavailable for specific pods, cluster autoscaler will either ignore a node, or scale it down.\nPod Affinity https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/ uses pod labels\nexample\napiVersion: v1 kind: Pod metadata: name: label-demo labels: environment: production app: nginx spec: . . . Pods Anti Affinity https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\nlabels allow us to use selectors\nlabels are also useful to slice/dice resources when using kubectl\nkubectl get pods -Lapp -Ltier -Lrole -L displays an extra column in kubectl output -l either selects or update the label applied to a resourse.\nSelectors can be of 2 types\nEquality based (accelerator=nvidia-tesla-p100)\napiVersion: v1 kind: Pod metadata: name: cuda-test spec: containers: - name: cuda-test image: \"registry.k8s.io/cuda-vector-add:v0.1\" resources: limits: nvidia.com/gpu: 1 nodeSelector: accelerator: nvidia-tesla-p100 Set based\napiVersion: v1 kind: Pod metadata: name: cuda-test spec: containers: - name: cuda-test image: \"registry.k8s.io/cuda-vector-add:v0.1\" resources: limits: nvidia.com/gpu: 1 nodeSelector: environment: qa,qa1 # and condition accelerator in (nvidia, intel) service, replicationcontroller format for selector\nselector: component: redis daemonset, replicaset, deployment, job format for selector\nselector: matchLabels: component: redis matchExpressions: - {key: component, values: [redis]} Labels Standard or default kubernetes.io/arch kubernetes.io/hostname # cloud provider specific kubernetes.io/os node.kubernetes.io/instance-type # if available to kubelet topology.kubernetes.io/region # topology.kubernetes.io/zone # Labels and selectors [[K8S Scheduling]]\nTroubleshooting Pod Error Alerts sum (kube_pod_container_status_waiting_reason{reason=~\"CrashLoopBackOff|ImagePullBackOff|ErrImagePull.+\"}) by (namespace, container, reason) Questions 1. What is the value of kubernetes.io/hostname in [[eks]]? I know it‚Äôs part of standard labels #todo (link it) but, not seen this tag really on [[eks]]. Found following tags instead ü§∑\nkubernetes.io/cluster/myawesomecluster=owned `aws:eks:cluster-name=myawesomecluster 2. How do pods communicate with each other in a cluster? 3. How will you control which pod runs on which node(s) Mix of scheduling options like node selector, affinity/anti-affinity, taints, tolerations etc.\n","wordCount":"5520","inLanguage":"en","datePublished":"2024-04-15T00:00:00Z","dateModified":"2025-04-04T12:48:24+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://abiydv.github.io/notes/k8s-main/"},"publisher":{"@type":"Organization","name":"@abiydv","logo":{"@type":"ImageObject","url":"https://abiydv.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://abiydv.github.io/ accesskey=h title="@abiydv (Alt + H)">@abiydv</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://abiydv.github.io/ title=Home><span>Home</span></a></li><li><a href=https://abiydv.github.io/about/ title=About><span>About</span></a></li><li><a href=https://abiydv.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://abiydv.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://abiydv.github.io/notes/ title=Notes><span>Notes</span></a></li><li><a href=https://abiydv.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: true });
</script><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://abiydv.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://abiydv.github.io/notes/>Notes</a></div><h1 class=post-title>K8s Kubernetes<sup>&nbsp;<span class="entry-stage incubating">&nbsp;incubating&nbsp;</span></sup></h1><div class=post-meta><s>15.4.24</s>&nbsp;4.4.25 / Notes / 26 min</div></header><div class=post-content><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#architecture aria-label=Architecture>Architecture</a></li><li><a href=#components aria-label=Components>Components</a><ul><li><a href=#metric-server aria-label="Metric Server">Metric Server</a></li></ul></li><li><a href=#cncf aria-label=CNCF>CNCF</a><ul><ul><li><a href=#project-status aria-label="Project status">Project status</a></li></ul></ul></li><li><a href=#kubernetes-plugins aria-label="Kubernetes Plugins?">Kubernetes Plugins?</a><ul><li><a href=#crio aria-label=CRIO>CRIO</a></li><li><a href=#cni aria-label=CNI>CNI</a></li><li><a href=#jaeger aria-label=Jaeger>Jaeger</a></li><li><a href=#rook aria-label=Rook>Rook</a></li><li><a href=#cluster-autoscaler aria-label="Cluster Autoscaler">Cluster Autoscaler</a></li></ul></li><li><a href=#kubernetes-distributions aria-label="Kubernetes Distributions">Kubernetes Distributions</a><ul><li><a href=#rancher aria-label=Rancher>Rancher</a></li><li><a href=#red-hat-openshift aria-label="Red Hat OpenShift">Red Hat OpenShift</a></li><li><a href=#suse-containers-as-a-service aria-label="SUSE Containers as a Service">SUSE Containers as a Service</a></li></ul></li><li><a href=#kubernetes-managed-services aria-label="Kubernetes Managed Services">Kubernetes Managed Services</a><ul><li><a href=#aws-elastic-kubernetes-service aria-label="AWS Elastic Kubernetes Service">AWS Elastic Kubernetes Service</a></li><li><a href=#azure-kubernetes-service aria-label="Azure Kubernetes Service">Azure Kubernetes Service</a></li><li><a href=#google-kubernetes-engine aria-label="Google Kubernetes Engine">Google Kubernetes Engine</a></li></ul></li><li><a href=#certifications aria-label=Certifications>Certifications</a><ul><li><a href=#ckad aria-label=CKAD>CKAD</a></li><li><a href=#cka aria-label=CKA>CKA</a></li></ul></li><li><a href=#kubernetes-dashboard aria-label="Kubernetes Dashboard">Kubernetes Dashboard</a></li><li><a href=#kubernetes-database aria-label="Kubernetes Database">Kubernetes Database</a></li><li><a href=#kubectl aria-label=kubectl>kubectl</a><ul><li><a href=#commands aria-label=Commands>Commands</a></li><li><a href=#troubleshooting-and-debugging aria-label="Troubleshooting and Debugging">Troubleshooting and Debugging</a></li></ul></li><li><a href=#kubernetes-objects aria-label="Kubernetes Objects">Kubernetes Objects</a><ul><li><a href=#diagram aria-label=Diagram>Diagram</a></li><li><a href=#deployments aria-label=Deployments>Deployments</a></li><li><a href=#update-strategy aria-label="Update Strategy">Update Strategy</a></li><li><a href=#replicaset aria-label=ReplicaSet>ReplicaSet</a></li><li><a href=#daemonset aria-label=DaemonSet>DaemonSet</a></li><li><a href=#statefulset aria-label=StatefulSet>StatefulSet</a></li><li><a href=#pods aria-label=Pods>Pods</a></li><li><a href=#labels aria-label=Labels>Labels</a><ul><li><a href=#selector aria-label=Selector>Selector</a></li></ul></li><li><a href=#annotations aria-label=Annotations>Annotations</a></li><li><a href=#labels-vs-annotations aria-label="Labels vs Annotations">Labels vs Annotations</a></li><li><a href=#namespace aria-label=Namespace>Namespace</a><ul><li><a href=#all-namespaces aria-label="All namespaces">All namespaces</a></li><li><a href=#existing-namespaces aria-label="Existing namespaces">Existing namespaces</a></li><li><a href=#namespace-issue aria-label="Namespace Issue">Namespace Issue</a></li></ul></li><li><a href=#service aria-label=Service>Service</a><ul><li><a href=#cluster-ip aria-label="Cluster IP">Cluster IP</a></li><li><a href=#nodeport aria-label=NodePort>NodePort</a></li><li><a href=#loadbalancer aria-label=LoadBalancer>LoadBalancer</a></li><li><a href=#externalname aria-label=ExternalName>ExternalName</a></li><li><a href=#service-without-selector aria-label="Service without selector">Service without selector</a></li></ul></li><li><a href=#ingress aria-label=Ingress>Ingress</a></li><li><a href=#ingress-controller aria-label="Ingress Controller">Ingress Controller</a></li></ul></li><li><a href=#netowrking aria-label=Netowrking>Netowrking</a><ul><li><a href=#plugin aria-label=Plugin>Plugin</a></li></ul></li><li><a href=#dns aria-label=DNS>DNS</a></li><li><a href=#storage aria-label=Storage>Storage</a><ul><li><a href=#volumes aria-label=Volumes>Volumes</a></li><li><a href=#storage-class aria-label="Storage Class">Storage Class</a></li></ul></li><li><a href=#configmap aria-label=ConfigMap>ConfigMap</a><ul><li><a href=#secrets aria-label=Secrets>Secrets</a></li></ul></li><li><a href=#kubernetes-api aria-label="Kubernetes API">Kubernetes API</a><ul><li><a href=#api-extensions aria-label="API Extensions">API Extensions</a></li></ul></li><li><a href=#create-a-cluster-manually aria-label="Create a Cluster Manually!">Create a Cluster Manually!</a></li><li><a href=#older-needs-refining aria-label="Older, needs refining">Older, needs refining</a></li><li><a href=#labels-1 aria-label=Labels>Labels</a><ul><li><a href=#standard-or-default aria-label="Standard or default">Standard or default</a></li></ul></li></ul><li><a href=#labels-and-selectors aria-label="Labels and selectors">Labels and selectors</a><ul><li><a href=#troubleshooting aria-label=Troubleshooting>Troubleshooting</a><ul><li><a href=#pod-error-alerts aria-label="Pod Error Alerts">Pod Error Alerts</a></li></ul></li><li><a href=#questions aria-label=Questions>Questions</a><ul><ul><ul><li><a href=#1-what-is-the-value-of-kubernetesiohostname-in-eks aria-label="1. What is the value of kubernetes.io/hostname in [[eks]]?">1. What is the value of <code>kubernetes.io/hostname</code> in [[eks]]?</a></li><li><a href=#2-how-do-pods-communicate-with-each-other-in-a-cluster aria-label="2. How do pods communicate with each other in a cluster?">2. How do pods communicate with each other in a cluster?</a></li><li><a href=#3-how-will-you-control-which-pod-runs-on-which-nodes aria-label="3. How will you control which pod runs on which node(s)">3. How will you control which pod runs on which node(s)</a></li></ul></li></ul></ul></ul></li></ul></div></details></div></div><div class=post-content><h2 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h2><pre class=mermaid>graph LR;
c1(Cluster) --> n1(node1) --> p11[pod 1]
n1 --> p12[pod 2]
c1 --> n2(node2) --> p21[pod 1]
</pre><p>API docs - <a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/>https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/</a></p><p>Insert diagram about control nodes + worker nodes + api servers etc.</p><p><img loading=lazy src=https://kubernetes.io/images/docs/components-of-kubernetes.svg alt=k8s-arch></p><p>#refine <a href=https://kubernetes.io/docs/concepts/architecture/>https://kubernetes.io/docs/concepts/architecture/</a></p><h2 id=components>Components<a hidden class=anchor aria-hidden=true href=#components>#</a></h2><h3 id=metric-server>Metric Server<a hidden class=anchor aria-hidden=true href=#metric-server>#</a></h3><p>Use <code>HostNetwork</code></p><p>Helps monitor pods
Can run <code>kubectl top pod</code> to check resource usage for pods
<code>kubectl top nodes</code></p><h2 id=cncf>CNCF<a hidden class=anchor aria-hidden=true href=#cncf>#</a></h2><h4 id=project-status>Project status<a hidden class=anchor aria-hidden=true href=#project-status>#</a></h4><pre class=mermaid>graph LR;
a(Sandbox<br>New) --> b(Incubating<br>More wide-spread adoption, active development) --> c[Graduated<br>mature, stable part of k8s core]
</pre><p><a href=https://www.cncf.io/projects>https://www.cncf.io/projects</a></p><h2 id=kubernetes-plugins>Kubernetes Plugins?<a hidden class=anchor aria-hidden=true href=#kubernetes-plugins>#</a></h2><h3 id=crio>CRIO<a hidden class=anchor aria-hidden=true href=#crio>#</a></h3><p>Kubernetes container runtime #readmore</p><h3 id=cni>CNI<a hidden class=anchor aria-hidden=true href=#cni>#</a></h3><p>Common network interface</p><h3 id=jaeger>Jaeger<a hidden class=anchor aria-hidden=true href=#jaeger>#</a></h3><p>Kubernetes Operator, manages packaging, deploying and managing applications</p><h3 id=rook>Rook<a hidden class=anchor aria-hidden=true href=#rook>#</a></h3><p>Storage Orchestrator</p><h3 id=cluster-autoscaler>Cluster Autoscaler<a hidden class=anchor aria-hidden=true href=#cluster-autoscaler>#</a></h3><p><a href=https://github.com/kubernetes/autoscaler>https://github.com/kubernetes/autoscaler</a></p><h2 id=kubernetes-distributions>Kubernetes Distributions<a hidden class=anchor aria-hidden=true href=#kubernetes-distributions>#</a></h2><h3 id=rancher>Rancher<a hidden class=anchor aria-hidden=true href=#rancher>#</a></h3><h3 id=red-hat-openshift>Red Hat OpenShift<a hidden class=anchor aria-hidden=true href=#red-hat-openshift>#</a></h3><h3 id=suse-containers-as-a-service>SUSE Containers as a Service<a hidden class=anchor aria-hidden=true href=#suse-containers-as-a-service>#</a></h3><h2 id=kubernetes-managed-services>Kubernetes Managed Services<a hidden class=anchor aria-hidden=true href=#kubernetes-managed-services>#</a></h2><h3 id=aws-elastic-kubernetes-service>AWS Elastic Kubernetes Service<a hidden class=anchor aria-hidden=true href=#aws-elastic-kubernetes-service>#</a></h3><h3 id=azure-kubernetes-service>Azure Kubernetes Service<a hidden class=anchor aria-hidden=true href=#azure-kubernetes-service>#</a></h3><h3 id=google-kubernetes-engine>Google Kubernetes Engine<a hidden class=anchor aria-hidden=true href=#google-kubernetes-engine>#</a></h3><h2 id=certifications>Certifications<a hidden class=anchor aria-hidden=true href=#certifications>#</a></h2><h3 id=ckad>CKAD<a hidden class=anchor aria-hidden=true href=#ckad>#</a></h3><p>Developers</p><h3 id=cka>CKA<a hidden class=anchor aria-hidden=true href=#cka>#</a></h3><p>Admins</p><h2 id=kubernetes-dashboard>Kubernetes Dashboard<a hidden class=anchor aria-hidden=true href=#kubernetes-dashboard>#</a></h2><p><a href=https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/>Link</a></p><h2 id=kubernetes-database>Kubernetes Database<a hidden class=anchor aria-hidden=true href=#kubernetes-database>#</a></h2><p><code>etcd</code> essentially a key value pair</p><p>all k8s resources are stored in etcd in json format</p><p>json is not very human friendly, so yaml is the de-facto choice for k8s config files, which are called manifests.</p><p>A manifest file broadly contains -</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>:     <span style=color:#75715e># v1, v1beta1, v1beta2 etc.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>:           <span style=color:#75715e># pod, deployment, secret, configmap etc.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>annotations</span>:  <span style=color:#75715e># used for configurations sometimes</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:        
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>:         <span style=color:#75715e># name of the object</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>resourceVersion</span>: <span style=color:#75715e># value changes with each update</span>
</span></span><span style=display:flex><span><span style=color:#f92672>data</span>: <span style=color:#75715e># found in secret, and configmap objects</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>: <span style=color:#75715e># configs, varies by object, absent for some like secret, configmaps</span>
</span></span></code></pre></div><h2 id=kubectl>kubectl<a hidden class=anchor aria-hidden=true href=#kubectl>#</a></h2><p>Config file <code>~/.kube/config</code></p><p>Structure of the config file, and the values that need to be specified -</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>clusters</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>cluster</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>certificate-authority-data</span>: <span style=color:#ae81ff>xxxx</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>server</span>: <span style=color:#ae81ff>https://172.22.28.5:6443</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kubernetes</span>
</span></span><span style=display:flex><span><span style=color:#f92672>contexts</span>: <span style=color:#75715e># combination of cluster, username and namespace</span>
</span></span><span style=display:flex><span>- <span style=color:#f92672>context</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>cluster</span>: <span style=color:#ae81ff>kubernetes</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>user</span>: <span style=color:#ae81ff>kubernetes-admin</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kubernetes-admin@kubernetes</span>
</span></span><span style=display:flex><span><span style=color:#f92672>current-context</span>: <span style=color:#ae81ff>kubernetes-admin@kubernetes</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Config</span>
</span></span><span style=display:flex><span><span style=color:#f92672>preferences</span>: {}
</span></span><span style=display:flex><span><span style=color:#f92672>users</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kubernetes-admin</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>user</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>client-certificate-data</span>: <span style=color:#ae81ff>xxxx</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>client-key-data</span>: <span style=color:#ae81ff>xxxx</span>
</span></span></code></pre></div><p>If this file is not present or has invalid details of a cluster, you might see an error like</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ kubectl get all
</span></span><span style=display:flex><span>E1223 11:43:00.538822   <span style=color:#ae81ff>14558</span> memcache.go:265<span style=color:#f92672>]</span> couldn<span style=color:#960050;background-color:#1e0010>&#39;</span>t get current server API group list: Get <span style=color:#e6db74>&#34;http://localhost:8080/api?timeout=32s&#34;</span>: dial tcp 127.0.0.1:8080: connect: connection refused
</span></span></code></pre></div><p>This file is usually generated with help of <code>/etc/kubernetes/admin.conf</code> file from the control node. This file&rsquo;s user is kube admin</p><h3 id=commands>Commands<a hidden class=anchor aria-hidden=true href=#commands>#</a></h3><p><del><code>kubectl create deployment</code> can&rsquo;t specify replicas!</del> incorrect! see below
<code>kubectl create deployment my-dep --image=nginx --replicas=3</code></p><p><code>kubectl explain pod</code> shows all the fields that are necessary to configure a pod.
To deep dive into a particular property, use
<code>kubectl explain pod.Spec</code></p><p>To find out specific fields to specify for configuring [[K8S Scheduling#Node Affinity|nodeAffinity]], use
<code>kubectl explain pod.spec.affinity.nodeAffinity</code></p><p>Generate <code>yaml</code> from existing resources, use
<code>kubectl get &lt;resource> -o yaml</code></p><p>Remember to cleanup the output (metadata, and status) as these should be added automatically when the resource is created</p><p><code>kubectl delete pod/podname --grace-period=0 --force</code> to delete pod immediately</p><h3 id=troubleshooting-and-debugging>Troubleshooting and Debugging<a hidden class=anchor aria-hidden=true href=#troubleshooting-and-debugging>#</a></h3><p>When creating a pod, kubernetes first adds it to the etcd store.</p><p><code>kubectl describe</code> can highlight problems if there is an issue during this initial step. Once the pod is added to etcd, it&rsquo;s then started up.</p><p><code>kubectl -n namespace describe pod &lt;podname></code>
Containers[].State shows the current state of containers in the pod</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>Containers</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>busybox</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>Container ID</span>:   <span style=color:#ae81ff>containerd://</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>Image</span>:          <span style=color:#ae81ff>busybox</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>Image ID</span>:       <span style=color:#ae81ff>docker.io/library/busybox@sha256:ver</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>Port</span>:           <span style=color:#ae81ff>&lt;none&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>Host Port</span>:      <span style=color:#ae81ff>&lt;none&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>State</span>:          <span style=color:#ae81ff>Waiting</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>Reason</span>:       <span style=color:#ae81ff>CrashLoopBackOff</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>Last State</span>:     <span style=color:#ae81ff>Terminated</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>Reason</span>:       <span style=color:#ae81ff>Completed </span> <span style=color:#75715e># this can hint at a problem where the container has exited after completing its task</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>Exit Code</span>:    <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>Started</span>:      <span style=color:#ae81ff>Sun, 10 Dec 2023 00:06:04 +0000</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>Finished</span>:     <span style=color:#ae81ff>Sun, 10 Dec 2023 00:06:04 +0000</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>Ready</span>:          <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>Restart Count</span>:  <span style=color:#ae81ff>7</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>Environment</span>:    <span style=color:#ae81ff>&lt;none&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>Mounts</span>:
</span></span><span style=display:flex><span>      <span style=color:#ae81ff>/var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sfpsw (ro)</span>
</span></span></code></pre></div><p>Events section shows any errors including any errors like <code>CrashLoopBackOff</code>. Latest events are at the bottom.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-zsh data-lang=zsh><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  Type     Reason     Age                   From               Message
</span></span><span style=display:flex><span>  ----     ------     ----                  ----               -------  
</span></span><span style=display:flex><span>  ... 
</span></span><span style=display:flex><span>  Warning  BackOff    3m24s <span style=color:#f92672>(</span>x48 over 13m<span style=color:#f92672>)</span>  kubelet            Back-off restarting failed container busybox in pod mydep-8677c6d8bd-8c2c6_default<span style=color:#f92672>(</span>72a66cb8-e29a-4be3-ac99-8f88565327f3<span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Once the pod is running, in addition to <code>kubectl describe</code> more information can be found out using the following commands.</p><p><code>kubectl -n namespace get pods</code> - high level view of pods in a namespace</p><p><code>kubectl -n namespace get pod/pod-id -o yaml</code> another way of getting similar info as <code>kubectl describe</code>, here the interesting field to watch is <code>status.condition</code>, and <code>status.containerStatuses:</code></p><p>Restart a pod?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl scale
</span></span><span style=display:flex><span>kubectl rollout restart deployment name
</span></span><span style=display:flex><span>kubectl delete pod name
</span></span><span style=display:flex><span>kunectl replace -f pod/name
</span></span></code></pre></div><p>To find problems when a container/pod is running, use the following commands -</p><p><code>kubectl -n namespace logs podname --all-containers</code> get logs from all containers in pod <code>podname</code></p><p><code>kubectl -n namespace logs deployment/mydep --tail=10 -f</code> follow logs from all pods under dep <code>mydep</code></p><p><code>kubectl -n namespace logs podname -c container</code> get logs from <code>container</code> in pod <code>podname</code></p><p><code>kubectl -n namespace exec -it podname -- /bin/sh</code> get a session into the container, if the container has a shell</p><p>Inspect last 1h events, helpful to find details about pods that don&rsquo;t exist anymore
<code>kubectl -n namespace get events --sort-by='.lastTimestamp'</code></p><p>PS: Node level events are displayed under <code>-n default</code></p><p><code>kubectl -n namespace get events --field-selector involvedObject.name=podname</code> Find events related to a specific pod</p><p>Even if a container has a shell, you will find many of the regular utilities missing since images are usually optimized for runtime.</p><p>The [[Linux#Proc|proc]] file system can still help in such a case to find running processes etc.</p><p>Helpful debugging <code>kubectl</code> commands for most objects -</p><p><code>describe</code> Show details of a specific resource or group of resources
<code>logs</code> Print the logs for a container in a pod
<code>events</code> List events</p><p><code>attach</code> Attach to a running container
<code>exec</code> Execute a command in a container
<code>cp</code> Copy files and directories to and from containers
<code>port-forward</code> Forward one or more local ports to a pod</p><p><code>proxy</code> Run a proxy to the Kubernetes API server
<code>auth</code> Inspect authorization
<code>debug</code> Create debugging sessions for troubleshooting workloads and nodes</p><p>Problems with Nodes</p><p><code>kubectl get nodes</code> shows which nodes are available and in a ready state</p><p><code>kubectl cordon</code> - Use to mark node(s) unschedulable, can use selector. Use <code>uncordon</code> once the maintenance is done.</p><p><code>kubectl drain</code> - Prepare node for maintenance by removing running pods gracefully and marking it unschedulable for new pods.</p><p>The behaviour differs based on how the pod is started on the node -</p><ul><li>If controlled by a daemon-set, the pods are ignored! Since the daemonset controller ignores the unschedulable node state.</li><li>If controlled by deployment, replicat-set, stateful-set, job, replication controller, then drain will either evict the pods (if supported by API server), or delete them.</li><li>If there are standalone pods, these won&rsquo;t be deleted or evicted unless <code>--force</code> flag is specified.</li></ul><p>If a node is <code>NOT_READY</code>,</p><ul><li>Check if <code>kubelet</code> is running on a node.</li><li>Check networking plugin is setup properly and running</li></ul><p>Q: How to port forward to local, when running <code>kubectl</code> in docker?<br>A: start the <code>kubectl</code> container on docker, and expose a port</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>docker run -it --name kubectl -p 8000:8000 kubectl:latest
</span></span></code></pre></div><p>now run port forward as normal, but listen on <code>0.0.0.0</code> in addition to <code>localhost</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl -n workload port-forward svc/workload --address localhost,0.0.0.0 8000:8000
</span></span></code></pre></div><p>Upgrade checks</p><p>Verify api versions</p><p>On existing cluster</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl api-resources --verbs<span style=color:#f92672>=</span>list --namespaced -o name | xargs -n <span style=color:#ae81ff>1</span> kubectl get --ignore-not-found -o<span style=color:#f92672>=</span>go-template<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;{{range .items}}{{.metadata.namespace}}: {{.kind}}: {{.apiVersion}}{{&#34;\n&#34;}}{{end}}&#39;</span> -n namespace &gt;&gt; namespace-all-resources.txt
</span></span><span style=display:flex><span>cat namespace-all-resources.txt
</span></span><span style=display:flex><span>namespace: Event: events.k8s.io/v1
</span></span><span style=display:flex><span>namespace: Event: events.k8s.io/v1 <span style=color:#75715e># lists all resources, so some duplication</span>
</span></span><span style=display:flex><span>namespace: ExternalSecret: external-secrets.io/v1beta1
</span></span><span style=display:flex><span>namespace: ExternalSecret: external-secrets.io/v1beta1
</span></span><span style=display:flex><span>namespace: InMemoryChannel: messaging.knative.dev/v1
</span></span><span style=display:flex><span>namespace: Subscription: messaging.knative.dev/v1 <span style=color:#75715e># lists crd as well</span>
</span></span><span style=display:flex><span>namespace: Subscription: messaging.knative.dev/v1
</span></span><span style=display:flex><span>namespace: PodMetrics: metrics.k8s.io/v1beta1
</span></span><span style=display:flex><span>namespace: ConfigMap: v1
</span></span><span style=display:flex><span>namespace: Endpoints: v1
</span></span><span style=display:flex><span>namespace: PersistentVolumeClaim: v1
</span></span><span style=display:flex><span>namespace: Pod: v1
</span></span><span style=display:flex><span>namespace: Secret: v1
</span></span><span style=display:flex><span>namespace: ServiceAccount: v1
</span></span><span style=display:flex><span>namespace: Deployment: apps/v1
</span></span><span style=display:flex><span>namespace: ReplicaSet: apps/v1
</span></span><span style=display:flex><span>... 
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>cat namespace-all-resources.txt  | uniq | cut -f3 -d<span style=color:#e6db74>&#34;: &#34;</span> | uniq &gt;&gt; check-api-list.txt <span style=color:#75715e># or, just shorten the initial query to only get apiVersion, and skip this</span>
</span></span></code></pre></div><p>On new cluster</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl api-resources &gt;&gt; available-api-list.txt
</span></span><span style=display:flex><span><span style=color:#75715e># search for api which appears in check-api-list.txt but not in available-api-list.txt</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>while</span> read api; <span style=color:#66d9ef>do</span> echo -n <span style=color:#e6db74>&#34;</span>$api<span style=color:#e6db74> &#34;</span>; grep -cx $api available-api-list.txt; <span style=color:#66d9ef>done</span> &lt; check-api-list.txt
</span></span><span style=display:flex><span>tool/v1alpha1 <span style=color:#ae81ff>0</span> <span style=color:#75715e># not supported </span>
</span></span><span style=display:flex><span>tool/v1beta <span style=color:#ae81ff>1</span>   <span style=color:#75715e># supported</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=kubernetes-objects>Kubernetes Objects<a hidden class=anchor aria-hidden=true href=#kubernetes-objects>#</a></h2><h3 id=diagram>Diagram<a hidden class=anchor aria-hidden=true href=#diagram>#</a></h3><p>![[k8s-objects.png]]</p><h3 id=deployments>Deployments<a hidden class=anchor aria-hidden=true href=#deployments>#</a></h3><p>Adds scalability, high availability, self healing capabilities to a pod by defining replication strategy and update strategy</p><p><code>kubectl create deployment my-dep --image=nginx --replicas=3</code></p><p>Example declaration</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx-deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>2</span> <span style=color:#75715e># tells deployment to run 2 pods matching the template</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>image</span>: <span style=color:#ae81ff>nginx:1.14.2</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>containerPort</span>: <span style=color:#ae81ff>80</span>
</span></span></code></pre></div><p><code>kubectl rollout history deployments</code> provides recent rollout events including reason for change (scale out/in not included)</p><p><code>kubectl rollout history deployment/my-app</code></p><p>Rollback a failed deployment to previous version
<code>kubectl rollout undo deployment/my-app --to-revision=1</code></p><p>Check logs across all pods in a deployment</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl logs deployment/deployment-name -n namespace
</span></span><span style=display:flex><span>found x pods
</span></span></code></pre></div><h3 id=update-strategy>Update Strategy<a hidden class=anchor aria-hidden=true href=#update-strategy>#</a></h3><p>Specify <code>rollingUpdate</code> or <code>recreate</code> (can cause temporary )</p><table><thead><tr><th>Deployment</th><th><code>rollingUpdate</code></th><th><code>recreate</code></th></tr></thead><tbody><tr><td>Note</td><td>deploys new replicaset, then removes old replicaset</td><td></td></tr><tr><td>Disruption</td><td>no</td><td>yes</td></tr><tr><td>Useful for</td><td>add examples</td><td>add examples</td></tr></tbody></table><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>:
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>strategy</span>: <span style=color:#ae81ff>rollingUpdate | recreate</span>
</span></span></code></pre></div><h3 id=replicaset>ReplicaSet<a hidden class=anchor aria-hidden=true href=#replicaset>#</a></h3><p>Use labels to monitor pods. If you remove a label from the pod, see another come up within seconds, check the 1st and 2nd pod in output below.</p><pre tabindex=0><code>root@controlplane:~$ kubectl get pods --show-labels
NAME                     READY   STATUS    RESTARTS   AGE     LABELS
my-dep-7674c564c-9t2wk   1/1     Running   0          6m39s   pod-template-hash=7674c564c,test=worksok
my-dep-7674c564c-gxzb7   1/1     Running   0          6m39s   app=my-dep,pod-template-hash=7674c564c
my-dep-7674c564c-svzgv   1/1     Running   0          4s      app=my-dep,pod-template-hash=7674c564c
my-dep-7674c564c-v4mnc   1/1     Running   0          6m39s   app=my-dep,pod-template-hash=7674c564c
</code></pre><h3 id=daemonset>DaemonSet<a hidden class=anchor aria-hidden=true href=#daemonset>#</a></h3><h3 id=statefulset>StatefulSet<a hidden class=anchor aria-hidden=true href=#statefulset>#</a></h3><p>Persistence and consistent naming. Restarted pods in a statefulset use the same name.</p><h3 id=pods>Pods<a hidden class=anchor aria-hidden=true href=#pods>#</a></h3><p>Usually a group of containers, volume declarations</p><p>Smallest app building block in k8s, replicated across nodes to achieve the app&rsquo;s desired availability, scalability, performance, capacity requirements.</p><p>Smallest unit of compute that can be deployed.</p><p>A Pod is similar to a set of containers with shared namespaces and shared filesystem volumes</p><p>Offers similar isolation as [[Containers]] using cgroups, namespaces etc.</p><p>Execute a command in a container contained in the pod -
<code>kubectl exec -it &lt;podname> -c &lt;container-name> -- /bin/sh</code></p><p>There no ntworking <em>within</em> a pod. Any containers running within a pod use the same IP.</p><p>Run a single stand alone pod, change it&rsquo;s default image &ldquo;command&rdquo;, check the output using <code>kubectl logs</code>
<code>kubectl run busybox --image busybox --command -- nslookup kubernetes</code></p><p><code>kubectl run busybox --image busybox --command -- sleep 3600</code>
<code>kubectl exec busybox -it -- nslookup kubernetes</code></p><p>Double dash <code>--</code> separates the <code>kubectl</code> command from the command you want to run in the container. Use <code>-n namespace</code> immediately after <code>kubectl</code> to avoid passing this argument to the container command instead.</p><p>#ask can you do this in a single step? run a pod/container, and get the output on command line?</p><p>Find all containers within a pod</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl -n namespace get pods podname -o jsonpath<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;{.spec[&#39;containers&#39;,&#39;initContainers&#39;][*].name}&#34;</span>
</span></span></code></pre></div><p>Find resource utilization of containers within a pod</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl -n namespace top pod podname --containers
</span></span></code></pre></div><p>Display multiple fields from each container within a pod, ex - name, image and resources</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl -n namespace get pods podname -o jsonpath<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;{range .spec.containers[*]}{.name}{&#34;\t&#34;}{.image}{&#34;\t&#34;}{.resources}{&#34;\n&#34;}{end}&#39;</span>
</span></span></code></pre></div><h3 id=labels>Labels<a hidden class=anchor aria-hidden=true href=#labels>#</a></h3><p>Add identifying information to an object. This information can then be used to query and select objects. Labels help add information to objects that is relevant to users, so are useful in UI or CLI.</p><p>Labels allow users to map their own org structure on system resources. Things like environment, team etc.</p><p>A label key and value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters each.</p><p>Optionally, the key can begin with a DNS subdomain prefix and a single &lsquo;/&rsquo;, like <code>example.com/my-app</code>.</p><p>It appears under the metadata field</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>:
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>:
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>label</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>myawesomeapp</span>
</span></span></code></pre></div><p>List labels applied to a pod. By default, <code>show-labels=false</code>
<code>kubectl get pod/nginx --show-labels</code></p><p>Apply label to a pod
<code>kubectl label pod/podid newlabel=value</code></p><p>Remove an existing label from a pod
<code>kubectl label pod/podid newlabel-</code> Note the trailing <code>-</code></p><p>Update an existing label
<code>kubectl label pod/podid oldlabel=newvalue --overwrite</code> without <code>--overwrite</code> flag label is not updated</p><p>If <code>--overwrite</code> is true, then existing labels can be overwritten, otherwise attempting to overwrite a label will result in an error.</p><p>Inspect labels applied to all objects
<code>kubectl get all --all-namespaces --show-labels</code></p><p>Use Selector flag to list only resources with a specific label
<code>kubectl get all --selector app=my-dep</code></p><p>Some labels are applied automatically, example on a [[#Namespace]], <code>kubernetes.io/metadata.name=namespacename</code></p><p>If <code>--resource-version</code> is specified, then updates will use this resource version, otherwise the existing resource-version will be used. This <code>resource-version</code> available under <code>metadata.resourceVersion</code>.</p><h4 id=selector>Selector<a hidden class=anchor aria-hidden=true href=#selector>#</a></h4><p>Appears under spec</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>:
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>:
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>myawesomeapp</span>
</span></span></code></pre></div><p>Use Selector flag to list only resources with a specific label
<code>kubectl get all --selector app=my-dep</code></p><h3 id=annotations>Annotations<a hidden class=anchor aria-hidden=true href=#annotations>#</a></h3><p>Add non-identifying information/metadata to objects. Annotations cannot be used to query and select objects. Information or metadata added as annotation to objects is mostly for use by machines ex - iam role annotations in case of IRSA</p><p>Deployment versions are added as annotations to the <code>metadata</code> field in the manifest yml</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>annotations</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>deployment.kubernetes.io/revision</span>: <span style=color:#e6db74>&#34;1&#34;</span>
</span></span></code></pre></div><h3 id=labels-vs-annotations>Labels vs Annotations<a hidden class=anchor aria-hidden=true href=#labels-vs-annotations>#</a></h3><ul><li>Labels = identifying information, Annotations = non-identifying information</li><li>Labels can be used to <em>select</em> objects or collection of objects, annotations cannot be used to identify or select objects</li><li>Annotations can contain characters not allowed by labels</li></ul><table><thead><tr><th>Property</th><th>Labels</th><th>Annotations</th><th>Notes</th></tr></thead><tbody><tr><td>Identifying information</td><td>yes</td><td>no</td><td>a</td></tr><tr><td>Limited characters</td><td>yes</td><td>no</td><td>a</td></tr><tr><td>Use with selector</td><td>yes</td><td>no</td><td>a</td></tr><tr><td>User friendly</td><td>yes</td><td>no</td><td>a</td></tr></tbody></table><h3 id=namespace>Namespace<a hidden class=anchor aria-hidden=true href=#namespace>#</a></h3><ul><li>Provides isolation for resources</li><li>Some objects are namespaced scoped while others are cluster wide</li><li>Objects can have same name across namespaces, but must be unique within a namespace. So? Every service can use names like <code>frontend</code>, <code>backend</code>, <code>cache</code> without worrying about name collisions.</li><li>Hierarchical namespaces - <a href=https://github.com/kubernetes-sigs/hierarchical-namespaces/blob/v1.1.0/docs/user-guide/concepts.md>userguide</a></li></ul><h4 id=all-namespaces>All namespaces<a hidden class=anchor aria-hidden=true href=#all-namespaces>#</a></h4><p>Use <code>--all-namespace</code> and <code>-n</code> flags to work with all, or a specific namespace
<code>kubectl [verb] [resource] --all-namespaces</code>
<code>kubectl [verb] [resource] -A</code>
<code>kubectl [verb] [resource] -n namespace</code></p><h4 id=existing-namespaces>Existing namespaces<a hidden class=anchor aria-hidden=true href=#existing-namespaces>#</a></h4><p><code>kubectl get ns</code> on a fresh cluster will show these 4 existing namespaces</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-zsh data-lang=zsh><span style=display:flex><span>default           Active   48m
</span></span><span style=display:flex><span>kube-node-lease   Active   48m
</span></span><span style=display:flex><span>kube-public       Active   48m
</span></span><span style=display:flex><span>kube-system       Active   48m
</span></span></code></pre></div><h4 id=namespace-issue>Namespace Issue<a hidden class=anchor aria-hidden=true href=#namespace-issue>#</a></h4><p>When creating a [[#Service]], a corresponding DNS entry like <code>service.namespace.svc.cluster.local</code> is created. Due to this, all namespace names must be valid DNS name.</p><p>To connect to a service in the same namespace, just specifying <code>service</code> is enough. It will be resolved locally <em>within</em> the same namespace. This is useful to launch multiple environments with the same config without much modifications.</p><p>To connect to a service in a different namespace, fully qualified name <code>service.othernamespace.svc.cluster.local</code> must be used.</p><blockquote><p>[!danger]
Be careful about namespaces matching public domain names.</p><p>Suppose, a namespace is named <code>com</code>, it contains a service called <code>google</code>. The local DNS name for it will be <code>google.com.svc.cluster.local</code>. If another service, <code>foo</code> in the same namespace tries to reach the public <code>google.com</code>, it will get resolved to the local <code>google</code> service instead.</p><p>Restrict permissions to create namespaces, and use admission controllers to further enforce this.</p></blockquote><blockquote><p>[!Test]
Launch a service called landing in ai namespace, are other services in that space able to reach the public landing.ai service?</p></blockquote><p>I wasn&rsquo;t able to reproduce this behaviour :(</p><p>Update: I don&rsquo;t see this happening with the <code>busybox</code> image, BUT this can be seen with the <code>dnsutils</code> image. All properties are exactly the same between both pods, so it might be down to the OS used in each image ü§∑</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-zsh data-lang=zsh><span style=display:flex><span>$ kubectl -n ai get svc
</span></span><span style=display:flex><span>NAME      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span style=color:#f92672>(</span>S<span style=color:#f92672>)</span>   AGE
</span></span><span style=display:flex><span>landing   ClusterIP   10.97.252.177   &lt;none&gt;        80/TCP    21m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># this still resolves to the public landing.ai service</span>
</span></span><span style=display:flex><span>$ kubectl exec busybox -it -- nslookup landing.ai
</span></span><span style=display:flex><span>Server:         10.96.0.10
</span></span><span style=display:flex><span>Address:        10.96.0.10:53
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Non-authoritative answer:
</span></span><span style=display:flex><span>Name:   landing.ai
</span></span><span style=display:flex><span>Address: 35.196.113.152
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Non-authoritative answer:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># this resolves to the private landing.ai service</span>
</span></span><span style=display:flex><span>$ kubectl exec busybox -it -- nslookup landing.ai.svc.cluster.local
</span></span><span style=display:flex><span>Server:         10.96.0.10
</span></span><span style=display:flex><span>Address:        10.96.0.10:53
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Name:   landing.ai.svc.cluster.local
</span></span><span style=display:flex><span>Address: 10.97.252.177
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl exec -it busybox -- cat  /etc/resolv.conf
</span></span><span style=display:flex><span>search default.svc.cluster.local svc.cluster.local cluster.local
</span></span><span style=display:flex><span>nameserver 10.96.0.10
</span></span><span style=display:flex><span>options ndots:5
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-zsh data-lang=zsh><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl exec -it dnsutils -- nslookup launch.ai
</span></span><span style=display:flex><span>Server:         10.96.0.10
</span></span><span style=display:flex><span>Address:        10.96.0.10#53
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Name:   launch.ai.svc.cluster.local
</span></span><span style=display:flex><span>Address: 10.106.142.53
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl exec -it dnsutils -- nslookup launch
</span></span><span style=display:flex><span>Server:         10.96.0.10
</span></span><span style=display:flex><span>Address:        10.96.0.10#53
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>** server can<span style=color:#ae81ff>\&#39;</span>t find launch: NXDOMAIN
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl exec -it dnsutils -- cat /etc/resolv.conf
</span></span><span style=display:flex><span>search default.svc.cluster.local svc.cluster.local cluster.local
</span></span><span style=display:flex><span>nameserver 10.96.0.10
</span></span><span style=display:flex><span>options ndots:5
</span></span></code></pre></div><h3 id=service>Service<a hidden class=anchor aria-hidden=true href=#service>#</a></h3><p>Almost like a virtual load balancer, connected to [[#Deployments]] using [[#Labels]]</p><p>Properties - id address, target port, and endpoints, session affinity?</p><p>It connects to the nodes which run kube-proxy. kube-proxy uses iptables to connect to the pods running on the nodes.</p><p>This service object ensures, the traffic is redirected to one of the pods.</p><p><code>kubectl get svc -A</code> shows all services running in a cluster</p><p><code>kubectl expose</code> creates a service by looking up a deployment, replica set, replication controller, pod or another service by name and using the selector of the resource.</p><p><code>kubectl expose deployment nginx --port=80 --target-port=8000</code></p><p>Port vs Target Port? target port is the port on the pod that the service target, port is the port that the service exposes</p><h4 id=cluster-ip>Cluster IP<a hidden class=anchor aria-hidden=true href=#cluster-ip>#</a></h4><p>default, internal access only</p><h4 id=nodeport>NodePort<a hidden class=anchor aria-hidden=true href=#nodeport>#</a></h4><p>ties a port of the node to the node of a pod, accessible from outside the cluster</p><h4 id=loadbalancer>LoadBalancer<a hidden class=anchor aria-hidden=true href=#loadbalancer>#</a></h4><p>Public cloud load balancers</p><h4 id=externalname>ExternalName<a hidden class=anchor aria-hidden=true href=#externalname>#</a></h4><p>uses DNS names, redirection happens at DNS level</p><h4 id=service-without-selector>Service without selector<a hidden class=anchor aria-hidden=true href=#service-without-selector>#</a></h4><p>use for direct connections based on ip/port, without an endpoint. Useful for databases and within namespaces</p><p>#ask Can I not use a service for resources with no labels?</p><p>#ask What is a headless service?</p><h3 id=ingress>Ingress<a hidden class=anchor aria-hidden=true href=#ingress>#</a></h3><p>Successor [[GatewayApi]]</p><p>Provides a http route from outside the cluster to services running in the cluster. It can also handle ssl termination, load balancing and name based virtual hosting.</p><p>Example ingress sending all traffic to a single service</p><pre class=mermaid>graph LR;

client([client])-. Ingress-managed <br> load balancer .->ingress[Ingress];

ingress-->|routing rule|service[Service];

subgraph cluster

ingress;

service-->pod1[Pod];

service-->pod2[Pod];

end

classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;

classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;

classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;

class ingress,service,pod1,pod2 k8s;

class client plain;

class cluster cluster;
</pre><p>Example ingress config</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>networking.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Ingress</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>minimal-ingress</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>annotations</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>nginx.ingress.kubernetes.io/rewrite-target</span>: <span style=color:#ae81ff>/</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>ingressClassName</span>: <span style=color:#ae81ff>nginx-example</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>rules</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>http</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>paths</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/testpath</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>pathType</span>: <span style=color:#ae81ff>Prefix</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>backend</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>service</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>name</span>: <span style=color:#ae81ff>test</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>port</span>:
</span></span><span style=display:flex><span>              <span style=color:#f92672>number</span>: <span style=color:#ae81ff>80</span>
</span></span></code></pre></div><p>Ingress spec has rules which are matched against all incoming http requests, and the traffic is directed accordingly.</p><p>Ingress Annotations are often used to configure certain properties depending on the ingress controller in use.</p><p>If no <code>host</code> is specified in rules as in the example above, it matches all hosts.</p><p>Backend can also be a <code>resource</code>, but you cannot specify both <code>resource</code> and <code>service</code> for a path. <code>resource</code> backend is useful for directing requests for static assets to an object storage.</p><p><code>pathType</code> can be one of <code>Prefix</code>, <code>Exact</code>, or <code>ImplementationSpecific</code> (upto the IngressClass)</p><p>For exposing arbitraty protocols and ports, [[NodePort]] or LoadBalancer service type can be used.</p><p>An ingress resource on its own doesn&rsquo;t mean anything, it needs an [[Ingress Controller]] to be present on the cluster to provide the required functionality.</p><p>For handling TLS, the ingress spec should refer to a secret which provides the cert and secret key. For TLS to work properly, the <code>host</code> values in <code>spec.tls.hosts</code> must match <code>spec.rules.host</code>.</p><p>#find how is the ingress configured in the general eks cluster?</p><p><a href=https://aws.amazon.com/blogs/opensource/kubernetes-ingress-aws-alb-ingress-controller/>Blog post</a></p><h3 id=ingress-controller>Ingress Controller<a hidden class=anchor aria-hidden=true href=#ingress-controller>#</a></h3><p>Various options like nginx, aws alb, istio etc.</p><p>Each ingress controller implements a particular ingress class. For ex, for aws load balancer controller, it is <code>alb</code>. (<a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.6/deploy/configurations/#limiting-ingress-class>ref</a>)</p><h2 id=netowrking>Netowrking<a hidden class=anchor aria-hidden=true href=#netowrking>#</a></h2><p><a href=https://kubernetes.io/docs/concepts/cluster-administration/networking/>Offical docs</a>
<a href=https://github.com/kubernetes/design-proposals-archive/blob/main/network/networking.md>Design doc</a></p><p>Node contains pods which is controlled by a deployment, each pod has an IP. But</p><p>Service is connected to deployment using label</p><p>IP is a pod property, not container property, <code>kubectl describe pod</code> shows the IP assigned to a pod, or use <code>kubectl get pods -o wide</code></p><p>4 major problems -</p><ol><li>container to container communication - handled by [[##Pods|pod]], <code>localhost</code> communication</li><li>pod to pod communication - explained below</li><li>pod to service communication - handled by [[#Service|services]]</li><li>external to service communication - handled by [[#Service|services]]</li></ol><h3 id=plugin>Plugin<a hidden class=anchor aria-hidden=true href=#plugin>#</a></h3><p>When changing a network plugin - ensure the network cidr stays the same</p><h2 id=dns>DNS<a hidden class=anchor aria-hidden=true href=#dns>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-zsh data-lang=zsh><span style=display:flex><span><span style=color:#75715e># service</span>
</span></span><span style=display:flex><span>NAMESPACE     NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT<span style=color:#f92672>(</span>S<span style=color:#f92672>)</span>                  AGE
</span></span><span style=display:flex><span>kube-system   kube-dns     ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   33s
</span></span><span style=display:flex><span><span style=color:#75715e>#pods</span>
</span></span><span style=display:flex><span>NAMESPACE      NAME                                   READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>kube-system    coredns-5dd5756b68-2l28z               1/1     Running   <span style=color:#ae81ff>0</span>          33s
</span></span><span style=display:flex><span>kube-system    coredns-5dd5756b68-t55kw               1/1     Running   <span style=color:#ae81ff>0</span>          <span style=color:#ae81ff>33</span>
</span></span></code></pre></div><p>What objects get dns names?</p><ol><li>[[#Service]] <code>service.namespace.svc.cluster.local</code></li><li>[[#Pods]] <code>pod-ipv4.namespace.pod.cluster.local</code></li></ol><p>Each pod has a dns policy defined under <code>pod.spec.dnsPolicy</code>, value is either of</p><ul><li><code>Default</code>, inherits from the node</li><li><code>ClusterFirst</code>, any query not matching cluster domain is forwarded to upstream DNS servers.</li><li><code>ClusterFirstWithHostNet</code>, for pods running with <code>hostNetwork: true</code>.</li><li>None, specify dns configs under <code>pod.spec.dnsConfig</code></li></ul><p>Note: <code>default</code> is NOT the default dns policy. If no policy is used <code>ClusterFirst</code> is used.</p><p>do an nsloop on <code>kubernetes</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ kubectl run busybox --image busybox --command -- nslookup kubernetes
</span></span><span style=display:flex><span>pod/busybox created
</span></span><span style=display:flex><span>$ kubectl logs pod/busybox
</span></span><span style=display:flex><span>Server:         10.96.0.10
</span></span><span style=display:flex><span>Address:        10.96.0.10:53
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>** server can<span style=color:#e6db74>&#39;t find kubernetes.cluster.local: NXDOMAIN
</span></span></span><span style=display:flex><span><span style=color:#e6db74>** server can&#39;</span>t find kubernetes.cluster.local: NXDOMAIN
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Name:   kubernetes.default.svc.cluster.local
</span></span><span style=display:flex><span>Address: 10.96.0.1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>** server can<span style=color:#e6db74>&#39;t find kubernetes.svc.cluster.local: NXDOMAIN
</span></span></span><span style=display:flex><span><span style=color:#e6db74>** server can&#39;</span>t find kubernetes.svc.cluster.local: NXDOMAIN
</span></span></code></pre></div><p>This provides the ip of the kubernetes service which can be verified using <code>kubectl describe svc/kubernetes</code></p><p>Note: lookup only works within the namespace. Outside the namespace, you won&rsquo;t get the result!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ kubectl run dnsnginx --image busybox --command -- nslookup nginx
</span></span><span style=display:flex><span>pod/dnsnginx created
</span></span><span style=display:flex><span>$ kubectl run dnskube --image busybox --command -- nslookup kube-dns
</span></span><span style=display:flex><span>pod/dnskube created
</span></span><span style=display:flex><span>$ 
</span></span><span style=display:flex><span>$ kubectl logs pod/dnsnginx
</span></span><span style=display:flex><span>Server:         10.96.0.10
</span></span><span style=display:flex><span>Address:        10.96.0.10:53
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>** server can<span style=color:#e6db74>&#39;t find nginx.cluster.local: NXDOMAIN
</span></span></span><span style=display:flex><span><span style=color:#e6db74>** server can&#39;</span>t find nginx.cluster.local: NXDOMAIN
</span></span><span style=display:flex><span>** server can<span style=color:#e6db74>&#39;t find nginx.default.svc.cluster.local: NXDOMAIN
</span></span></span><span style=display:flex><span><span style=color:#e6db74>** server can&#39;</span>t find nginx.default.svc.cluster.local: NXDOMAIN
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl -n nginx run dnsnginx --image busybox --command -- nslookup nginx 
</span></span><span style=display:flex><span>pod/dnsnginx created
</span></span><span style=display:flex><span>root@controlplane:~$ kubectl logs pod/dnsnginx -n nginx
</span></span><span style=display:flex><span>Server:         10.96.0.10
</span></span><span style=display:flex><span>Address:        10.96.0.10:53
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Name:   nginx.nginx.svc.cluster.local
</span></span><span style=display:flex><span>Address: 10.99.230.232
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>** server can<span style=color:#e6db74>&#39;t find nginx.cluster.local: NXDOMAIN
</span></span></span><span style=display:flex><span><span style=color:#e6db74>** server can&#39;</span>t find nginx.cluster.local: NXDOMAIN
</span></span></code></pre></div><p>Why is that? Check the dns config <em>inserted</em> into a pod -</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ kubectl -n nginx exec -it dnsnginx -- /bin/sh
</span></span><span style=display:flex><span>/ <span style=color:#75715e># cat /etc/resolv.conf </span>
</span></span><span style=display:flex><span>search nginx.svc.cluster.local svc.cluster.local cluster.local
</span></span><span style=display:flex><span>nameserver 10.96.0.10
</span></span><span style=display:flex><span>options ndots:5
</span></span></code></pre></div><p>The name server <code>10.96.0.10</code> points to the <code>kube-dns</code> service running in the <code>kube-system</code> namespace</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ kubectl get svc -A
</span></span><span style=display:flex><span>NAMESPACE     NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span style=color:#f92672>(</span>S<span style=color:#f92672>)</span>                  AGE
</span></span><span style=display:flex><span>default       kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP                  52m
</span></span><span style=display:flex><span>kube-system   kube-dns     ClusterIP   10.96.0.10      &lt;none&gt;        53/UDP,53/TCP,9153/TCP   52m
</span></span><span style=display:flex><span>nginx         nginx        ClusterIP   10.99.230.232   &lt;none&gt;        80/TCP                   43m
</span></span></code></pre></div><p>Q: How to connect to a service running in namespace B if it can&rsquo;t be queried from pods in namespace A?
A: Service name can be queried using the format <code>servicename.namespace</code> from any namespace in the cluster</p><p><a href=https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/>https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/</a></p><h2 id=storage>Storage<a hidden class=anchor aria-hidden=true href=#storage>#</a></h2><p><code>kubectl explain pod.spec.volumes</code> shows the different volume types that are available for use.</p><h3 id=volumes>Volumes<a hidden class=anchor aria-hidden=true href=#volumes>#</a></h3><p>Volumes can be ephermal or persistent.</p><p>To use a volume within a pod&rsquo;s containers, you need to specify <code>spec.volumes</code> and <code>spec.containers[*].volumeMounts</code>. The container so created sees the data contained in the image + any data mounted as a volume.</p><p>Specified for a pod in <code>spec.volumes</code>, to check all the available configuration options, use <code>kubectl explain pod.spec.volumes</code></p><p>Volume types were cloud specific which have now been deprecated in favor of 3rd party [[##storage drivers]] instead. The following volume types are still valid -</p><ul><li>Secret (always mounted as RO, don&rsquo;t use as <a href=https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath>subpath</a> to receive updates)</li><li>ConfigMap (always mounted as RO, don&rsquo;t use as <a href=https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath>subpath</a> to receive updates)</li><li>Local, Empty Dir, Host Path relate to local filesystems of the node.</li><li>PVC</li><li>Projected</li><li>Downward API - check <code>coredns</code> pods</li></ul><pre class=mermaid>graph LR;
subgraph pod
 subgraph container1
  m1[volMount]
 end
 subgraph container2
  m2[volMount]
 end
 subgraph volumes
  v[vol]
 end
end
subgraph storage
 pv[pv]
end
subgraph claim
 pvc[pvc]
end

pv --bound--> pvc
v --> m1
v --> m2
pvc --> v
</pre><p>PV <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes/>Persistent Volumes</a> decouple the storage requirements from pod development.
PV use properties like accessModes, capacity, mountOptions, pvreclaimPolicy, volumeMode etc to mount the persistent volume to the pod.</p><p>PV can be created manually (manifest) or dynamically (using a storage class)</p><p><a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes>Access Modes</a> can be one of the following</p><ul><li>ReadWriteOnce (RWO) - A single node can mount this volume as read write. Many pods on this node can still use the volume.</li><li>ReadOnlyMany (ROX) - Many pods can mount the volume as read only.</li><li>ReadWriteMany (RWX) - Many pods can mount the volume as read, write.</li><li>ReadWriteOncePod (RWOP) - A single pod can mount the volume as read, write (version v1.22 onwards only).</li></ul><p>PVC Persistent volume claims are used by pod authors to add storage needs in a declarative way, without worrying about storage specifics.</p><p>PVC use properties like accessModes, volumeMode, storageClassName, resources, selector to provision the storage as per the requirements.
<code>kubectl explain pvc.spec</code> to know about all the properties.</p><p>Simple local example</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#75715e># pv.yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>PersistentVolume</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>pv-vol</span> <span style=color:#75715e># not used anywhere</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>type</span>: <span style=color:#ae81ff>local</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>accessModes</span>:
</span></span><span style=display:flex><span>   - <span style=color:#ae81ff>ReadWriteOnce</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>capacity</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>storage</span>: <span style=color:#ae81ff>2Gi</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>hostPath</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>path</span>: <span style=color:#e6db74>&#34;/data&#34;</span> <span style=color:#75715e># this should exist on host</span>
</span></span><span style=display:flex><span><span style=color:#75715e># pvc.yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>PersistentVolumeClaim</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>pv-claim</span> <span style=color:#75715e># used in pod.spec.volumes[].pvc.claimName</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>accessModes</span>:
</span></span><span style=display:flex><span>   - <span style=color:#ae81ff>ReadWriteOnce</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>storage</span>: <span style=color:#ae81ff>1Gi</span> <span style=color:#75715e># &lt;= pv.spec.capacity.storage</span>
</span></span><span style=display:flex><span><span style=color:#75715e># pod.yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>pv-pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>pv-container</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>containerPort</span>: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginxhttp</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>mountPath</span>: <span style=color:#e6db74>&#34;/usr/share/nginx/html&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cvol</span> <span style=color:#75715e># from pod.spec.volumes[].name</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cvol</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>persistentVolumeClaim</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>claimName</span>: <span style=color:#ae81ff>pv-claim</span> <span style=color:#75715e># from pvc.metadata.name</span>
</span></span></code></pre></div><p>#ask what happens if <code>pvc.spec.requests.storage</code> > <code>pv.spec.capacity.storage</code> ?</p><h3 id=storage-class>Storage Class<a hidden class=anchor aria-hidden=true href=#storage-class>#</a></h3><ul><li>can be grouped according to anything - capacity, type, location etc.</li><li>Uses <code>spec.provisioner</code> to connect to the storage</li><li>When a PVC does not specify a¬†<code>storageClassName</code>, the default StorageClass is used.</li><li>The cluster can only have one default StorageClass. If more than one default StorageClass is set, the newest default is used.</li></ul><p>Example</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>storage.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>StorageClass</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>standard</span>
</span></span><span style=display:flex><span><span style=color:#f92672>provisioner</span>: <span style=color:#ae81ff>kubernetes.io/aws-ebs</span>
</span></span><span style=display:flex><span><span style=color:#f92672>parameters</span>: <span style=color:#75715e># provisioner specific parameters</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>gp2</span>
</span></span><span style=display:flex><span><span style=color:#f92672>reclaimPolicy</span>: <span style=color:#ae81ff>Retain</span>
</span></span><span style=display:flex><span><span style=color:#f92672>allowVolumeExpansion</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span><span style=color:#f92672>mountOptions</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>debug</span>
</span></span><span style=display:flex><span><span style=color:#f92672>volumeBindingMode</span>: <span style=color:#ae81ff>Immediate</span>
</span></span></code></pre></div><h2 id=configmap>ConfigMap<a hidden class=anchor aria-hidden=true href=#configmap>#</a></h2><p>Decouple configuration from application</p><p>example, notice it uses <code>data</code> instead of the usual <code>spec</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ConfigMap</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span><span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginxcm</span>
</span></span><span style=display:flex><span><span style=color:#f92672>data</span>:
</span></span><span style=display:flex><span>  <span style=color:#75715e># use in `pod.spec.volumes[].configMap.items[].key`</span>
</span></span><span style=display:flex><span>	<span style=color:#f92672>nginx-custom-config.conf</span>: <span style=color:#ae81ff>| </span>
</span></span><span style=display:flex><span>		<span style=color:#ae81ff>server {</span>
</span></span><span style=display:flex><span>			<span style=color:#ae81ff>listen 8080;</span>
</span></span><span style=display:flex><span>			<span style=color:#ae81ff>server_name localhost;</span>
</span></span><span style=display:flex><span>			<span style=color:#ae81ff>location / {</span>
</span></span><span style=display:flex><span>				<span style=color:#ae81ff>root /usr/share/nginx/html;</span>
</span></span><span style=display:flex><span>				<span style=color:#ae81ff>index index.html index.htm;</span>
</span></span><span style=display:flex><span>			}
</span></span><span style=display:flex><span>		}
</span></span></code></pre></div><p>Use it in a pod</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span> <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span> <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>     - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>conf</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/etc/nginx/conf.d/</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>   - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>conf</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>configMap</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginxcm</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>items</span>:
</span></span><span style=display:flex><span>         <span style=color:#75715e># key as in configMap.data.key</span>
</span></span><span style=display:flex><span>       - <span style=color:#f92672>key</span>: <span style=color:#ae81ff>nginx-custom-config.conf</span>
</span></span><span style=display:flex><span>         <span style=color:#75715e># path within the container</span>
</span></span><span style=display:flex><span>         <span style=color:#f92672>path</span>: <span style=color:#ae81ff>default.conf</span>
</span></span></code></pre></div><h3 id=secrets>Secrets<a hidden class=anchor aria-hidden=true href=#secrets>#</a></h3><p>Decouple sensitive variables from application</p><p>example - notice it uses <code>data</code> instead of the usual <code>spec</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Secret</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>secret</span>
</span></span><span style=display:flex><span><span style=color:#f92672>data</span>:
</span></span><span style=display:flex><span>	<span style=color:#f92672>username</span>: <span style=color:#ae81ff>encodedusername</span>
</span></span><span style=display:flex><span>	<span style=color:#f92672>password</span>: <span style=color:#ae81ff>encodedpassword</span>
</span></span></code></pre></div><h2 id=kubernetes-api>Kubernetes API<a hidden class=anchor aria-hidden=true href=#kubernetes-api>#</a></h2><p>Collection of [[RESTful APIs]], supports GET, POST, DELETE. It is crucial to identify api version to use.</p><p>#ask why did kubernetes project choose this RESTful API approach?</p><p>To allow the system to continuously evolve and grow.</p><p>New features can be easily added without impacting existing clients as alpha, and moved to beta, then stable version as they mature.</p><p>It also allows the project to maintain compatibility with existing clients by offering both beta and stable version of an API simultaneously (for a length of time).</p><p>Versioning is done at the API level rather than at the resource or field level to ensure that the API presents a clear, consistent view of system resources and behavior, and to enable controlling access to end-of-life and/or experimental APIs.</p><p>The API server handles the conversion between API versions transparently: all the different versions are actually representations of the same persisted data. The API server may serve the same underlying data through multiple API versions.</p><p>So, if I create a resource using an API version <code>v1beta1</code>, I can later use <code>v1</code> version to query or manage it (within the deprecation period). Some fields may need updating due to the API graduating to <code>v1</code>, but, I can still migrate to the newer version of the API without having to destroy and recreate the resource.</p><p>API versions cannot be removed in future versions until this <a href=https://github.com/kubernetes/kubernetes/issues/52185>issue</a> is fixed.</p><p>API access is controlled by the API server.</p><p>It saves the serialized objects in [[etcd]].</p><p>API resources are distinguished by their API group, resource type, namespace (for namespaced resources), and name.</p><p>Monitor deprecated API requests - <code>apiserver_requested_deprecated_apis</code> metric. This can help identify if there are objects in the cluster still using deprecated APIs.</p><p>#ask kube-proxy, where is it hosted, host it works?</p><pre class=mermaid>graph LR
subgraph server
api[api/etcd]
end

cr[curl] --> kp[kube-proxy] --> api
</pre><p>List available resource APIs, their kind, groups, version, namespaced (bool), version, any shortnames etc, use
<code>kubectl api-resources -o wide</code></p><p><a href=https://kubernetes.io/docs/reference/using-api/#api-groups>API groups</a> can be¬†<a href=https://kubernetes.io/docs/reference/using-api/#enabling-or-disabling>enabled or disabled</a> using <code>--runtime-config</code> flag on API server</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-zsh data-lang=zsh><span style=display:flex><span>$ kubectl api-resources
</span></span><span style=display:flex><span>NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
</span></span><span style=display:flex><span>..
</span></span><span style=display:flex><span>configmaps                        cm           v1                                     true         ConfigMap
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>namespaces                        ns           v1                                     false        Namespace
</span></span><span style=display:flex><span>nodes                             no           v1                                     false        Node
</span></span><span style=display:flex><span>persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
</span></span><span style=display:flex><span>persistentvolumes                 pv           v1                                     false        PersistentVolume
</span></span><span style=display:flex><span>pods                              po           v1                                     true         Pod
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>secrets                                        v1                                     true         Secret
</span></span><span style=display:flex><span>serviceaccounts                   sa           v1                                     true         ServiceAccount
</span></span><span style=display:flex><span>services                          svc          v1                                     true         Service
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>networking.k8s.io/v1                   false        IngressClass
</span></span><span style=display:flex><span>ingresses                         ing          networking.k8s.io/v1                   true         Ingress
</span></span><span style=display:flex><span>networkpolicies                   netpol       
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>List versions of available API
<code>kubectl api-versions</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-zsh data-lang=zsh><span style=display:flex><span>$ kubectl api-versions
</span></span><span style=display:flex><span>..
</span></span><span style=display:flex><span>apps/v1
</span></span><span style=display:flex><span>authentication.k8s.io/v1
</span></span><span style=display:flex><span>authorization.k8s.io/v1
</span></span><span style=display:flex><span>autoscaling/v1
</span></span><span style=display:flex><span>autoscaling/v2
</span></span><span style=display:flex><span>..
</span></span><span style=display:flex><span>flowcontrol.apiserver.k8s.io/v1beta2 flowcontrol.apiserver.k8s.io/v1beta3
</span></span><span style=display:flex><span>networking.k8s.io/v1
</span></span><span style=display:flex><span>node.k8s.io/v1
</span></span><span style=display:flex><span>policy/v1
</span></span><span style=display:flex><span>rbac.authorization.k8s.io/v1
</span></span><span style=display:flex><span>scheduling.k8s.io/v1
</span></span><span style=display:flex><span>storage.k8s.io/v1
</span></span><span style=display:flex><span>v1
</span></span></code></pre></div><p>Find properties required for an object, see <code>kubectl</code> commands section above
<code>kubectl explain &lt;object.property></code></p><p>Examples -</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl explain externalsecrets             <span style=color:#75715e># explanation only for top level properties</span>
</span></span><span style=display:flex><span>kubectl explain externalsecrets --recursive <span style=color:#75715e># no explanation, prints the complete schema</span>
</span></span><span style=display:flex><span>kubectl explain externalsecrets.spec.target <span style=color:#75715e># explanation for a specific property</span>
</span></span></code></pre></div><p>Find namespace scoped APIs or cluster wide APIs
<code>kubectl api-resources --namespaced=true</code>
<code>kubectl api-resources --namespaced=false</code></p><p>Proxy <code>kubectl</code> to access the API more easily using [[Curl]]</p><p>#ask But why would you do this?
If an app needs to interact with kubernetes, it can simply use the language specific http library to do this directly instead of going through <code>kubectl</code></p><p><code>kubectl proxy --port=8080</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-zsh data-lang=zsh><span style=display:flex><span>$ curl http://localhost:8080/version
</span></span><span style=display:flex><span><span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;major&#34;</span>: <span style=color:#e6db74>&#34;1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;minor&#34;</span>: <span style=color:#e6db74>&#34;28&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;gitVersion&#34;</span>: <span style=color:#e6db74>&#34;v1.28.2&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;gitCommit&#34;</span>: <span style=color:#e6db74>&#34;89a4ea3e1e4ddd7f7572286090359983e0387b2f&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;gitTreeState&#34;</span>: <span style=color:#e6db74>&#34;clean&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;buildDate&#34;</span>: <span style=color:#e6db74>&#34;2023-09-13T09:29:07Z&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;goVersion&#34;</span>: <span style=color:#e6db74>&#34;go1.20.8&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;compiler&#34;</span>: <span style=color:#e6db74>&#34;gc&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;platform&#34;</span>: <span style=color:#e6db74>&#34;linux/amd64&#34;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><p>Get pods from <code>kube-system</code> namespace (truncated output)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-zsh data-lang=zsh><span style=display:flex><span>$ curl http://localhost:8080/api/v1/namespaces/kube-system/pods | les
</span></span><span style=display:flex><span><span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;kind&#34;</span>: <span style=color:#e6db74>&#34;PodList&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;apiVersion&#34;</span>: <span style=color:#e6db74>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;metadata&#34;</span>: <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;resourceVersion&#34;</span>: <span style=color:#e6db74>&#34;1555&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;coredns-5dd5756b68-fcz42&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;generateName&#34;</span>: <span style=color:#e6db74>&#34;coredns-5dd5756b68-&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;namespace&#34;</span>: <span style=color:#e6db74>&#34;kube-system&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;uid&#34;</span>: <span style=color:#e6db74>&#34;326ba1b7-31b6-4d6c-9978-1057f6734154&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;resourceVersion&#34;</span>: <span style=color:#e6db74>&#34;553&#34;</span>,
</span></span><span style=display:flex><span>..
</span></span></code></pre></div><p>Check the openapi v3 specification (truncated output) on <code>/openapi/v3</code>, and v2 specification on <code>/openapi/v2</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-zsh data-lang=zsh><span style=display:flex><span>$ curl http://localhost:8080/openapi/v3
</span></span><span style=display:flex><span><span style=color:#f92672>{</span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;paths&#34;</span>: <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;.well-known/openid-configuration&#34;</span>: <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;serverRelativeURL&#34;</span>: <span style=color:#e6db74>&#34;/openapi/v3/.well-known/openid-configuration?hash=4488--&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;api&#34;</span>: <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;serverRelativeURL&#34;</span>: <span style=color:#e6db74>&#34;/openapi/v3/api?hash=929E--&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;api/v1&#34;</span>: <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;serverRelativeURL&#34;</span>: <span style=color:#e6db74>&#34;/openapi/v3/api/v1?hash=5133--&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;apis&#34;</span>: <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;serverRelativeURL&#34;</span>: <span style=color:#e6db74>&#34;/openapi/v3/apis?hash=27E0--&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;apis/admissionregistration.k8s.io&#34;</span>: <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;serverRelativeURL&#34;</span>: <span style=color:#e6db74>&#34;/openapi/v3/apis/admissionregistration.k8s.io?hash=E8D5..&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><h3 id=api-extensions>API Extensions<a hidden class=anchor aria-hidden=true href=#api-extensions>#</a></h3><p><a href=https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/>Custom Resources</a></p><p>This is a way to make the API server recognize new non-standard Kubernetes objects.</p><p>Example - Prometheus Operator uses a number of CRDs to manage the deployment in a cluster.</p><p><a href=https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/>Aggregation Layer</a></p><p>Needs to be enabled and then runs in-process in the <code>kube-apisever</code>.</p><p>You first need to create an APIService object, say <code>myawesomeapi</code>, at a path, say <code>apis/myawesomeapi/v1beta1/</code>. The aggregation layer then proxies any requests API server receives for this API to the registered APIService.</p><p>Example - <a href=https://github.com/kubernetes-sigs/metrics-server>metrics server</a></p><h2 id=create-a-cluster-manually>Create a Cluster Manually!<a hidden class=anchor aria-hidden=true href=#create-a-cluster-manually>#</a></h2><p>Kubernetes releases before v1.24 included a direct integration with Docker Engine, using a component named¬†<em>dockershim</em>.</p><p>What is <em>dockershim</em>?</p><p>To provide support for multile container runtimes, <a href=https://kubernetes.io/docs/concepts/architecture/cri/>CRI API</a>/ specification was developed. But since docker was the first container runtime k8s supported, and to maintain backward compatibility, <code>dockershim</code> was developed which allowed kubelet to interact with docker runtime via the CRI API, sort of like a proxy?</p><pre class=mermaid>graph LR;
kb[kubelet] <--cri--> ds[dockershim] <--> dc[docker] <--> cd[containerd] --> c1[container 1]
cd --> c2[container 2]
cd --> cn[container n]
</pre><pre class=mermaid>graph LR;
kb[kubelet] <--cri--> ccd[cri-containerd] <--> cd[containerd] --> c1[container 1]
cd --> c2[container 2]
cd --> cn[container n]
</pre><p>Create a 3 node cluster - 1 control node, and 2 worker nodes.</p><ol><li>On control node, <code>kubeadm init</code></li><li>On control node, Networking</li><li>On worker node, <code>kubeadm join</code></li></ol><p>Control node</p><ol><li>Install a container runtime like <code>docker</code>, <code>containerd</code>, <code>cri-o</code>.</li><li>Install kube tools like <code>kubeadm</code>, <code>kubelet</code>, <code>kubectl</code></li><li><code>kubeadm init</code> <a href=https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/>Ref</a></li><li>Setup <code>$HOME/.kube/config</code></li><li>Verify all hosts are present under <code>/etc/hosts</code></li><li>Install a pod network add-on - any one of calico, cilium, flannel etc.</li></ol><p>Worker nodes</p><ol><li>Install a container runtime like <code>docker</code>, <code>containerd</code>, <code>cri-o</code>.</li><li>Install kube tools like <code>kubeadm</code>, <code>kubelet</code>, <code>kubectl</code></li><li><code>kubeadm join --token xx --discovery-cert xx</code></li></ol><p>To create high availability - use 3 controller nodes, each running with etcd, or use a dedicated etcd cluster</p><h2 id=older-needs-refining>Older, needs refining<a hidden class=anchor aria-hidden=true href=#older-needs-refining>#</a></h2><p>Pod Disruption Budgets <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb/>https://kubernetes.io/docs/tasks/run-application/configure-pdb/</a>
Based on the value of <code>maxUnavailable</code> for specific pods, cluster autoscaler will either ignore a node, or scale it down.</p><p>Pod Affinity <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/>https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/</a> uses pod <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/>labels</a></p><p>example</p><pre tabindex=0><code>apiVersion: v1
kind: Pod
metadata:
  name: label-demo
  labels:
    environment: production
    app: nginx
spec: . . .
</code></pre><p>Pods Anti Affinity <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity</a></p><p><a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/>labels</a> allow us to use selectors</p><p>labels are also useful to slice/dice resources when using <code>kubectl</code></p><pre tabindex=0><code>kubectl get pods -Lapp -Ltier -Lrole
</code></pre><p><code>-L</code> displays an extra column in <code>kubectl</code> output
<code>-l</code> either selects or update the label applied to a resourse.</p><p>Selectors can be of 2 types</p><p>Equality based (<code>accelerator=nvidia-tesla-p100</code>)</p><pre tabindex=0><code>apiVersion: v1
kind: Pod
metadata:
  name: cuda-test
spec:
  containers:
    - name: cuda-test
      image: &#34;registry.k8s.io/cuda-vector-add:v0.1&#34;
      resources:
        limits:
          nvidia.com/gpu: 1
  nodeSelector:
    accelerator: nvidia-tesla-p100
</code></pre><p>Set based</p><pre tabindex=0><code>apiVersion: v1
kind: Pod
metadata:
  name: cuda-test
spec:
  containers:
    - name: cuda-test
      image: &#34;registry.k8s.io/cuda-vector-add:v0.1&#34;
      resources:
        limits:
          nvidia.com/gpu: 1
  nodeSelector:
    environment: qa,qa1 # and condition
    accelerator in (nvidia, intel)
</code></pre><p>service, replicationcontroller format for selector</p><pre tabindex=0><code>selector:
	component: redis
</code></pre><p>daemonset, replicaset, deployment, job format for selector</p><pre tabindex=0><code>selector:
	matchLabels:
		component: redis
	matchExpressions:
		- {key: component, values: [redis]}
</code></pre><h2 id=labels-1>Labels<a hidden class=anchor aria-hidden=true href=#labels-1>#</a></h2><h3 id=standard-or-default>Standard or default<a hidden class=anchor aria-hidden=true href=#standard-or-default>#</a></h3><pre tabindex=0><code>kubernetes.io/arch
kubernetes.io/hostname # cloud provider specific
kubernetes.io/os
node.kubernetes.io/instance-type # if available to kubelet
topology.kubernetes.io/region    #
topology.kubernetes.io/zone      # 
</code></pre><h1 id=labels-and-selectors>Labels and selectors<a hidden class=anchor aria-hidden=true href=#labels-and-selectors>#</a></h1><p>[[K8S Scheduling]]</p><h2 id=troubleshooting>Troubleshooting<a hidden class=anchor aria-hidden=true href=#troubleshooting>#</a></h2><h3 id=pod-error-alerts>Pod Error Alerts<a hidden class=anchor aria-hidden=true href=#pod-error-alerts>#</a></h3><pre tabindex=0><code>sum (kube_pod_container_status_waiting_reason{reason=~&#34;CrashLoopBackOff|ImagePullBackOff|ErrImagePull.+&#34;}) by (namespace, container, reason)
</code></pre><hr><h2 id=questions>Questions<a hidden class=anchor aria-hidden=true href=#questions>#</a></h2><h5 id=1-what-is-the-value-of-kubernetesiohostname-in-eks>1. What is the value of <code>kubernetes.io/hostname</code> in [[eks]]?<a hidden class=anchor aria-hidden=true href=#1-what-is-the-value-of-kubernetesiohostname-in-eks>#</a></h5><p>I know it&rsquo;s part of <a href=/>standard labels</a> #todo (link it) but, not seen this tag really on [[eks]]. Found following tags instead ü§∑</p><ul><li><code>kubernetes.io/cluster/myawesomecluster=owned</code></li><li>`aws:eks:cluster-name=myawesomecluster</li></ul><h5 id=2-how-do-pods-communicate-with-each-other-in-a-cluster>2. How do pods communicate with each other in a cluster?<a hidden class=anchor aria-hidden=true href=#2-how-do-pods-communicate-with-each-other-in-a-cluster>#</a></h5><h5 id=3-how-will-you-control-which-pod-runs-on-which-nodes>3. How will you control which pod runs on which node(s)<a hidden class=anchor aria-hidden=true href=#3-how-will-you-control-which-pod-runs-on-which-nodes>#</a></h5><p>Mix of scheduling options like node selector, affinity/anti-affinity, taints, tolerations etc.</p></div><footer class=post-footer><hr><div class=align-center><div class=slimlist-entry><h2>Topics</h2></div><ul class=post-tags><div class=slimlist-entry><li class=post-tags><a href=https://abiydv.github.io/topics/k8s/>k8s</a></li><li class=post-tags><a href=https://abiydv.github.io/topics/stage-incubating/>stage/incubating</a></li><li class=post-tags><a href=https://abiydv.github.io/topics/containers/>containers</a></li></div></ul></div></footer><script src=https://utteranc.es/client.js repo=abiydv/abiydv.github.io issue-term=title label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2019-2025 <a href=https://abiydv.github.io/>@abiydv</a> / </span><span>Published 4.4.25 /
</span><span>Thanks <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>, <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>