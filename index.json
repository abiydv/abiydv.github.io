[{"content":"What is Observability? Ability to understand the health of a system by observing it\u0026rsquo;s key signals. For humans, the most basic signal is a heartbeat.\nSignals can vary from system to sytem. For a high traffic online store, the latency users experience, or the time it takes to confirm a purchase, can be a key signal. For a batch workload, the amount of records it processes over a specific duration can be a key signal.\nInstrumenting the system is a key component of Observability.\nMetrics Prometheus Thanos Logs Use structured logging. Plenty of language specific libraries available to make it easy. Evaluating an app\u0026rsquo;s logging during performance tests can provide some actionable improvements - too many, or too few entries, swallows important info, wrong classification of severity etc.\nLoki Searches through labels describing the source of the logs rather than the log extry itself (free text search)\nAvoid high cardinality labels, best practices is a good place to start Add high cardinality fields (ex - timestamp, transactionid) into structured metadata rather than labels. Promtail, Alloy can do this while scraping the logs Visualization Grafana Alerts Avoid alert fatigue! No action, no need for an alert\nAlert Manager Links OpenTelemetry ","permalink":"https://abiydv.github.io/notes/observability/","summary":"What is Observability? Ability to understand the health of a system by observing it\u0026rsquo;s key signals. For humans, the most basic signal is a heartbeat.\nSignals can vary from system to sytem. For a high traffic online store, the latency users experience, or the time it takes to confirm a purchase, can be a key signal. For a batch workload, the amount of records it processes over a specific duration can be a key signal.","title":"Observability"},{"content":"Introduction Typically, read only data for business intelligence queries. Companies have multiple different OLTP databases. Data is Extracted from these, Transformed, and Loaded into a data warehouse (OLAP). Why not use OLTP directly for analytics queries? Access controls on business critical data Analytics queries often need aggregation, leads to performance issues when reading large amounts of data. Suitable for column oriented storage. Each record has hundreds of columns, but most queries only read 3 or 4 columns at most. Use bitmap indexes for queries on data (bitmap encoded). Vectorized processing Schemas Both schemas typically depend on Fact (wide, 100s columns) and Dimension (less wide) tables. Fact tables are records of transactions (at a retailer, for ex), and dimension tables contain further metadata about fields in a record like product (brand, supplier, category), customer (name, address, phone), price (sale price, purchase price, promotions) etc.\nStar Snowflake Examples AWS Redshift Refer https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-techniques-for-amazon-redshift/ ","permalink":"https://abiydv.github.io/notes/data-warehouse/","summary":"Introduction Typically, read only data for business intelligence queries. Companies have multiple different OLTP databases. Data is Extracted from these, Transformed, and Loaded into a data warehouse (OLAP). Why not use OLTP directly for analytics queries? Access controls on business critical data Analytics queries often need aggregation, leads to performance issues when reading large amounts of data. Suitable for column oriented storage. Each record has hundreds of columns, but most queries only read 3 or 4 columns at most.","title":"Data Warehouse"},{"content":"What is a service mesh? Examples Istio Uses Envoy\nSidecar or Ambient mode\nhttps://istio.io/latest/docs/concepts/traffic-management/\nComponents Virtual Service Destination Rule Gateway Service Entry (can register external services) How does Traffic flow through Istio? Some important components to be aware of, in order\nIngress Gateway Gateway Virtual Service Service Entry Deployment App pod proxy container App pod app container Istio Ingress Gateway (configures the cloud load balancer)\napiVersion: v1 kind: Service metadata: annotations: meta.helm.sh/release-name: istio-ingressgateway meta.helm.sh/release-namespace: istio-system service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http service.beta.kubernetes.io/aws-load-balancer-internal: \u0026#34;true\u0026#34; service.beta.kubernetes.io/aws-load-balancer-name: nlb-istio service.beta.kubernetes.io/aws-load-balancer-ssl-cert: .. service.beta.kubernetes.io/aws-load-balancer-ssl-ports: https service.beta.kubernetes.io/aws-load-balancer-type: nlb .. finalizers: - service.kubernetes.io/load-balancer-cleanup labels: app: istio-ingressgateway app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: istio-ingressgateway app.kubernetes.io/version: 1.20.0 helm.sh/chart: gateway-1.20.0 istio: ingressgateway name: istio-ingressgateway namespace: istio-system .. spec: .. selector: app: istio-ingressgateway istio: ingressgateway .. type: LoadBalancer Istio Gateway\napiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: annotations: name: app-gateway namespace: app-namespace spec: selector: istio: ingressgateway # maps to istio ingress gateway defined above servers: - hosts: - app-name.example.com port: name: http number: 80 protocol: HTTP tls: httpsRedirect: true - hosts: - app-name.example.com port: name: https number: 443 protocol: HTTPS # can be http tls: credentialName: app-name-cert # do i need this? mode: SIMPLE Istio Virtual Service\napiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: app-name namespace: app-namespace spec: gateways: - app-gateway # refers to the gateway created above hosts: - app-name.example.com http: - route: - destination: host: app-name port: number: 80 Logging Deploy this to enable access logging on a specific app. Ref\napiVersion: telemetry.istio.io/v1alpha1 kind: Telemetry metadata: name: app-access-logs namespace: app-namespace spec: selector: matchLabels: service.istio.io/canonical-name: app-name accessLogging: - providers: - name: envoy Log response body for a particular app using lua http filters.\napiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: app-resp-log namespace: app-namespace spec: workloadSelector: labels: service.istio.io/canonical-name: app-name configPatches: - applyTo: HTTP_FILTER match: context: ANY listener: filterChain: filter: name: envoy.filters.network.http_connection_manager patch: operation: INSERT_BEFORE value: name: envoy.lua typed_config: \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\u0026#34; inlineCode: | function envoy_on_response(response_handle) local body = response_handle:body() local jsonString = tostring(body:getBytes(0, body:length())) response_handle:logErr(\u0026#34;Status: \u0026#34;..response_handle:headers():get(\u0026#34;:status\u0026#34;)) response_handle:logErr(\u0026#34;BodyJSONString: \u0026#34;..jsonString) end Modify log level on running containers. Ref\nkubectl -n namespace exec -i -t pod/podname -c istio-proxy -- /bin/sh $ curl -X POST http://localhost:15000/logging?level=debug ","permalink":"https://abiydv.github.io/notes/service-mesh/","summary":"What is a service mesh? Examples Istio Uses Envoy\nSidecar or Ambient mode\nhttps://istio.io/latest/docs/concepts/traffic-management/\nComponents Virtual Service Destination Rule Gateway Service Entry (can register external services) How does Traffic flow through Istio? Some important components to be aware of, in order\nIngress Gateway Gateway Virtual Service Service Entry Deployment App pod proxy container App pod app container Istio Ingress Gateway (configures the cloud load balancer)\napiVersion: v1 kind: Service metadata: annotations: meta.","title":"Service Mesh"},{"content":"Questions What is an Active Directory? It is sort of a database, and holds information about entities and their properties.\nWhy use an AD? Centralized identity management Who provides AD? Who can use AD? What is Posix? What is LDAP and DNS What is kerberos ","permalink":"https://abiydv.github.io/notes/active-directory/","summary":"Questions What is an Active Directory? It is sort of a database, and holds information about entities and their properties.\nWhy use an AD? Centralized identity management Who provides AD? Who can use AD? What is Posix? What is LDAP and DNS What is kerberos ","title":"Active Directory"},{"content":"What is git? Distributed version control system.\ngit repo history git hooks git hooks redux\nHistory Linus developed git after being frustrated with the state of version control systems in that era. He had experienced the benefits of using a version control system using BitKeeper earlier, but given the restrictive terms, and removal of free tier, he decided to build something of his own.\nFun read, this one https://blog.brachiosoft.com/en/posts/git/\nAside, add \u0026ldquo;Just for Fun: The Story of an Accidental Revolutionary\u0026rdquo; to reading list\nInternals How does git work? Why does it use filesystem instead of a database like other systems of the time? Is there a bottleneck? Can it be improved? What comes next? Useful commands Log $ git log --oneline # show one line per commit $ git log -5 --oneline # show last 5 commits $ git log -5 --pretty=\u0026#34;format:%C(white)%ai %C(yellow)%h %C(blue)%ae %C(reset)%s $ git log --pretty=\u0026#34;format:%C(white)%ai %C(yellow)%h %C(blue)%an %C(green)(%ae) %C(reset)%s\u0026#34; -6 2024-05-29 15:00:00 +0000 b06d833 Bot (bot@example.com) chore(release): 1.0.0 deployment Amend Change author name/email for commits already pushed to remote\n$ git rebase -i HEAD~4 $ git commit --amend --no-edit --reset-author \u0026lt;commit hash\u0026gt; Push git push --force-with-lease If somebody else built on top of your original history while you are rebasing, the tip of the branch at the remote may advance with their commit, and blindly pushing with \u0026ndash;force will lose their work.\nThis option allows you to say that you expect the history you are updating is what you rebased and want to replace. If the remote ref still points at the commit you specified, you can be sure that no other people did anything to the ref. It is like taking a \u0026ldquo;lease\u0026rdquo; on the ref without explicitly locking it, and the remote ref is updated only if the \u0026ldquo;lease\u0026rdquo; is still valid.\nManage multiple Git \u0026ldquo;workspaces\u0026rdquo; Work1, work2, personal etc. etc.\nSeparate the workspaces into different directories, like -\n. ├── gitlab ├── github └── workgithub Next, in ~/.gitconfig, create individual configs for each directory\n# ~/.gitconfig [user] name = username email = username@email.com [includeIf \u0026#34;gitdir/i:~/code/gitlab/\u0026#34;] path = ~/.gitlab.gitconfig [includeIf \u0026#34;gitdir/i:~/code/github/\u0026#34;] path = ~/.github.gitconfig [includeIf \u0026#34;gitdir/i:~/code/workgithub/\u0026#34;] path = ~/.workgithub.gitconfig # no need to use --set-upstream origin branch [push] autoSetupRemote = true Specify the relevant email, name in directory specific gitconfig file.\n# ~/.gitlab.gitconfig [user] name = gluser email = gluser@gitlab.com # ~/.github.gitconfig [user] email = ghuser@users.noreply.github.com name = ghuser # ~/.workgithub.gitconfig [user] email = ghworkuser@users.noreply.github.com name = ghworkuser Modify your ssh config to use the correct keys for each (don\u0026rsquo;t reuse the same keys!)\n# ~/.ssh/config Host gitlab.com User git IdentityFile ~/.ssh/glid_rsa Host ghuser.github.com User git Hostname github.com IdentityFile ~/.ssh/ghuserid_rsa Host github.com User git IdentityFile ~/.ssh/ghworkuserid_rsa Finally, when adding an origin for repos under github directory, use the HOST value. Since it is overridden by HOSTNAME, there is no other impact.\n# first git remote add origin git@ghuser.github.com:ghuser/repo.git # edit existing origin git remote set-url origin git@ghuser.github.com:ghuser/repo.git LFS Can you use webp images in git to reduce the binary file sizes? How do you convert png to webp images? References Official site - https://git-scm.com Book - https://git-scm.com/book/en/v2 ","permalink":"https://abiydv.github.io/notes/git/","summary":"What is git? Distributed version control system.\ngit repo history git hooks git hooks redux\nHistory Linus developed git after being frustrated with the state of version control systems in that era. He had experienced the benefits of using a version control system using BitKeeper earlier, but given the restrictive terms, and removal of free tier, he decided to build something of his own.\nFun read, this one https://blog.brachiosoft.com/en/posts/git/\nAside, add \u0026ldquo;Just for Fun: The Story of an Accidental Revolutionary\u0026rdquo; to reading list","title":"Git"},{"content":"Inspired by open-source AOE Tech Radar\nTechniques Name Notes Stage Conventional commits better commits ✅ Cornell Note-taking System better notes 👀 No Hello better async comms ✅ - - - Languages/Frameworks Name Notes Stage artipie 🆕 host your repository, pypi, conda etc 🔍 astral/ruff 🆕 fast py linter, formatter 👀 astral/uv 🆕 fast py package manager 👀 atlasgo manage db schemas as code 🔍 click py framework to make cli apps 🔍 fastapi Building APIs with Python 👀 gofr Golang microservice framework 👀 guardrailsai 🆕 py library for llm apps 🔍 hugo Generate static sites ✅ pkl 🆕 pickle - better configs than yaml, json 🔍 pydantic Data validation library for Python 👀 semantic-release commit based, auto version management ✅ testcontainers 🆕 Disposable containers for tests 👀 vite 🆕 Generate static sites 👀 - - - Local Tools Name Notes Stage allsky star trails using Rasberry Pi 👀 anki flash cards 🔍 brew macos pkg manager ✅ draw.io diagrams, achitectures ✅ duplicati backups 👀 excalidraw diagrams 🔍 font/firacode fancy font ✅ font/jetbrains 🆕 new fancy font 👀 ghostty 🆕 new fancy terminal 👀 k9s 🆕 k8slens in terminal ✅ lazygit 🆕 git for the lazy 🔍 logseq 🆕 outliner, only bullets, no cloud 👀 mermaid diagrams as code ✅ mob peer programming, and git handover 👀 obsidian notes app, not for me 🛑 rectangleapp arrange windows ✅ syncthing backups, file sync 👀 telepresence 🆕 local dev with k8s microservices 🔍 vscode IDE ✅ vscode/ext/dev-containers ✅ vscode/ext/gitlens ✅ - - - Workload Tools Name Notes Stage alertmanager alerting ✅ argo/argocd gitops k8s cd ✅ argo/events for event driven arch ✅ argo/workflows workflow orchestration ✅ atlantis decorate terraform pull-requests 👀 authentik open-source Identity Provider 👀 backstage internal developer platform 🔍 cartography visualize cloud infrastructure 🔍 cloud-custodian rules engine 👀 cloudevents cloud events 🔍 cortex horizontally scalable prometheus storage 👀 crossplane manage infrastructure as native k8s objects 🔍 dapr event-driven, runtime for building distributed applications 👀 devpi private pypi 🔍 external-dns sync dns records for ingress, svc ✅ external-secrets external secrets in k8s cluster ✅ grafana visualize metrics ✅ harbor open source cloud native registry, OCI only ✅ harness internal developer platform, ex-notes 🛑 hashicorp/nomad simple orchestrator 🔍 hashicorp/terraform infrastructure as code ✋ hashicorp/vault secrets management ✋ infracost surface cloud cost in cicd pipelines 👀 istio service mesh ✅ juicefs posix file system backed by redis and s3 👀 k8sgpt chatgpt for k8s 🔍 kafka event streaming 🔍 keda event driven auto-sclaing 👀 keycloak open-source identity and access management 👀 komiser visualize cloud infrastructure ✅ koperator useful to manage [kafka] 🔍 kro 🆕 manage any group of resources as one unit 👀 kyverno policies for k8s cluster 👀 linkerd2 service mesh 👀 loki logs ✅ mimir horizontally scalable prometheus storage 👀 minio s3 compatible object store ✅ opa-gatekeeper k8s admission controller ✅ open-telemetry 🔍 openbao 🆕 vault, but open-source 👀 openllmetry monitor your LLM app 👀 opentofu 🆕 terraform, but open-source 👀 ormb ^New^ use oci compliant image registry for models 🔍 prefect workflow orchestration 🛑 prometheus tsdb ✅ ray 🆕 scale mlops 👀 rook storage orchestrator - file, block, object 👀 sentry developer-first error tracking and performance monitoring 🔍 slurm workflow orchestration 👀 thanos scalable prometheus storage ✅ traefik proxy 👀 volcano k8s batch workloads 👀 - - - References Icon Status 🆕 added recently 🔍 bookmark 👀 try ✅ adopt ✋ contain, retire 🛑 avoid Extended Notes Harness open-source Limited tools available - git, ci pipelines, artifact registry Artifact registry only supports docker, helm No plugins? No way to pull app logs etc ","permalink":"https://abiydv.github.io/notes/radar/","summary":"Inspired by open-source AOE Tech Radar\nTechniques Name Notes Stage Conventional commits better commits ✅ Cornell Note-taking System better notes 👀 No Hello better async comms ✅ - - - Languages/Frameworks Name Notes Stage artipie 🆕 host your repository, pypi, conda etc 🔍 astral/ruff 🆕 fast py linter, formatter 👀 astral/uv 🆕 fast py package manager 👀 atlasgo manage db schemas as code 🔍 click py framework to make cli apps 🔍 fastapi Building APIs with Python 👀 gofr Golang microservice framework 👀 guardrailsai 🆕 py library for llm apps 🔍 hugo Generate static sites ✅ pkl 🆕 pickle - better configs than yaml, json 🔍 pydantic Data validation library for Python 👀 semantic-release commit based, auto version management ✅ testcontainers 🆕 Disposable containers for tests 👀 vite 🆕 Generate static sites 👀 - - - Local Tools Name Notes Stage allsky star trails using Rasberry Pi 👀 anki flash cards 🔍 brew macos pkg manager ✅ draw.","title":"Radar"},{"content":"Types Remote vs Local Media HDD SSD RAID What is a \u0026ldquo;RAID\u0026rdquo;? Redundant Array of Independent Disks\nWhat problem does it solve?\nProblems with RAID?\nCommon Types of RAID RAID mirroring striping parity drives# failures## 0 - - - 2 0 1 y - - 2 1 5 - block dist 3 1 6 - block double dist 4 2 10 y block # Minimum number of drives required to create this RAID array ## Number of drives that can fail without data loss\nRAID 0 atleast 2 drives typically 2 hard drives of equal capacity block level striping, no mirroring, no parity doubles write speed by writing to 2 drives alternatively any failed drive can cause data loss RAID 1 atleast 2 drives mirrored 2 hard drives of equal capacity mirroring, but no striping, no parity either drive can fail without data loss write performance hit due to writing to 2 hard disks, so not good for high performance writes read performance similar to raid 0, data can be read from any drive RAID 5 atleast 3 drives, typically 5 block level striping, distributed parity provides higher capacity than 1, and better redundancy than 0 array rebuild needs all data to be read, and can cause other drive failures 1 drive can fail without data loss RAID 6 atleast 4 drives, typically 5 similar to 5, but 2 drives are dedicated for redundancy block level striping, double distributed parity 2 drives can fail without data loss performance hit until a failed drive is replaced. why? Software RAID - ZFS (RAID-Z3) RAID 10 combination of 1 and 0, typically 4 hard drives striped set of mirrored hard drives benefits of both - redundancy and performace twice the cost Questions What is the definition of storage? Is cache a storage? Is file storage? Why or why not? What is \u0026ldquo;stripe\u0026rdquo; re: RAID? What are \u0026ldquo;parity bits\u0026rdquo;? Simple form of error detecting code Even (total 1 bits are even) or Odd (total 1 bits are odd) Calculate parity for 1001, even = 1, odd = 0 RAIDs recreate data using XOR gate What is a \u0026ldquo;block\u0026rdquo;? ","permalink":"https://abiydv.github.io/notes/storage/","summary":"Types Remote vs Local Media HDD SSD RAID What is a \u0026ldquo;RAID\u0026rdquo;? Redundant Array of Independent Disks\nWhat problem does it solve?\nProblems with RAID?\nCommon Types of RAID RAID mirroring striping parity drives# failures## 0 - - - 2 0 1 y - - 2 1 5 - block dist 3 1 6 - block double dist 4 2 10 y block # Minimum number of drives required to create this RAID array ## Number of drives that can fail without data loss","title":"Storage"},{"content":"Introduction Service control policies(SCPs) are a type of organization policy that you can use to centrally manage maximum available permissions for the IAM users and IAM roles in your organization. Since they are usually applied over a group of accounts, it is best practice to test changes in a sandbox environment/account. SCPs are often used to restrict actions as per standards applied by a company.\nBehaviour-Driven Development(BDD) is a test driven software development philosophy. TDD generally involves writing tests before actually writing code. In BDD, tests are written using natural language to validate the expected outcome of a system. Domain specific language using natural language constructs, makes it easy to read for software developers and business users alike.\nI had previously written about using BDD to make reliable deployments to Fastly. In this post, we will use BDD to reliably deploy Organization policies (service control and tag policies) to an entire AWS Organization or group of AWS accounts.\nProblem With growing complexity in AWS environments, SCPs are becoming increasingly important for organizations to apply guardrails, to ensure all member accounts adhere to best practices and established standards. As they are applied on a group of accounts at a time, any unintended side-effect due to a change can have massive impact. This makes testing, a crucial part of deploying any SCPs. AWS recommends testing these policies thoroughly in a sandbox environment before applying them to live accounts.\nRunning a suite of tests for every change in the policy, however minor, can be a time consuming process, if done manually. While it may be easy to verify the changes manually when the policies are short, it does not scale well as the size (and complexity) of these policies increases.\nExample Let\u0026rsquo;s consider the following requirements -\nEnsure all instances have 3 tags - dept, cost and proj. cost tag value is one of 001,002 or 003. Block any EC2 from using a public IP unless it has a tag exempt: public-ip-control In the next few sections, we will develop tests, and the policies to achieve this goal.\nSolution BDD tests (or specifications) are written in plain-text and use basic syntax rules called Gherkin. Cucumber reads and executes these specs, and validates the outcome is as expected.\nTest Scenarios For the example requirements (1,2) defined above, let\u0026rsquo;s write a basic feature test with some scenarios.\nFeature: EC2 instances must have basic tags Scenario: Block ec2 with no tags Given I want to launch 1 ec2 instance And use subnet \u0026#34;private-subnet-1\u0026#34; in vpc \u0026#34;my-vpc\u0026#34; And add tags: | key | value | add_to_resources | When I launch the ec2 instance Then the response is \u0026#34;UnauthorizedOperation\u0026#34; Scenario: Allow ec2 with basic tags Given I want to launch 1 ec2 instance And use subnet \u0026#34;private-subnet-1\u0026#34; in vpc \u0026#34;my-vpc\u0026#34; And add tags: | key | value | add_to_resources | | Name | first | instance, network-interface, volume | | dept | blue | instance, network-interface, volume | | cost | 001 | instance, network-interface, volume | | proj | one | instance, network-interface, volume | When I launch the ec2 instance Then the response is \u0026#34;OK\u0026#34; The first scenario checks if you don\u0026rsquo;t specify any tags, you are unable to launch an instance. The second scenario checks if you do specify the basic tags you are to launch the instance just fine. Full feature files are here.\nTest Specifications After the tests, we need to write an implementation of these scenarios, which will make AWS API calls to validate the behaviour.\nIn this example, we use godog, the official Cucumber BDD framework for Golang. It merges specification and test documentation into one cohesive whole, using Gherkin formatted scenarios. Their example section also provides an easy to follow quickstart guide.\ngodog helpfully creates the scaffolding around your tests, so you can easily fill in the gaps. Create a minimal go module like below, and execute go test to get the initial scaffolding.\n// ec2_test.go package main import ( \u0026#34;testing\u0026#34; \u0026#34;github.com/cucumber/godog\u0026#34; ) func InitializeScenario(ctx *godog.ScenarioContext) {} func TestFeatures(t *testing.T) { suite := godog.TestSuite{ ScenarioInitializer: InitializeScenario, Options: \u0026amp;godog.Options{ Format: \u0026#34;pretty\u0026#34;, Paths: []string{\u0026#34;features\u0026#34;}, TestingT: t, // Testing instance that will run subtests. }, } if suite.Run() != 0 { t.Fatal(\u0026#34;non-zero status returned, failed to run feature tests\u0026#34;) } } For reference, full specifications are here.\nDevelop Organization Policies With our tests in place, we can now create the policies to meet the requirements. For the example problem statements defined above, we need to use both, a service control and tag policy. Example policies are here.\nExecute Tests Once you are happy with the specifications, you can run through the scenarios same way as you would execute go tests - go test. Remember to provide AWS credentials as explained here. The easist method, while testing, is to simply export the credentials as environment variables. Best to use a sandbox environment during the testing phase.\nIt\u0026rsquo;s good to execute these tests before applying any organization policy to establish a baseline and tweak the tests accordingly. Depending on the state your sandbox environment (and permissions), there is a possiblity that some of the negative tests might succeed. This is because, missing IAM permissions will also result in the AWS API returning an error. Therefore, it\u0026rsquo;s important to distinguish between the type of error. See this function, which checks for different types of errors. This function fails the test if it encounters a permission error.\nAfter an initial test, deploy the organization policies to a sandbox environment via your usual method. Execute these tests again post deployment, to validate whether the policy is working as expected. If not, tweak the policy as necessary. This is an iterative process - you will need to repeat these steps a few times to get the desired result.\nContinous Integration Test runs can (rather, should) be incorporated into your continuous integration pipeline, to run as part of pull-request checks. This will ensure changes do not have any unintended consequences. Once reviewed, and approved, a pull-request merge can trigger deployment of the polcies across larger groups of AWS accounts, or even the entire AWS Organization.\nTo take it a step further, you can even run these tests parallely against all accounts the policies are applied to. If these fail for any reason, trigger a rollback.\nSource Code You can refer to this repo which contains a complete working example - a service control, and tag policy, as well as some BDD tests. It does not contain the continous integration pipeline, to deploy the policies or run the tests. But, it should be relatively straight forward to configure one, based on the steps, we have already discussed in the previous sections.\nCaveats As with most solutions, this isn\u0026rsquo;t a silver bullet, but rather a starting point to make changes more reliable and predictable. It\u0026rsquo;s good to be mindful of the following caveats when working with this pattern.\nTests can succeed or fail due to lack of permissions. Ensure you do baseline tests on existing infrastructure, as well as distinguish between different errors that can be returned by AWS API. For example, here is a list of errors returned by the EC2 API. This example uses DryRun flag available for RunInstances action. This may not be the case for every resource. If this isn\u0026rsquo;t available, tests will have to create actual resources, and that means managing the whole lifecycle of such resources. Tests need AWS account credentials, which will need to be setup in every account you want to run the tests in. In light of the above caveats, it may not always be possible to encode and automate such tests to cover the service control or tag policy in their entirety. There might be an element of manual testing that is still required.\nClosing Thoughts Service control (and tag) policies are a great way to establish guardrails across an AWS Oganization and drive best practices. However, it is essential to test the changes before deploying them to live accounts. Introducing a BDD methodology helps make these changes more reliable, predictable and less likely to have unintended side effects.\nHope this post has given you some ideas on how to adapt this into your own environment. Please comment below if you want to add something or have any questions.\nReferences (6) Handling Errors\u0026nbsp; Orgs Manage Policies\u0026nbsp; #What Is Gherkin\u0026nbsp; #What Is Cucumber\u0026nbsp; Godog\u0026nbsp; Aws Scp Bdd\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/aws-scp-test-bdd/","summary":"Introduction Service control policies(SCPs) are a type of organization policy that you can use to centrally manage maximum available permissions for the IAM users and IAM roles in your organization. Since they are usually applied over a group of accounts, it is best practice to test changes in a sandbox environment/account. SCPs are often used to restrict actions as per standards applied by a company.\nBehaviour-Driven Development(BDD) is a test driven software development philosophy.","title":"Service Control Policies and BDD"},{"content":"Pre-requisites Full access with Business/Enterprise support plan, otherwise full access to service limit checks, and 6 security checks.\nThis applies to accounts in an org as well. If the management account has Enterprise access, but the member account only has a Basic support plan, full checks for the member account will not be available.\nFeatures Checks are refreshed automatically every week, you can force a manual refresh as well for all or a specific check (api/console) Configure upto 50 reports with region/OU filters etc. Priority Focus on the \u0026ldquo;important\u0026rdquo; stuff. There are huge amount of recommendations to go through otherwise\nSources of Priority recommendations - AWS services like Security Hub, Trusted Advisor, Well-Architected Admin team can add priority recommendations manually from the Organization management account ","permalink":"https://abiydv.github.io/notes/aws/trusted-advisor/","summary":"Pre-requisites Full access with Business/Enterprise support plan, otherwise full access to service limit checks, and 6 security checks.\nThis applies to accounts in an org as well. If the management account has Enterprise access, but the member account only has a Basic support plan, full checks for the member account will not be available.\nFeatures Checks are refreshed automatically every week, you can force a manual refresh as well for all or a specific check (api/console) Configure upto 50 reports with region/OU filters etc.","title":"Trusted Advisor"},{"content":"Background I set this site up back in Feb 2019, probably as an experiment to see what GitHub pages was all about. Don\u0026rsquo;t recall what the verdict was, but I was just sitting on it for the next year or so.\nI moved over to Hugo and PaperMod in Nov 2020, and have stayed put ever since. I upgraded the versions just once in the last 4 years.\nOver this period, I built up quite a list of things-to-do. But, revamping the site always remained a backlog item, until earlier this month.\nUpdates Basics First The first order of business, before diving into any of the customizations, was to upgrade Hugo, PaperMod, and GitHub actions to their latest versions. There were a few hiccups, but hey, this site project is still very much a pet, so some effort is a given!\nCustomizations PaperMod is a nice, clean and minimal theme out of the box (\u0026#x1f64c; kudos to all the contributors!). I didn\u0026rsquo;t have any particular reason to look around, as I was pretty happy with it. However, I still wanted to simplify it some more. Things like a post\u0026rsquo;s metadata, listing of posts on pages like Home, Posts, Notes (more on this specific section later) etc.\nApart from a new Notes section, some other changes on the site are, a new simplified post list, search and posts now show updated timestamp.\nPosts vs Notes Like almost everyone else, I have probably taken thousands of notes over the years. Unfortunately, I never really paid much attention to where I was saving them. A lot of these from the inital years got lost with job changes since they were mostly local to the work machine. Later down the road, I added some to git gists. But, I always found myself coming back to local notes simply due to their ease of use.\nThe more I thought about it, the more I started leaning towards Git as the solution which could help me centralize them, as well as offering other benefits like backups, version control, syncing between devices etc. Using VS Code, I can easily work with a local copy of these notes.\nI hear ya! but, why publish \u0026rsquo;em? You ask.\nI don\u0026rsquo;t recall how, but one fine day, I found myself reading Maggie\u0026rsquo;s post about something referred to, as a \u0026ldquo;Digital Garden\u0026rdquo;. Reading through it, I realized I wanted to write more, but often ended up with a rough outline of a post, the main ideas etc, but that\u0026rsquo;s where it then remained. I couldn\u0026rsquo;t get myself to complete these posts in a way which would make them an acceptable \u0026ldquo;blog post\u0026rdquo;. By nature, most of these unpublished notes were imperfect, and ever evolving. I found myself going back to them often, when I found something useful, that was related to the original idea. At times, I even split them out as more information was added, and a new direction emerged.\nProbing further, I wanted to understand, what was it that was making me write these posts? Was it to keep up with the \u0026ldquo;tech\u0026rdquo; world or something for future me? Admittedly, there was an element of the former when I first started this personal blog. But, over the past few years, most of the posts have been the latter kind. Something I thought would prove useful for future me.\nIf that\u0026rsquo;s the case, then these posts need not really follow the personal blog template, I reasoned, and strictly speaking, these aren\u0026rsquo;t even posts anymore. But, just notes!\nPublishing these notes here helps make them available to me (and anyone else interested in the topic), and also triggers what Maggie calls \u0026ldquo;Learning in Public\u0026rdquo;. Secondly, (and this relates to another of Maggie\u0026rsquo;s post) with the proliferation of Gen AI, the web is probably going to drown in well-written and polished blog posts. Imcomplete, and imperfect notes then, will help distinguish humans from the bots in the not-so-distant future.\nPublishing Notes Once I had decided to publish my half-baked ideas, my obvious thought was to publish it via GitHub pages, same as this site, albeit through a different repo. Such a setup would allow me to choose a more purpose built theme for notes. Something which will be able to showcase how they link to each other, and help create a mindmap of sorts. However, I quickly dropped that idea seeing my past performance on this site. I can\u0026rsquo;t expect my future self to keep maintaining 2 different sites, when my past self couldn\u0026rsquo;t find the time to maintain 1!\nSo, I started researching on what Hugo offers in terms of essentially maintaining 2 different types of content under the same site. Thankfully, Hugo\u0026rsquo;s documentation is pretty straight forward, and their discourse is a great place to ask for help. I realized, all I needed was to create a new section on the site, and also a different taxonomy that wouldn\u0026rsquo;t interfere with the existing one for posts (tags). Maybe, future me will merge both these together and still find a way to meaningfully display both types of content! \u0026#x1f91e;\nAnyway, with most of the steps identified, it was time to set the ball rolling. I reckon it took me about a weekend\u0026rsquo;s worth of tinkering to get the site to where it is now. And probably the same to write this all up!\nClosing Thoughts Phew, that was a loong post!\nTo be honest, it didn\u0026rsquo;t feel as long when I was actually making these changes, or even writing about them. I kinda enjoyed the process. Well, did I mention, I love tinkering? Haha, maybe that explains it.\nAnyway, with these new changes, I hope to externalize more things and make \u0026rsquo;em more useful in the longer run.\nThanks for stopping by!\n","permalink":"https://abiydv.github.io/posts/fresh-paint-notes/","summary":"Background I set this site up back in Feb 2019, probably as an experiment to see what GitHub pages was all about. Don\u0026rsquo;t recall what the verdict was, but I was just sitting on it for the next year or so.\nI moved over to Hugo and PaperMod in Nov 2020, and have stayed put ever since. I upgraded the versions just once in the last 4 years.\nOver this period, I built up quite a list of things-to-do.","title":"Fresh coat of paint!"},{"content":"Introduction traditional runtime env is physical/virutal host\nself contained, no dependency problems\nportable, platform independent\nOCI Open containers initiative ensures compatibility between containers created in different container engine env\nArchitecture build on top of linux kernel features\nnamespaces provide strict isolation at kernel level\ncgroups provide resource allocation to ensure dedicated resources to containers\nselinux helps enforce security, can also secure access to containers from the kernel\nContainer Images Built in multiple layers of filesystem\nImmutable - any changes go into a new image\nUnionFS allows container to see simple merged file system\nMost frequently changes should be on the top layers to speed up docker build\nContainer Solutions LXE Podman [[Docker]] 2013 Made containers big due to registry, dockerhub\nNetworking Ports Each container gets an IP from the network it is attached to.\nissue, publishing ports as -p 8080:8080 opens them to the world, prefer to use -p 127.0.0.1:8080:8080\nHosts within the same L2 segment (for example, hosts connected to the same network switch) can reach ports published to localhost. For more information, see moby/moby#45610\nDNS Containers inherit the DNS settings of the host.\nContainers DON\u0026rsquo;T inherit entries defined in /etc/hosts, use --add-host flag in docker run to specify container specific host entries.\nContainers in custom network, use docker embedded DNS server, and forward external DNS queries to DNS as defined on host.\nContainer\u0026rsquo;s hostname defaults to it\u0026rsquo;s ID in docker, but can be overridden with --hostname.\nDNS servers can be specified on per container basis using the --dns, --dns-search, --dns-opt flags with docker run\n","permalink":"https://abiydv.github.io/notes/containers/","summary":"Introduction traditional runtime env is physical/virutal host\nself contained, no dependency problems\nportable, platform independent\nOCI Open containers initiative ensures compatibility between containers created in different container engine env\nArchitecture build on top of linux kernel features\nnamespaces provide strict isolation at kernel level\ncgroups provide resource allocation to ensure dedicated resources to containers\nselinux helps enforce security, can also secure access to containers from the kernel\nContainer Images Built in multiple layers of filesystem","title":"Containers"},{"content":"Introduction Ciphers Algorithms, workflows Symmetric Asymmetric References As an outside-the-box approach, some organizations choose to encrypt sensitive data via code, processing the data while it is in the web servers and before transmitting it to the next tier. Access to the decryption keys is granted to specific application servers that handle decryption requests. This scheme can be extended further, to the user side, by encrypting the sensitive data field using a public key in the client-side code before transmitting it to the web server. The organization can then control access to the private key and thereby ensure that the data is encrypted during transmission all the way through to the application component that is authorized to decrypt the data.\n","permalink":"https://abiydv.github.io/notes/cryptography/","summary":"Introduction Ciphers Algorithms, workflows Symmetric Asymmetric References As an outside-the-box approach, some organizations choose to encrypt sensitive data via code, processing the data while it is in the web servers and before transmitting it to the next tier. Access to the decryption keys is granted to specific application servers that handle decryption requests. This scheme can be extended further, to the user side, by encrypting the sensitive data field using a public key in the client-side code before transmitting it to the web server.","title":"Cryptograpghy"},{"content":"Curl is an invaluable tool for anyone who needs to debug webapps fequently. Given it\u0026rsquo;s vast amount of options, it is easy to miss some of it\u0026rsquo;s simplest yet powerful uses.\ncurl cheatsheet\n","permalink":"https://abiydv.github.io/notes/curl/","summary":"Curl is an invaluable tool for anyone who needs to debug webapps fequently. Given it\u0026rsquo;s vast amount of options, it is easy to miss some of it\u0026rsquo;s simplest yet powerful uses.\ncurl cheatsheet","title":"Curl"},{"content":"Introduction Dockerfile FROM =\u0026gt; a base image to use while building image ADD =\u0026gt; copy from local to container RUN =\u0026gt; execute commands to generate the image, this does not run when container starts CMD [\u0026#34;/usr/bin/nmap\u0026#34;, \u0026#34;-s\u0026#34;, \u0026#34;172.17.0.0/24\u0026#34;] =\u0026gt; default command to run at container startup, if not specified, container starts and stops immediately Docker cli docker build . -t myawesomeimage:v1 docker run myawesomeimage:v1 pull image myawesomeimage:v1, and run container docker run -d nginx run a container in background docker run -it ubuntu:latest /bin/bash open an interactive shell within the container ctrl-p, ctrl-q does not kill the container started with it flag docker inspect container_name json output of all properties of the container docker inspect \u0026lt;container id\u0026gt; --format='{{.NetworkSettings.IPAddress}}' filter json output from inspect command to get a specific property docker --help docker start start a stopped container docker ps -a show all containers, running, stopped, failed docker rmi delete/untag images docker rm remove containers bash is PID 1 in linux\nImages Find docker image id\ndocker images alpine -q ace17d5d883e Find all tags for a particular image\ndocker inspect ace17d5d883e | jq -rc \u0026#39;.[].RepoTags\u0026#39; [\u0026#34;docker.io/library/alpine:3\u0026#34;,\u0026#34;docker.io/library/alpine:latest\u0026#34;] Refine it further to display in desired formats\ndocker inspect ace17d5d883e | jq -rc \u0026#39;.[].RepoTags | join(\u0026#34; \u0026#34;)\u0026#39; docker.io/library/alpine:3 docker.io/library/alpine:latest docker inspect ace17d5d883e | jq -rc \u0026#39;[.[].RepoTags[]|split(\u0026#34;/\u0026#34;)[2]] | join(\u0026#34; \u0026#34;)\u0026#39; alpine:3 alpine:latest docker inspect ace17d5d883e | jq -rc \u0026#39;[.[].RepoTags[]|split(\u0026#34;:\u0026#34;)[1]] | join(\u0026#34; \u0026#34;)\u0026#39; 3 latest Networking each container needs an IP!\ndocker networking is created via drivers\n![[docker-nw.png]]\nDrivers Docker bridge is really a bridge that you should visualize as a physical device like like a network switch. This bridge is connected to the actual physical network. Uses NAT. Plug this in to the network switch, we have the virtual Ethernet interfaces veth. veth connects to container network interface.\noverlay software defined n/w, used in docker swarm\nbridge default\nUsually docker uses 172.17.0.0/16, which falls within the private IP range.\n~ docker network ls NETWORK ID NAME DRIVER SCOPE af439a7244c6 bridge bridge local 4b054ec242a6 host host local 076b498369ab none null local ~ docker network inspect af439a7244c6 --format=\u0026#39;{{json .IPAM.Config}}\u0026#39; [{\u0026#34;Subnet\u0026#34;:\u0026#34;172.17.0.0/16\u0026#34;,\u0026#34;Gateway\u0026#34;:\u0026#34;172.17.0.1\u0026#34;}] nmap - scan a network, https://github.com/instrumentisto/nmap-docker-image, https://nmap.org/\nnmap can also be used to inspect the docker network, and available containers.\n~ docker run --rm -it instrumentisto/nmap -sn 172.17.0.0/24 Starting Nmap 7.94 ( https://nmap.org ) at 2023-12-09 20:08 UTC Nmap scan report for 172.17.0.1 Host is up (0.000058s latency). MAC Address: 02:42:59:2C:33:19 (Unknown) Nmap scan report for 172.17.0.2 Host is up (0.000018s latency). MAC Address: 02:42:AC:11:00:02 (Unknown) Nmap scan report for 172.17.0.3 Host is up (0.000046s latency). MAC Address: 02:42:AC:11:00:03 (Unknown) Nmap scan report for f1b89c1009ab (172.17.0.4) Host is up. Nmap done: 256 IP addresses (4 hosts up) scanned in 2.08 second host mcvlan assign a mac address to a container\nnone no networking defined!\nPorts Publish ports by using the -p flag with docker run\nEach container gets an IP from the network it is attached to.\nissue, publishing ports as -p 8080:8080 opens them to the world, prefer to use -p 127.0.0.1:8080:8080\nHosts within the same L2 segment (for example, hosts connected to the same network switch) can reach ports published to localhost. For more information, see moby/moby#45610\nDNS Container\u0026rsquo;s hostname defaults to it\u0026rsquo;s ID in docker, but can be overridden with --hostname.\nContainers inherit the DNS settings of the host.\nContainers in custom network, use docker embedded DNS server, and forward external DNS queries to DNS as defined on host.\nContainers DON\u0026rsquo;T inherit entries defined in /etc/hosts, use --add-host flag in docker run to specify container specific host entries.\nDNS servers can be specified on per container basis using the --dns, --dns-search, --dns-opt flags with docker run\nStorage bind-mount mechanism\nPodman Alternative to docker\nSync time\npodman machine ssh \u0026#39;sudo hwclock -s\u0026#39; podman machine ssh \u0026#39;sudo systemctl restart chronyd.service\u0026#39; https://github.com/containers/podman/issues/11541\nReferences ","permalink":"https://abiydv.github.io/notes/docker/","summary":"Introduction Dockerfile FROM =\u0026gt; a base image to use while building image ADD =\u0026gt; copy from local to container RUN =\u0026gt; execute commands to generate the image, this does not run when container starts CMD [\u0026#34;/usr/bin/nmap\u0026#34;, \u0026#34;-s\u0026#34;, \u0026#34;172.17.0.0/24\u0026#34;] =\u0026gt; default command to run at container startup, if not specified, container starts and stops immediately Docker cli docker build . -t myawesomeimage:v1 docker run myawesomeimage:v1 pull image myawesomeimage:v1, and run container docker run -d nginx run a container in background docker run -it ubuntu:latest /bin/bash open an interactive shell within the container ctrl-p, ctrl-q does not kill the container started with it flag docker inspect container_name json output of all properties of the container docker inspect \u0026lt;container id\u0026gt; --format='{{.","title":"Docker"},{"content":"Intro Introduced at Google, statically typed compiled language. Fast, and platform independent\nDebugging A good example is available in the documentation of terraform aws provider here\nDelve A third party golang debugger\nDelve on GitHub Golang Debugging With Delve (Step-by-Step) Using the Go Delve Debugger from the command line Stop debugging Go with Println and use Delve instead ","permalink":"https://abiydv.github.io/notes/golang/","summary":"Intro Introduced at Google, statically typed compiled language. Fast, and platform independent\nDebugging A good example is available in the documentation of terraform aws provider here\nDelve A third party golang debugger\nDelve on GitHub Golang Debugging With Delve (Step-by-Step) Using the Go Delve Debugger from the command line Stop debugging Go with Println and use Delve instead ","title":"Golang"},{"content":" Diagrams as code!\nLive test - https://mermaid.live\nDocumentation - https://mermaid.js.org/intro/getting-started.html\nExamples Flowchart (Top to Bottom) --- config: --- flowchart TD A[Christmas] --Get money--\u003e B(Go shopping) B --\u003e C{Let me think} C --\u003e|One| D[Laptop] C --\u003e|Two| E[iPhone] C --\u003e|Three| F[fa:fa-car Car] Code\n--- config: --- flowchart TD A[Christmas] --Get money--\u0026gt; B(Go shopping) B --\u0026gt; C{Let me think} C --\u0026gt;|One| D[Laptop] C --\u0026gt;|Two| E[iPhone] C --\u0026gt;|Three| F[fa:fa-car Car] Flowchart (Left to Right) --- config: --- flowchart LR A[Christmas] --Get money--\u003e B(Go shopping) B --\u003e C{Let me think} C --\u003e|One| D[Laptop] C --\u003e|Two| E[iPhone] C --\u003e|Three| F[fa:fa-car Car] Code\n--- config: --- flowchart LR A[Christmas] --Get money--\u0026gt; B(Go shopping) B --\u0026gt; C{Let me think} C --\u0026gt;|One| D[Laptop] C --\u0026gt;|Two| E[iPhone] C --\u0026gt;|Three| F[fa:fa-car Car] Sankey --- config: sankey: showValues: false --- sankey-beta Agricultural 'waste',Bio-conversion,124.729 Bio-conversion,Liquid,0.597 Bio-conversion,Losses,26.862 Bio-conversion,Solid,280.322 Bio-conversion,Gas,81.144 Biofuel imports,Liquid,35 Biomass imports,Solid,35 Code\n--- config: sankey: showValues: false --- sankey-beta Agricultural \u0026#39;waste\u0026#39;,Bio-conversion,124.729 Bio-conversion,Liquid,0.597 Bio-conversion,Losses,26.862 Bio-conversion,Solid,280.322 Bio-conversion,Gas,81.144 Biofuel imports,Liquid,35 Biomass imports,Solid,35 ","permalink":"https://abiydv.github.io/notes/mermaid-diagrams/","summary":"Diagrams as code!\nLive test - https://mermaid.live\nDocumentation - https://mermaid.js.org/intro/getting-started.html\nExamples Flowchart (Top to Bottom) --- config: --- flowchart TD A[Christmas] --Get money--\u003e B(Go shopping) B --\u003e C{Let me think} C --\u003e|One| D[Laptop] C --\u003e|Two| E[iPhone] C --\u003e|Three| F[fa:fa-car Car] Code\n--- config: --- flowchart TD A[Christmas] --Get money--\u0026gt; B(Go shopping) B --\u0026gt; C{Let me think} C --\u0026gt;|One| D[Laptop] C --\u0026gt;|Two| E[iPhone] C --\u0026gt;|Three| F[fa:fa-car Car] Flowchart (Left to Right) --- config: --- flowchart LR A[Christmas] --Get money--\u003e B(Go shopping) B --\u003e C{Let me think} C --\u003e|One| D[Laptop] C --\u003e|Two| E[iPhone] C --\u003e|Three| F[fa:fa-car Car] Code","title":"Mermaid Diagrams"},{"content":"Layers Layer Name Function Devices OSI Layer 4 Application HTTP, FTP, SMB, SMTP, DHCP etc. AWS ALB L5-7 3 Transport packets, TCP, UDP Routers, nlb L4 2 Internet IPv4, IPv6, ICMP, IPSEC Switches L3 1 Link Physical medium Cables L2 OSI Open Systems Interconnection Model Why? Useful for defining standards so computers from different manufacturers could \u0026ldquo;talk\u0026rdquo; to each other.\nLayer Name Function Devices 7 Application HTTP, FTP, SMB, SMTP, DHCP etc. AWS ALB 6 Presentation MIME/ASCII (char encoding), Encryption/Decryption, Compression 5 Session 4 Transport TCP, UDP 3 Network Packets, IPv4, IPv6, ICMP, IPSEC Routers, NLB 2 Data MAC/LLC Switches 1 Physical Physical medium, CAN Cables, Ports Protocols TCP/IP Connection oriented, reliable Data order is important Duplicate data is discarded Minimal error Retry lost or discarded packets traffic congestion control UDP Connectionless, unreliable Streaming content, VoIP, dropped packets are not a concern DNS lookups RTP built on top of UDP for real time streaming PPTP The Point-to-Point Tunneling Protocol (PPTP) is an obsolete method for implementing VPN. PPTP has many well known security issues.\nPPTP uses a TCP/IP control channel and a GRE tunnel to encapsulate PPPpackets. Many modern VPNs use various forms of UDP for this same functionality. The PPTP specification does not describe encryption or authentication features and relies on the Point-to-Point Protocol being tunneled to implement any and all security functionalities.\nIPSec Internet Protocol Security (IPsec) is a secure network protocol that authenticates and encrypts packets of data to provide secure encrypted communication between two computers over an Internet Protocol network. It is used in VPNs.\nBGP Border Gateway Protocol (BGP) is a standardized exterior gateway protocol designed to exchange routing and reachability information among autonomous systems (AS) on the Internet. BGP is classified as a path-vector routing protocol and it makes routing decisions based on paths, network policies, or rule-sets configured.\nBGP used for routing within an autonomous system is called Interior Border Gateway Protocol(IBGP). In contrast, the Internet application of the protocol is called Exterior Border Gateway Protocol (EBGP).\nGRE Generic Routing Encapsulation (GRE) is a tunneling protocol developed by Cisco Systems that can encapsulate a wide variety of network layer protocols inside virtual point-to-point links or point-to-multipoint links over an Internet Protocol network.\nUses -\nIn conjunction with PPTP to create VPN (obsolete) In conjunction with IPSec VPN to allow passing of routing information between connected networks. IP Addresses Class Notation CIDR Classless Inter-Domain Routing\nPrivate IPs Prefixes - 8 -\u0026gt; 12 -\u0026gt; 16 Ranges - 10.0 -\u0026gt; 172.16 -\u0026gt; 192.168 CGNAT - 100.64.0.0/10\nThis is defined in [[RFC#1918]]\n10.0.0.0/8 Prefix - 8, Divide it by 8, quotient 1, remainder 0. Subnet mask - 255.0.0.0 (First octet, all bits are ON, value 255) Usable IPs - 10.0.0.1 to 10.255.255.254 Network part - 10.x.x.x Host part - x.0.0.0\n172.16.0.0/12 Prefix - 12, Divide it by 8, quotient 1, remainder 4. Subnet mask - 255.240.0.0 (First octet, all bits are ON, value 255. Second octet, 4 bits are ON, value, 128 + 64 + 32 + 16 = 240) Usable IPs - 172.16.0.1 to 172.31.255.254 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-^^ How was this 31 calculated? \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- 255 - 240 (subnet mask, second octet) = 15 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- 16 + 15 = 31 Network part - ? Host part - ?\n#ask What\u0026rsquo;s the network part, and host part in an IP like 172.16.0.0/12? ✅ 2023-12-07 It\u0026rsquo;s not really relevant for CIDR since it is classless 192.168.0.0/16 Prefix /16. Divide it by 8, quotient 1, remainder 2 Subnet mask - 255.255.0.0 (First 2 octets, are bits all ON, value 255 each) Usable IPs - 192.168.0.1 to 192.168.255.254 Network part - 192.168.x.x Host part - x.x.0.0\n100.64.0.0/10 For use in carrier-grade NAT (CGNAT) environments\nPrefix - 10, Divide it by 8, quotient 1, remainder 2. Subnet mask - 255.192.0.0 (First octet, all bits are ON, value 255. Second octet, 2 bits are ON, value, 128 + 64 = 192) Usable IPs - 100.64.0.1 to 100.127.255.254 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-^^ How was this 127 calculated? \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- 255 - 192 (subnet mask, second octet) = 63 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- 64 + 63 = 127 Network part - ? Host part - ?\n#ask What\u0026rsquo;s the network part, and host part in an IP like 100.64.0.0/10? ✅ 2023-12-07 It\u0026rsquo;s not really relevant for CIDR since it is classless Subnet Mask Convert 255 octet to binary using the following table\n128 64 32 16 8 4 2 1 1 1 1 1 1 1 1 1 128 + 64 + 32 + 16 + 8 + 4 +2 + 1 = 255\nInspect IPs 10.1.0.0 subnet mask 255.255.240.0 Prefix Prefix can be calculated from the given subnet mask. First 2 octets are fully ON (refer the binary notation above), while the third is partially ON. Add the bit values, we discover 4 bits are ON (128 + 64 + 32 + 16 = 240). So, total ON bits in the subnet mask, 8 + 8 + 4 = 20. Prefix is /20 10.1.0.0 prefix /20 Subnet Mask Subnet mask can be calculated from the given prefix. If we divide 20 by 8, we get quotient 2, and remainder 4. So, the first 2 octets in the subnet mask are fully ON, where as the third octet has 4 bits ON. Now adding the 4 bits, 128 + 64 + 32 + 16 = 240. Subnet mask is 255.255.240.0 IP addresses First IP is the same as the IP provided 10.1.0.0, but what is the last IP? Based on the subnet mask 255.255.240.0, calculate the max value of the third octet. Subtracting the current value from the max value possible, we get 255 - 240 = 15. The last octet is host specific, so can take values from 0 to 255. So the last IP in the range is 10.1.15.255. Routes \u0026ldquo;networking law\u0026rdquo;\nIt seems unlikely to me that there\u0026rsquo;d ever be any packets with a destination address within 100.64.0.0/16 as all those entities are likely fully managed by other means that, for example, you wouldn\u0026rsquo;t be SSHing to a firewall to configure it. \u0026ldquo;Networking law\u0026rdquo; says 10/8, 172.16/12, 192.168/16 and 100/16 addressed packets should never appear on the public Internet, so this route ensures they don\u0026rsquo;t get sent in the 0/0 direction and go to where they really live. Pro network admins have tight routing and filtering so they never send private packets into public space, but messy leaks do happen.\nRoute Tables DNS Address book for the internet, maps friendly names to IPs. It is a not a centralized system. The knowledge is distributed across the internet.\nWatch https://www.youtube.com/watch?v=e2xLV7pCOLI\nAn APEX domain can only have an A-Record.\nTypes of records A (A for Apex domain?) CNAME MX NS [[route53|AWS Route53]] is an AWS managed DNS solution.\nresolv.conf ref man page search directive is added to a host before quering the nameserver, if there are more than one, they are tried in sequence.\n#/etc/resolv.conf search default.svc.cluster.local svc.cluster.local cluster.local $ nslookup serviceA # queried as # serviceA.default.svc.cluster.local # and so on $ nslookup serviceA.com # queried as # serviceA.com since it contains a \u0026#39;.\u0026#39;, it is queried as is # serviceA.com.default.svc.cluster.local # and so on ndots directive specifies minimum number of dots a domain can have before the resolver queries for the absolute name. Default value is 1.\nFirewall [[network-firewall]]\nStateful or Stateless? IPS (Intrusion Prevention Service) DPI (Deep Packet Inspection) Application Protocol Detection Domain Name Filtering\nVPN GENEVE Header https://datatracker.ietf.org/doc/html/rfc8926\n#todo\nECMP Equal cost multi path routing protocol\nSD-WAN Software-defined wide area network (SD-WAN)\nIt is a wide area network that uses software-defined networking technology, such as communicating over the Internet using overlay tunnels which are encrypted when destined for internal organization locations.\nSD-WAN allows companies to build high-performance WANs using low-cost and commercially available Internet access, enabling businesses to partially or wholly replace more expensive private WAN connection technologies such as MPLS.\nHowever, when SD-WAN traffic is carried over the Internet, there are no end-to-end performance guarantees.\nSD-WANs allow companies to extend their computer networks easily over large distances, connecting remote branch offices to data centers and to each other, for critical business functions using commercial internet. They alleviate the need to establish expensive, dedicated physical network infrastructure between sites.\nReferences 1 https://www.freecodecamp.org/news/subnet-cheat-sheet-24-subnet-mask-30-26-27-29-and-other-ip-address-cidr-network-references/ ","permalink":"https://abiydv.github.io/notes/network/","summary":"Layers Layer Name Function Devices OSI Layer 4 Application HTTP, FTP, SMB, SMTP, DHCP etc. AWS ALB L5-7 3 Transport packets, TCP, UDP Routers, nlb L4 2 Internet IPv4, IPv6, ICMP, IPSEC Switches L3 1 Link Physical medium Cables L2 OSI Open Systems Interconnection Model Why? Useful for defining standards so computers from different manufacturers could \u0026ldquo;talk\u0026rdquo; to each other.\nLayer Name Function Devices 7 Application HTTP, FTP, SMB, SMTP, DHCP etc.","title":"Network"},{"content":"Linux What is Proc? File system which shows\n$ cd /prod $ ls 1 acpi cmdline diskstats fb ioports keys loadavg modules partitions slabinfo sysrq-trigger uptime zoneinfo 29 bootconfig consoles dma filesystems irq kmsg locks mounts pressure softirqs sysvipc version 30 buddyinfo cpuinfo driver fs kallsyms kpagecgroup mdstat mtrr schedstat stat thread-self version_signature 31 bus crypto dynamic_debug interrupts kcore kpagecount meminfo net scsi swaps timer_list vmallocinfo 47 cgroups devices execdomains iomem key-users kpageflags misc pagetypeinfo self sys tty vmstat $ $ cat 30/cmdline # 30 is a pid of a process nginx: worker process $ Windows ","permalink":"https://abiydv.github.io/notes/operating-systems/","summary":"Linux What is Proc? File system which shows\n$ cd /prod $ ls 1 acpi cmdline diskstats fb ioports keys loadavg modules partitions slabinfo sysrq-trigger uptime zoneinfo 29 bootconfig consoles dma filesystems irq kmsg locks mounts pressure softirqs sysvipc version 30 buddyinfo cpuinfo driver fs kallsyms kpagecgroup mdstat mtrr schedstat stat thread-self version_signature 31 bus crypto dynamic_debug interrupts kcore kpagecount meminfo net scsi swaps timer_list vmallocinfo 47 cgroups devices execdomains iomem key-users kpageflags misc pagetypeinfo self sys tty vmstat $ $ cat 30/cmdline # 30 is a pid of a process nginx: worker process $ Windows ","title":"Operating Systems"},{"content":"What is RFC? Request For Comments 🙂\nThe RFC Series (ISSN 2070-1721) contains technical and organizational documents about the Internet, including the specifications and policy documents produced by five streams: the Internet Engineering Task Force (IETF), the Internet Research Task Force (IRTF), the Internet Architecture Board (IAB), Independent Submissions, and Editorial.\nMore details - https://www.ietf.org/rfc/rfc2555.txt\nFirst ever RFC document - https://www.ietf.org/rfc/rfc1.txt\n1918 Defines private IP address ranges. 10/8 172.16/12 192.168/16 https://www.ietf.org/rfc/rfc1918.txt\n2544 Defines standard IP addresses to be used for testing network devices. The complete range specified is 198.18/15. It is further split into 198.18/16, and 198.19/16 blocks. https://www.ietf.org/rfc/rfc2544.txt\n2992 Defines Equal-cost multi-path (ECMP) routing solution. https://www.ietf.org/rfc/rfc2992.txt\n3927 Defines link local address 169.254/16 https://www.ietf.org/rfc/rfc3927.txt\n6598 Defined shared space IP 100.64/10 https://www.ietf.org/rfc/rfc6598.txt\n","permalink":"https://abiydv.github.io/notes/rfc/","summary":"What is RFC? Request For Comments 🙂\nThe RFC Series (ISSN 2070-1721) contains technical and organizational documents about the Internet, including the specifications and policy documents produced by five streams: the Internet Engineering Task Force (IETF), the Internet Research Task Force (IRTF), the Internet Architecture Board (IAB), Independent Submissions, and Editorial.\nMore details - https://www.ietf.org/rfc/rfc2555.txt\nFirst ever RFC document - https://www.ietf.org/rfc/rfc1.txt\n1918 Defines private IP address ranges. 10/8 172.16/12 192.168/16 https://www.","title":"RFC"},{"content":"Command line options -target Use this to deploy the configs conditionally if terraform is unable to generate a plan due to dynamic values.\nWhen using the docker alias, wrap additional options in '' quotes\nterraform plan \u0026#39;-target=module.management_account_permissions.aws_ssoadmin_permission_set.this[\u0026#34;MgmtSsoOperator\u0026#34;]\u0026#39; Errors Message Fix/Workaround error deleting SSO Permission Set (arn:aws:sso:::permissionSet/ssoins-xxxxxxxxxxxxxxxx/ps-xxxxxxxxxxxxxxxx): ConflictException: Could not delete because PermissionSet has ApplicationProfile associated with it. Delete permission set from account assignment via console https://github.com/hashicorp/terraform-provider-aws/issues/26757\ndocker run --rm -it -v $(pwd):/opt/app -w /opt/app --entrypoint /bin/sh public.ecr.aws/hashicorp/terraform:1.6 ","permalink":"https://abiydv.github.io/notes/terraform/","summary":"Command line options -target Use this to deploy the configs conditionally if terraform is unable to generate a plan due to dynamic values.\nWhen using the docker alias, wrap additional options in '' quotes\nterraform plan \u0026#39;-target=module.management_account_permissions.aws_ssoadmin_permission_set.this[\u0026#34;MgmtSsoOperator\u0026#34;]\u0026#39; Errors Message Fix/Workaround error deleting SSO Permission Set (arn:aws:sso:::permissionSet/ssoins-xxxxxxxxxxxxxxxx/ps-xxxxxxxxxxxxxxxx): ConflictException: Could not delete because PermissionSet has ApplicationProfile associated with it. Delete permission set from account assignment via console https://github.com/hashicorp/terraform-provider-aws/issues/26757\ndocker run --rm -it -v $(pwd):/opt/app -w /opt/app --entrypoint /bin/sh public.","title":"Terraform"},{"content":"Introduction I had written about git hooks in an earlier post. Over the years, I have come to rely quite heavily on local commit msg hook to keep my commit messages consistent with angular commit message format.\nBut, it\u0026rsquo;s a pain to set up individually for every repo. I always wanted to have a single hook that could work with all my checked out repos. Since it wasn\u0026rsquo;t a huge problem, I always put it off as a someday task. Until that someday arrived last month!\nMy laptop needed replacement, and that meant setting up the local hooks for all the repos (which I had done gradually over the years) again.\nSo, I spent a few mins (or was it hours?) to skim through git docs to find a way to do this once (and for all!). Eventually, I was able to achieve what I wanted with a few steps and this script.\nSteps Clone or download this repo https://github.com/abiydv/commit-msg\n$ cd ~/Downloads $ git clone https://github.com/abiydv/commit-msg.git Add a git template directory location in git config\n$ git config --global init.templatedir \u0026#39;~/.git/global_templates\u0026#39; Copy the downloaded commit-msg hook script to the template directory\n$ mkdir -p ~/.git/global_templates/hooks $ cp ~/Downloads/commit-msg/commit-msg ~/.git/global_templates/hooks/commit-msg Check commit-msg filesystem permissions, and make it executable.\n$ chmod +x ~/.git/global_templates/hooks/commit-msg Run git init for any repo which was initialized prior to adding this config.\nAny new repos will automatically inherit the commit-msg hook.\n.. and, that\u0026rsquo;s all there is to it! \u0026#x1f44f;\nCaveats --template cli option or GIT_TEMPLATE_DIR env var can override the template config provided in ~/.gitconfig -n or --no-verify cli option bypasses all checks. Use server side hooks to enforce compliance or other checks. Local hooks work with Gitbash for Windows, so no extra config is required for Windows. Conclusion As I mentioned in the previous post too, git local hooks can be useful to setup oversight, to make sure you don\u0026rsquo;t stray from the standards. However, they are not for setting up guardrails or enforcement as they reside on each user\u0026rsquo;s system and are completely under their control.\nReferences (2) Commit Msg\u0026nbsp; Customizing Git Git Hooks\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/git-hooks-redux/","summary":"Introduction I had written about git hooks in an earlier post. Over the years, I have come to rely quite heavily on local commit msg hook to keep my commit messages consistent with angular commit message format.\nBut, it\u0026rsquo;s a pain to set up individually for every repo. I always wanted to have a single hook that could work with all my checked out repos. Since it wasn\u0026rsquo;t a huge problem, I always put it off as a someday task.","title":"Git Hooks Redux"},{"content":"Which pod goes where?\nHow does it help? Opportunities for cost saving by utilizing better scheduling. snorkel ai case-study Restrict workloads to specific nodes. Ex - run gpu workloads on gpu nodes, or don\u0026rsquo;t run cpu-only workloads on gpu nodes. Some users want to put multiple pods that communicate with one another in the same zone to avoid inter-zone traffic charges [[aws-well-architected#AZ Affinity]]\nAnti-affinity is useful to spread pods across failure domains/topology (AZ or Region)\nWhat is a Scheduler? profiles Summary Scheduling Depends On Reference nodeName node name nodeAffinity node label nodeaffinity nodeAntiAffinity node label podAffinity pod label, node label (topology key) podaffinity podAntiAffinity pod label, node label (topology key) There are a few options to control the scheduling of pods on specific nodes described below, starting with the simplest one.\nNode Name Schedule a pod to a specific node, no questions asked. Scheduler assumes resource requirements are met. If NO node with the specified nodeName exists, the pod is NOT scheduled, might even be deleted If the named node does not have enough resources, pod fails with errors like OutOfMemory In cloud environments, nodeName is likely to be dynamic and not fixed Useful for Custom [[#What is a Scheduler?|scheduler]] or If you need to bypass any configured schedulers Property spec: nodeName: kube01 Node Selector Simplest way to control the scheduling of a pod. Add property spec/nodeSelector to the pod configuration, and specify an existing node label Can also be used to restrict use of nodes, so new workloads are not deployed to it kubelet can apply labels to nodes Be careful when using labels to restrict nodes, the node\u0026rsquo;s kubelet shouldn\u0026rsquo;t be able to apply/update the label on itself Use node restriction plugin to control this node restriction better, uses label node-restriction.kubernetes.io/ Property spec: nodeSelector: matchExpressions: - key: string operator: In, NotIn, Exist, DoesNotExist, Gt, Lt values: [string] # array is replaced during strategic merge matchFields: - key: string operator: In, NotIn, Exist, DoesNotExist, Gt, Lt values: [string] # array is replaced during strategic merge matchExpressions or matchFields statements are treated as AND statements\nkubectl explain pod.spec.affinity.nodeAffinity\nKIND: Pod VERSION: v1 FIELD: nodeAffinity \u0026lt;NodeAffinity\u0026gt; DESCRIPTION: Describes node affinity scheduling rules for the pod. Node affinity is a group of node affinity scheduling rules. FIELDS: preferredDuringSchedulingIgnoredDuringExecution \u0026lt;[]PreferredSchedulingTerm\u0026gt; The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \u0026#34;weight\u0026#34; to the sum if the node matches the corresponding matchExpressions; the node(s) with the highest sum are the most preferred. requiredDuringSchedulingIgnoredDuringExecution \u0026lt;NodeSelector\u0026gt; If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to an update), the system may or may not try to eventually evict the pod from its node. #todo test this AND/OR condition\nExample #todo test if this actually works :D\napiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: matchFields: - key: disktype operator: In values: [ssd] Node Affinity Use node labels to schedule pods on preferred nodes.\nProperty kubernetes-api/v1.24/#affinity-v1-core\nspec: affinity: nodeAffinity: podAffinity: podAntiAffinity: RDS-IDE ^^ Easier to remember version of requiredDuringSchedulingIgnoredDuringExecution. Type of [[#Node Affinity]].\nLabel specified under this property must be present on the node during scheduling. If it\u0026rsquo;s subsequently removed, the pod still continues to run.\nScheduler will try to find a node that meets the expression. If no matching node is found, pod is NOT scheduled on any other node.\nIf the expression turns to false while the pod is running, it is still allowed to complete the execution.\nProperty spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - nodeSelectorTerm1 # same as spec.nodeSelector - nodeSelectorTerm2 Multiple nodeSelectorTerm can be specified, they are treated as OR statements\nPDS-IDE ^^ Easier to remember version of preferredDuringSchedulingIgnoredDuringExecution. Type of [[#Node Affinity]]\n[[#What is a Scheduler?|Scheduler]] will try to find a node that meets the expression, and has maximum aggregate weight. If no matching node is found, it still schedules the pod on a node.\nIf the expression turns to false while the pod is running, it is still allowed to complete the execution.\nProperty spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 0-100 preference: nodeSelectorTerm # same as spec.nodeSelector\tNode Anti-Affinity There is NO dedicated configuration field for this. Node anti-affinity is achieved by inverting the node affinity by using negation operators like NotIn, DoesNotExist in the specified nodeSelectorTerm\nPod Affinity Use running pod labels to schedule pods on preferred nodes. Like, deploy mysql pods on which ever nodes has postgresql pods running.\nThis is non-symmetric - no need to check if existing pods have specified any pod affinity before scheduling pods next to them that do specify a pod affinity\nalgorithm used by podaffinity\nProperty kubernetes-api/v1.24/#affinity-v1-core\nspec: affinity: nodeAffinity: podAffinity: podAntiAffinity: #todo What\u0026rsquo;s the difference between pod affinity and node affinity? #todo What happens if all pods are deployed with a \u0026lsquo;hard\u0026rsquo; pod affinity, RDS-IDE?\nRDS-IDE ^^ Easier to remember version of requiredDuringSchedulingIgnoredDuringExecution. Type of [[#Pod Affinity]].\nLabel specified under this property must be present during scheduling. If it\u0026rsquo;s subsequently removed, the pod still continues to run.\n[[#What is a Scheduler?|Scheduler]] will try to find a node that meets the expression. If NO matching node is found, pod is NOT scheduled on any other node.\nIf the expression turns to false while the pod is running, it is still allowed to complete the execution.\nProperty spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: # label query over pods matchExpressions: - key: string operator: In, NotIn, Exist, DoesNotExist, Gt, Lt values: [string] # array is replaced during strategic merge matchLabels: key: value namespaces: string namespaceSelector: matchExpressions: - key: string operator: In, NotIn, Exist, DoesNotExist, Gt, Lt values: [string] # array is replaced during strategic merge matchLabels: key: value topologyKey: string Multiple nodeSelectorTerm can be specified, they are treated as OR statements\ntopologyKey is the key of a node label. It is used to select nodes belonging to a particular topology domain. For ex - Only launch pods in a specific AZ for access to a required volume. Or, only launch pods in a specific region.\nPDS-IDE ^^ Easier to remember version of preferredDuringSchedulingIgnoredDuringExecution. Type of [[#Pod Affinity]].\n[[#What is a Scheduler?|Scheduler]] will try to find a node that meets the expression, and has maximum aggregate weight. If NO matching node is found, it still schedules the pod on a node.\nIf the expression turns to false while the pod is running, it is still allowed to complete the execution.\nProperty spec: affinity: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 0-100 podAffinityTerm: labelSelector: # label query over pods matchExpressions: - key: string operator: In, NotIn, Exist, DoesNotExist, Gt, Lt values: [string] # array is replaced during strategic merge matchLabels: key: value namespaces: string namespaceSelector: matchExpressions: - key: string operator: In, NotIn, Exist, DoesNotExist, Gt, Lt values: [string] # array is replaced during strategic merge matchLabels: key: value topologyKey: string\tWarning! labelSelector: null can cause NO pods to match the expression, and make a pod unschedulable!\nPod Anti-Affinity There is no dedicated configuration field for this. Pod anti-affinity is achieved by inverting the pod affinity by using negation operators like NotIn, DoesNotExist in the specified podAffinityTerm\nThis is symmetric - even if a pod doesn\u0026rsquo;t specify an anti-affinity rule, it is still checked so as not to violate the anti-affinity rule specified by an already running pod.\nTaints and Tolerations taint-and-toleration Autoscaling Karpenter https://karpenter.sh\nKarpenter consolidation policies - \u0026ldquo;whenempty\u0026rdquo; Pod Disruption Budgets Karpenter can switch instance types to rightsize the nodes if pods aren\u0026rsquo;t using the node\u0026rsquo;s resources - depends on consolidation policies Karpenter AZ awarness - How does it work with EBS across different AZ? aws blogs/volume topolog -awareness Karpenter can sometimes overestimate node sizes (daemonset overheads) - karpenter/issues/715 Cluster Auto-scaler cluster-autoscaling cluster-autoscaler Horizontal Pod Autoscaler HPA Vertical Pod Autoscaler VPA Event-driven Autoscaling KEDA References ","permalink":"https://abiydv.github.io/notes/k8s-scheduling/","summary":"Which pod goes where?\nHow does it help? Opportunities for cost saving by utilizing better scheduling. snorkel ai case-study Restrict workloads to specific nodes. Ex - run gpu workloads on gpu nodes, or don\u0026rsquo;t run cpu-only workloads on gpu nodes. Some users want to put multiple pods that communicate with one another in the same zone to avoid inter-zone traffic charges [[aws-well-architected#AZ Affinity]]\nAnti-affinity is useful to spread pods across failure domains/topology (AZ or Region)","title":"K8s Scheduling"},{"content":"Package manager for all the manifests that need to be deployed to a cluster\nComponents\nchart.yaml - metadata about the chart values.yaml - values for variables exposed by the chart author templates dir - yaml manifest that define the rsources/objects to be deployed, variables are replaces with values as specified in values.yaml Search charts called database in default repository helm seach hub database\nAdd a custom repository helm add repo myrep https://myrepo.io\nInstall myapp from myrepo on a cluster, and override values for param1, param2 helm install myapp myrepo/app --set param1=value1 --set param2=value2\nAlternatives - [[kustomize]]\n","permalink":"https://abiydv.github.io/notes/k8s-helm/","summary":"Package manager for all the manifests that need to be deployed to a cluster\nComponents\nchart.yaml - metadata about the chart values.yaml - values for variables exposed by the chart author templates dir - yaml manifest that define the rsources/objects to be deployed, variables are replaces with values as specified in values.yaml Search charts called database in default repository helm seach hub database\nAdd a custom repository helm add repo myrep https://myrepo.","title":"K8s Helm Charts"},{"content":"Architecture graph LR; c1(Cluster) --\u003e n1(node1) --\u003e p11[pod 1] n1 --\u003e p12[pod 2] c1 --\u003e n2(node2) --\u003e p21[pod 1] API docs - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/\nInsert diagram about control nodes + worker nodes + api servers etc.\n#refine https://kubernetes.io/docs/concepts/architecture/\nComponents Metric Server Use HostNetwork\nHelps monitor pods Can run kubectl top pod to check resource usage for pods kubectl top nodes\nCNCF Project status graph LR; a(Sandbox\nNew) --\u003e b(Incubating\nMore wide-spread adoption, active development) --\u003e c[Graduated\nmature, stable part of k8s core] https://www.cncf.io/projects\nKubernetes Plugins? CRIO Kubernetes container runtime #readmore\nCNI Common network interface\nJaeger Kubernetes Operator, manages packaging, deploying and managing applications\nRook Storage Orchestrator\nCluster Autoscaler https://github.com/kubernetes/autoscaler\nKubernetes Distributions Rancher Red Hat OpenShift SUSE Containers as a Service Kubernetes Managed Services AWS Elastic Kubernetes Service Azure Kubernetes Service Google Kubernetes Engine Certifications CKAD Developers\nCKA Admins\nKubernetes Dashboard Link\nKubernetes Database etcd essentially a key value pair\nall k8s resources are stored in etcd in json format\njson is not very human friendly, so yaml is the de-facto choice for k8s config files, which are called manifests.\nA manifest file broadly contains -\napiVersion: # v1, v1beta1, v1beta2 etc. kind: # pod, deployment, secret, configmap etc. metadata: annotations: # used for configurations sometimes labels: selector: name: # name of the object resourceVersion: # value changes with each update data: # found in secret, and configmap objects spec: # configs, varies by object, absent for some like secret, configmaps kubectl Config file ~/.kube/config Structure of the config file, and the values that need to be specified -\napiVersion: v1 clusters: - cluster: certificate-authority-data: xxxx server: https://172.22.28.5:6443 name: kubernetes contexts: # combination of cluster, username and namespace - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: xxxx client-key-data: xxxx If this file is not present or has invalid details of a cluster, you might see an error like\n$ kubectl get all E1223 11:43:00.538822 14558 memcache.go:265] couldn\u0026#39;t get current server API group list: Get \u0026#34;http://localhost:8080/api?timeout=32s\u0026#34;: dial tcp 127.0.0.1:8080: connect: connection refused This file is usually generated with help of /etc/kubernetes/admin.conf file from the control node. This file\u0026rsquo;s user is kube admin\nCommands kubectl create deployment can\u0026rsquo;t specify replicas! incorrect! see below kubectl create deployment my-dep --image=nginx --replicas=3\nkubectl explain pod shows all the fields that are necessary to configure a pod. To deep dive into a particular property, use kubectl explain pod.Spec\nTo find out specific fields to specify for configuring [[K8S Scheduling#Node Affinity|nodeAffinity]], use kubectl explain pod.spec.affinity.nodeAffinity\nGenerate yaml from existing resources, use kubectl get \u0026lt;resource\u0026gt; -o yaml\nRemember to cleanup the output (metadata, and status) as these should be added automatically when the resource is created\nkubectl delete pod/podname --grace-period=0 --force to delete pod immediately\nTroubleshooting and Debugging When creating a pod, kubernetes first adds it to the etcd store.\nkubectl describe can highlight problems if there is an issue during this initial step. Once the pod is added to etcd, it\u0026rsquo;s then started up.\nkubectl -n namespace describe pod \u0026lt;podname\u0026gt; Containers[].State shows the current state of containers in the pod\nContainers: busybox: Container ID: containerd:// Image: busybox Image ID: docker.io/library/busybox@sha256:ver Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; State: Waiting Reason: CrashLoopBackOff Last State: Terminated Reason: Completed # this can hint at a problem where the container has exited after completing its task Exit Code: 0 Started: Sun, 10 Dec 2023 00:06:04 +0000 Finished: Sun, 10 Dec 2023 00:06:04 +0000 Ready: False Restart Count: 7 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sfpsw (ro) Events section shows any errors including any errors like CrashLoopBackOff. Latest events are at the bottom.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- ... Warning BackOff 3m24s (x48 over 13m) kubelet Back-off restarting failed container busybox in pod mydep-8677c6d8bd-8c2c6_default(72a66cb8-e29a-4be3-ac99-8f88565327f3) Once the pod is running, in addition to kubectl describe more information can be found out using the following commands.\nkubectl -n namespace get pods - high level view of pods in a namespace\nkubectl -n namespace get pod/pod-id -o yaml another way of getting similar info as kubectl describe, here the interesting field to watch is status.condition, and status.containerStatuses:\nRestart a pod?\nkubectl scale kubectl rollout restart deployment name kubectl delete pod name kunectl replace -f pod/name To find problems when a container/pod is running, use the following commands -\nkubectl -n namespace logs podname --all-containers get logs from all containers in pod podname\nkubectl -n namespace logs deployment/mydep --tail=10 -f follow logs from all pods under dep mydep\nkubectl -n namespace logs podname -c container get logs from container in pod podname\nkubectl -n namespace exec -it podname -- /bin/sh get a session into the container, if the container has a shell\nInspect last 1h events, helpful to find details about pods that don\u0026rsquo;t exist anymore kubectl -n namespace get events --sort-by='.lastTimestamp'\nPS: Node level events are displayed under -n default\nkubectl -n namespace get events --field-selector involvedObject.name=podname Find events related to a specific pod\nEven if a container has a shell, you will find many of the regular utilities missing since images are usually optimized for runtime.\nThe [[Linux#Proc|proc]] file system can still help in such a case to find running processes etc.\nHelpful debugging kubectl commands for most objects -\ndescribe Show details of a specific resource or group of resources logs Print the logs for a container in a pod events List events\nattach Attach to a running container exec Execute a command in a container cp Copy files and directories to and from containers port-forward Forward one or more local ports to a pod\nproxy Run a proxy to the Kubernetes API server auth Inspect authorization debug Create debugging sessions for troubleshooting workloads and nodes\nProblems with Nodes\nkubectl get nodes shows which nodes are available and in a ready state\nkubectl cordon - Use to mark node(s) unschedulable, can use selector. Use uncordon once the maintenance is done.\nkubectl drain - Prepare node for maintenance by removing running pods gracefully and marking it unschedulable for new pods.\nThe behaviour differs based on how the pod is started on the node -\nIf controlled by a daemon-set, the pods are ignored! Since the daemonset controller ignores the unschedulable node state. If controlled by deployment, replicat-set, stateful-set, job, replication controller, then drain will either evict the pods (if supported by API server), or delete them. If there are standalone pods, these won\u0026rsquo;t be deleted or evicted unless --force flag is specified. If a node is NOT_READY,\nCheck if kubelet is running on a node. Check networking plugin is setup properly and running Q: How to port forward to local, when running kubectl in docker?\nA: start the kubectl container on docker, and expose a port\ndocker run -it --name kubectl -p 8000:8000 kubectl:latest now run port forward as normal, but listen on 0.0.0.0 in addition to localhost.\nkubectl -n workload port-forward svc/workload --address localhost,0.0.0.0 8000:8000 Upgrade checks\nVerify api versions\nOn existing cluster\nkubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --ignore-not-found -o=go-template=\u0026#39;{{range .items}}{{.metadata.namespace}}: {{.kind}}: {{.apiVersion}}{{\u0026#34;\\n\u0026#34;}}{{end}}\u0026#39; -n namespace \u0026gt;\u0026gt; namespace-all-resources.txt cat namespace-all-resources.txt namespace: Event: events.k8s.io/v1 namespace: Event: events.k8s.io/v1 # lists all resources, so some duplication namespace: ExternalSecret: external-secrets.io/v1beta1 namespace: ExternalSecret: external-secrets.io/v1beta1 namespace: InMemoryChannel: messaging.knative.dev/v1 namespace: Subscription: messaging.knative.dev/v1 # lists crd as well namespace: Subscription: messaging.knative.dev/v1 namespace: PodMetrics: metrics.k8s.io/v1beta1 namespace: ConfigMap: v1 namespace: Endpoints: v1 namespace: PersistentVolumeClaim: v1 namespace: Pod: v1 namespace: Secret: v1 namespace: ServiceAccount: v1 namespace: Deployment: apps/v1 namespace: ReplicaSet: apps/v1 ... ... cat namespace-all-resources.txt | uniq | cut -f3 -d\u0026#34;: \u0026#34; | uniq \u0026gt;\u0026gt; check-api-list.txt # or, just shorten the initial query to only get apiVersion, and skip this On new cluster\nkubectl api-resources \u0026gt;\u0026gt; available-api-list.txt # search for api which appears in check-api-list.txt but not in available-api-list.txt while read api; do echo -n \u0026#34;$api \u0026#34;; grep -cx $api available-api-list.txt; done \u0026lt; check-api-list.txt tool/v1alpha1 0 # not supported tool/v1beta 1 # supported ... Kubernetes Objects Diagram ![[k8s-objects.png]]\nDeployments Adds scalability, high availability, self healing capabilities to a pod by defining replication strategy and update strategy\nkubectl create deployment my-dep --image=nginx --replicas=3\nExample declaration\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # tells deployment to run 2 pods matching the template template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 kubectl rollout history deployments provides recent rollout events including reason for change (scale out/in not included)\nkubectl rollout history deployment/my-app\nRollback a failed deployment to previous version kubectl rollout undo deployment/my-app --to-revision=1\nCheck logs across all pods in a deployment\nkubectl logs deployment/deployment-name -n namespace found x pods Update Strategy Specify rollingUpdate or recreate (can cause temporary )\nDeployment rollingUpdate recreate Note deploys new replicaset, then removes old replicaset Disruption no yes Useful for add examples add examples apiVersion: kind: Deployment spec: strategy: rollingUpdate | recreate ReplicaSet Use labels to monitor pods. If you remove a label from the pod, see another come up within seconds, check the 1st and 2nd pod in output below.\nroot@controlplane:~$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS my-dep-7674c564c-9t2wk 1/1 Running 0 6m39s pod-template-hash=7674c564c,test=worksok my-dep-7674c564c-gxzb7 1/1 Running 0 6m39s app=my-dep,pod-template-hash=7674c564c my-dep-7674c564c-svzgv 1/1 Running 0 4s app=my-dep,pod-template-hash=7674c564c my-dep-7674c564c-v4mnc 1/1 Running 0 6m39s app=my-dep,pod-template-hash=7674c564c DaemonSet StatefulSet Persistence and consistent naming. Restarted pods in a statefulset use the same name.\nPods Usually a group of containers, volume declarations\nSmallest app building block in k8s, replicated across nodes to achieve the app\u0026rsquo;s desired availability, scalability, performance, capacity requirements.\nSmallest unit of compute that can be deployed.\nA Pod is similar to a set of containers with shared namespaces and shared filesystem volumes\nOffers similar isolation as [[Containers]] using cgroups, namespaces etc.\nExecute a command in a container contained in the pod - kubectl exec -it \u0026lt;podname\u0026gt; -c \u0026lt;container-name\u0026gt; -- /bin/sh\nThere no ntworking within a pod. Any containers running within a pod use the same IP.\nRun a single stand alone pod, change it\u0026rsquo;s default image \u0026ldquo;command\u0026rdquo;, check the output using kubectl logs kubectl run busybox --image busybox --command -- nslookup kubernetes\nkubectl run busybox --image busybox --command -- sleep 3600 kubectl exec busybox -it -- nslookup kubernetes\nDouble dash -- separates the kubectl command from the command you want to run in the container. Use -n namespace immediately after kubectl to avoid passing this argument to the container command instead.\n#ask can you do this in a single step? run a pod/container, and get the output on command line?\nFind all containers within a pod\nkubectl -n namespace get pods podname -o jsonpath=\u0026#34;{.spec[\u0026#39;containers\u0026#39;,\u0026#39;initContainers\u0026#39;][*].name}\u0026#34; Find resource utilization of containers within a pod\nkubectl -n namespace top pod podname --containers Display multiple fields from each container within a pod, ex - name, image and resources\nkubectl -n namespace get pods podname -o jsonpath=\u0026#39;{range .spec.containers[*]}{.name}{\u0026#34;\\t\u0026#34;}{.image}{\u0026#34;\\t\u0026#34;}{.resources}{\u0026#34;\\n\u0026#34;}{end}\u0026#39; Labels Add identifying information to an object. This information can then be used to query and select objects. Labels help add information to objects that is relevant to users, so are useful in UI or CLI.\nLabels allow users to map their own org structure on system resources. Things like environment, team etc.\nA label key and value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters each.\nOptionally, the key can begin with a DNS subdomain prefix and a single \u0026lsquo;/\u0026rsquo;, like example.com/my-app.\nIt appears under the metadata field\napiVersion: kind: metadata: label: app: myawesomeapp List labels applied to a pod. By default, show-labels=false kubectl get pod/nginx --show-labels\nApply label to a pod kubectl label pod/podid newlabel=value\nRemove an existing label from a pod kubectl label pod/podid newlabel- Note the trailing -\nUpdate an existing label kubectl label pod/podid oldlabel=newvalue --overwrite without --overwrite flag label is not updated\nIf --overwrite is true, then existing labels can be overwritten, otherwise attempting to overwrite a label will result in an error.\nInspect labels applied to all objects kubectl get all --all-namespaces --show-labels\nUse Selector flag to list only resources with a specific label kubectl get all --selector app=my-dep\nSome labels are applied automatically, example on a [[#Namespace]], kubernetes.io/metadata.name=namespacename\nIf --resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used. This resource-version available under metadata.resourceVersion.\nSelector Appears under spec\napiVersion: kind: metadata: spec: selector: matchLabels: app: myawesomeapp Use Selector flag to list only resources with a specific label kubectl get all --selector app=my-dep\nAnnotations Add non-identifying information/metadata to objects. Annotations cannot be used to query and select objects. Information or metadata added as annotation to objects is mostly for use by machines ex - iam role annotations in case of IRSA\nDeployment versions are added as annotations to the metadata field in the manifest yml\napiVersion: apps/v1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \u0026#34;1\u0026#34; Labels vs Annotations Labels = identifying information, Annotations = non-identifying information Labels can be used to select objects or collection of objects, annotations cannot be used to identify or select objects Annotations can contain characters not allowed by labels Property Labels Annotations Notes Identifying information yes no a Limited characters yes no a Use with selector yes no a User friendly yes no a Namespace Provides isolation for resources Some objects are namespaced scoped while others are cluster wide Objects can have same name across namespaces, but must be unique within a namespace. So? Every service can use names like frontend, backend, cache without worrying about name collisions. Hierarchical namespaces - userguide All namespaces Use --all-namespace and -n flags to work with all, or a specific namespace kubectl [verb] [resource] --all-namespaces kubectl [verb] [resource] -A kubectl [verb] [resource] -n namespace\nExisting namespaces kubectl get ns on a fresh cluster will show these 4 existing namespaces\ndefault Active 48m kube-node-lease Active 48m kube-public Active 48m kube-system Active 48m Namespace Issue When creating a [[#Service]], a corresponding DNS entry like service.namespace.svc.cluster.local is created. Due to this, all namespace names must be valid DNS name.\nTo connect to a service in the same namespace, just specifying service is enough. It will be resolved locally within the same namespace. This is useful to launch multiple environments with the same config without much modifications.\nTo connect to a service in a different namespace, fully qualified name service.othernamespace.svc.cluster.local must be used.\n[!danger] Be careful about namespaces matching public domain names.\nSuppose, a namespace is named com, it contains a service called google. The local DNS name for it will be google.com.svc.cluster.local. If another service, foo in the same namespace tries to reach the public google.com, it will get resolved to the local google service instead.\nRestrict permissions to create namespaces, and use admission controllers to further enforce this.\n[!Test] Launch a service called landing in ai namespace, are other services in that space able to reach the public landing.ai service?\nI wasn\u0026rsquo;t able to reproduce this behaviour :(\nUpdate: I don\u0026rsquo;t see this happening with the busybox image, BUT this can be seen with the dnsutils image. All properties are exactly the same between both pods, so it might be down to the OS used in each image 🤷\n$ kubectl -n ai get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE landing ClusterIP 10.97.252.177 \u0026lt;none\u0026gt; 80/TCP 21m # this still resolves to the public landing.ai service $ kubectl exec busybox -it -- nslookup landing.ai Server: 10.96.0.10 Address: 10.96.0.10:53 Non-authoritative answer: Name: landing.ai Address: 35.196.113.152 Non-authoritative answer: # this resolves to the private landing.ai service $ kubectl exec busybox -it -- nslookup landing.ai.svc.cluster.local Server: 10.96.0.10 Address: 10.96.0.10:53 Name: landing.ai.svc.cluster.local Address: 10.97.252.177 $ kubectl exec -it busybox -- cat /etc/resolv.conf search default.svc.cluster.local svc.cluster.local cluster.local nameserver 10.96.0.10 options ndots:5 $ kubectl exec -it dnsutils -- nslookup launch.ai Server: 10.96.0.10 Address: 10.96.0.10#53 Name: launch.ai.svc.cluster.local Address: 10.106.142.53 $ kubectl exec -it dnsutils -- nslookup launch Server: 10.96.0.10 Address: 10.96.0.10#53 ** server can\\\u0026#39;t find launch: NXDOMAIN $ kubectl exec -it dnsutils -- cat /etc/resolv.conf search default.svc.cluster.local svc.cluster.local cluster.local nameserver 10.96.0.10 options ndots:5 Service Almost like a virtual load balancer, connected to [[#Deployments]] using [[#Labels]]\nProperties - id address, target port, and endpoints, session affinity?\nIt connects to the nodes which run kube-proxy. kube-proxy uses iptables to connect to the pods running on the nodes.\nThis service object ensures, the traffic is redirected to one of the pods.\nkubectl get svc -A shows all services running in a cluster\nkubectl expose creates a service by looking up a deployment, replica set, replication controller, pod or another service by name and using the selector of the resource.\nkubectl expose deployment nginx --port=80 --target-port=8000\nPort vs Target Port? target port is the port on the pod that the service target, port is the port that the service exposes\nCluster IP default, internal access only\nNodePort ties a port of the node to the node of a pod, accessible from outside the cluster\nLoadBalancer Public cloud load balancers\nExternalName uses DNS names, redirection happens at DNS level\nService without selector use for direct connections based on ip/port, without an endpoint. Useful for databases and within namespaces\n#ask Can I not use a service for resources with no labels?\n#ask What is a headless service?\nIngress Successor [[GatewayApi]]\nProvides a http route from outside the cluster to services running in the cluster. It can also handle ssl termination, load balancing and name based virtual hosting.\nExample ingress sending all traffic to a single service\ngraph LR; client([client])-. Ingress-managed load balancer .-\u003eingress[Ingress]; ingress--\u003e|routing rule|service[Service]; subgraph cluster ingress; service--\u003epod1[Pod]; service--\u003epod2[Pod]; end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class ingress,service,pod1,pod2 k8s; class client plain; class cluster cluster; Example ingress config\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: minimal-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx-example rules: - http: paths: - path: /testpath pathType: Prefix backend: service: name: test port: number: 80 Ingress spec has rules which are matched against all incoming http requests, and the traffic is directed accordingly.\nIngress Annotations are often used to configure certain properties depending on the ingress controller in use.\nIf no host is specified in rules as in the example above, it matches all hosts.\nBackend can also be a resource, but you cannot specify both resource and service for a path. resource backend is useful for directing requests for static assets to an object storage.\npathType can be one of Prefix, Exact, or ImplementationSpecific (upto the IngressClass)\nFor exposing arbitraty protocols and ports, [[NodePort]] or LoadBalancer service type can be used.\nAn ingress resource on its own doesn\u0026rsquo;t mean anything, it needs an [[Ingress Controller]] to be present on the cluster to provide the required functionality.\nFor handling TLS, the ingress spec should refer to a secret which provides the cert and secret key. For TLS to work properly, the host values in spec.tls.hosts must match spec.rules.host.\n#find how is the ingress configured in the general eks cluster?\nBlog post\nIngress Controller Various options like nginx, aws alb, istio etc.\nEach ingress controller implements a particular ingress class. For ex, for aws load balancer controller, it is alb. (ref)\nNetowrking Offical docs Design doc\nNode contains pods which is controlled by a deployment, each pod has an IP. But\nService is connected to deployment using label\nIP is a pod property, not container property, kubectl describe pod shows the IP assigned to a pod, or use kubectl get pods -o wide\n4 major problems -\ncontainer to container communication - handled by [[##Pods|pod]], localhost communication pod to pod communication - explained below pod to service communication - handled by [[#Service|services]] external to service communication - handled by [[#Service|services]] Plugin When changing a network plugin - ensure the network cidr stays the same\nDNS # service NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-system kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 33s #pods NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-5dd5756b68-2l28z 1/1 Running 0 33s kube-system coredns-5dd5756b68-t55kw 1/1 Running 0 33 What objects get dns names?\n[[#Service]] service.namespace.svc.cluster.local [[#Pods]] pod-ipv4.namespace.pod.cluster.local Each pod has a dns policy defined under pod.spec.dnsPolicy, value is either of\nDefault, inherits from the node ClusterFirst, any query not matching cluster domain is forwarded to upstream DNS servers. ClusterFirstWithHostNet, for pods running with hostNetwork: true. None, specify dns configs under pod.spec.dnsConfig Note: default is NOT the default dns policy. If no policy is used ClusterFirst is used.\ndo an nsloop on kubernetes\n$ kubectl run busybox --image busybox --command -- nslookup kubernetes pod/busybox created $ kubectl logs pod/busybox Server: 10.96.0.10 Address: 10.96.0.10:53 ** server can\u0026#39;t find kubernetes.cluster.local: NXDOMAIN ** server can\u0026#39;t find kubernetes.cluster.local: NXDOMAIN Name: kubernetes.default.svc.cluster.local Address: 10.96.0.1 ** server can\u0026#39;t find kubernetes.svc.cluster.local: NXDOMAIN ** server can\u0026#39;t find kubernetes.svc.cluster.local: NXDOMAIN This provides the ip of the kubernetes service which can be verified using kubectl describe svc/kubernetes\nNote: lookup only works within the namespace. Outside the namespace, you won\u0026rsquo;t get the result!\n$ kubectl run dnsnginx --image busybox --command -- nslookup nginx pod/dnsnginx created $ kubectl run dnskube --image busybox --command -- nslookup kube-dns pod/dnskube created $ $ kubectl logs pod/dnsnginx Server: 10.96.0.10 Address: 10.96.0.10:53 ** server can\u0026#39;t find nginx.cluster.local: NXDOMAIN ** server can\u0026#39;t find nginx.cluster.local: NXDOMAIN ** server can\u0026#39;t find nginx.default.svc.cluster.local: NXDOMAIN ** server can\u0026#39;t find nginx.default.svc.cluster.local: NXDOMAIN $ kubectl -n nginx run dnsnginx --image busybox --command -- nslookup nginx pod/dnsnginx created root@controlplane:~$ kubectl logs pod/dnsnginx -n nginx Server: 10.96.0.10 Address: 10.96.0.10:53 Name: nginx.nginx.svc.cluster.local Address: 10.99.230.232 ** server can\u0026#39;t find nginx.cluster.local: NXDOMAIN ** server can\u0026#39;t find nginx.cluster.local: NXDOMAIN Why is that? Check the dns config inserted into a pod -\n$ kubectl -n nginx exec -it dnsnginx -- /bin/sh / # cat /etc/resolv.conf search nginx.svc.cluster.local svc.cluster.local cluster.local nameserver 10.96.0.10 options ndots:5 The name server 10.96.0.10 points to the kube-dns service running in the kube-system namespace\n$ kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 52m kube-system kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 52m nginx nginx ClusterIP 10.99.230.232 \u0026lt;none\u0026gt; 80/TCP 43m Q: How to connect to a service running in namespace B if it can\u0026rsquo;t be queried from pods in namespace A? A: Service name can be queried using the format servicename.namespace from any namespace in the cluster\nhttps://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/\nStorage kubectl explain pod.spec.volumes shows the different volume types that are available for use.\nVolumes Volumes can be ephermal or persistent.\nTo use a volume within a pod\u0026rsquo;s containers, you need to specify spec.volumes and spec.containers[*].volumeMounts. The container so created sees the data contained in the image + any data mounted as a volume.\nSpecified for a pod in spec.volumes, to check all the available configuration options, use kubectl explain pod.spec.volumes\nVolume types were cloud specific which have now been deprecated in favor of 3rd party [[##storage drivers]] instead. The following volume types are still valid -\nSecret (always mounted as RO, don\u0026rsquo;t use as subpath to receive updates) ConfigMap (always mounted as RO, don\u0026rsquo;t use as subpath to receive updates) Local, Empty Dir, Host Path relate to local filesystems of the node. PVC Projected Downward API - check coredns pods graph LR; subgraph pod subgraph container1 m1[volMount] end subgraph container2 m2[volMount] end subgraph volumes v[vol] end end subgraph storage pv[pv] end subgraph claim pvc[pvc] end pv --bound--\u003e pvc v --\u003e m1 v --\u003e m2 pvc --\u003e v PV Persistent Volumes decouple the storage requirements from pod development. PV use properties like accessModes, capacity, mountOptions, pvreclaimPolicy, volumeMode etc to mount the persistent volume to the pod.\nPV can be created manually (manifest) or dynamically (using a storage class)\nAccess Modes can be one of the following\nReadWriteOnce (RWO) - A single node can mount this volume as read write. Many pods on this node can still use the volume. ReadOnlyMany (ROX) - Many pods can mount the volume as read only. ReadWriteMany (RWX) - Many pods can mount the volume as read, write. ReadWriteOncePod (RWOP) - A single pod can mount the volume as read, write (version v1.22 onwards only). PVC Persistent volume claims are used by pod authors to add storage needs in a declarative way, without worrying about storage specifics.\nPVC use properties like accessModes, volumeMode, storageClassName, resources, selector to provision the storage as per the requirements. kubectl explain pvc.spec to know about all the properties.\nSimple local example\n# pv.yaml kind: PersistentVolume apiVersion: v1 metadata: name: pv-vol # not used anywhere labels: type: local spec: accessModes: - ReadWriteOnce capacity: storage: 2Gi hostPath: path: \u0026#34;/data\u0026#34; # this should exist on host # pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pv-claim # used in pod.spec.volumes[].pvc.claimName spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi # \u0026lt;= pv.spec.capacity.storage # pod.yaml kind: Pod apiVersion: v1 metadata: name: pv-pod spec: containers: name: pv-container image: nginx ports: - containerPort: 80 name: nginxhttp volumeMounts: - mountPath: \u0026#34;/usr/share/nginx/html\u0026#34; name: cvol # from pod.spec.volumes[].name volumes: - name: cvol persistentVolumeClaim: claimName: pv-claim # from pvc.metadata.name #ask what happens if pvc.spec.requests.storage \u0026gt; pv.spec.capacity.storage ?\nStorage Class can be grouped according to anything - capacity, type, location etc. Uses spec.provisioner to connect to the storage When a PVC does not specify a storageClassName, the default StorageClass is used. The cluster can only have one default StorageClass. If more than one default StorageClass is set, the newest default is used. Example\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: standard provisioner: kubernetes.io/aws-ebs parameters: # provisioner specific parameters type: gp2 reclaimPolicy: Retain allowVolumeExpansion: true mountOptions: - debug volumeBindingMode: Immediate ConfigMap Decouple configuration from application\nexample, notice it uses data instead of the usual spec\napiVersion: v1 kind: ConfigMap metadata: name: nginxcm data: # use in `pod.spec.volumes[].configMap.items[].key` nginx-custom-config.conf: | server { listen 8080; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } } Use it in a pod\napiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx volumeMounts: - name: conf mountPath: /etc/nginx/conf.d/ volumes: - name: conf configMap: name: nginxcm items: # key as in configMap.data.key - key: nginx-custom-config.conf # path within the container path: default.conf Secrets Decouple sensitive variables from application\nexample - notice it uses data instead of the usual spec\napiVersion: v1 kind: Secret metadata: name: secret data: username: encodedusername password: encodedpassword Kubernetes API Collection of [[RESTful APIs]], supports GET, POST, DELETE. It is crucial to identify api version to use.\n#ask why did kubernetes project choose this RESTful API approach?\nTo allow the system to continuously evolve and grow.\nNew features can be easily added without impacting existing clients as alpha, and moved to beta, then stable version as they mature.\nIt also allows the project to maintain compatibility with existing clients by offering both beta and stable version of an API simultaneously (for a length of time).\nVersioning is done at the API level rather than at the resource or field level to ensure that the API presents a clear, consistent view of system resources and behavior, and to enable controlling access to end-of-life and/or experimental APIs.\nThe API server handles the conversion between API versions transparently: all the different versions are actually representations of the same persisted data. The API server may serve the same underlying data through multiple API versions.\nSo, if I create a resource using an API version v1beta1, I can later use v1 version to query or manage it (within the deprecation period). Some fields may need updating due to the API graduating to v1, but, I can still migrate to the newer version of the API without having to destroy and recreate the resource.\nAPI versions cannot be removed in future versions until this issue is fixed.\nAPI access is controlled by the API server.\nIt saves the serialized objects in [[etcd]].\nAPI resources are distinguished by their API group, resource type, namespace (for namespaced resources), and name.\nMonitor deprecated API requests - apiserver_requested_deprecated_apis metric. This can help identify if there are objects in the cluster still using deprecated APIs.\n#ask kube-proxy, where is it hosted, host it works?\ngraph LR subgraph server api[api/etcd] end cr[curl] --\u003e kp[kube-proxy] --\u003e api List available resource APIs, their kind, groups, version, namespaced (bool), version, any shortnames etc, use kubectl api-resources -o wide\nAPI groups can be enabled or disabled using --runtime-config flag on API server\n$ kubectl api-resources NAME SHORTNAMES APIVERSION NAMESPACED KIND .. configmaps cm v1 true ConfigMap ... namespaces ns v1 false Namespace nodes no v1 false Node persistentvolumeclaims pvc v1 true PersistentVolumeClaim persistentvolumes pv v1 false PersistentVolume pods po v1 true Pod ... secrets v1 true Secret serviceaccounts sa v1 true ServiceAccount services svc v1 true Service ... networking.k8s.io/v1 false IngressClass ingresses ing networking.k8s.io/v1 true Ingress networkpolicies netpol ... List versions of available API kubectl api-versions\n$ kubectl api-versions .. apps/v1 authentication.k8s.io/v1 authorization.k8s.io/v1 autoscaling/v1 autoscaling/v2 .. flowcontrol.apiserver.k8s.io/v1beta2 flowcontrol.apiserver.k8s.io/v1beta3 networking.k8s.io/v1 node.k8s.io/v1 policy/v1 rbac.authorization.k8s.io/v1 scheduling.k8s.io/v1 storage.k8s.io/v1 v1 Find properties required for an object, see kubectl commands section above kubectl explain \u0026lt;object.property\u0026gt;\nExamples -\nkubectl explain externalsecrets # explanation only for top level properties kubectl explain externalsecrets --recursive # no explanation, prints the complete schema kubectl explain externalsecrets.spec.target # explanation for a specific property Find namespace scoped APIs or cluster wide APIs kubectl api-resources --namespaced=true kubectl api-resources --namespaced=false\nProxy kubectl to access the API more easily using [[Curl]]\n#ask But why would you do this? If an app needs to interact with kubernetes, it can simply use the language specific http library to do this directly instead of going through kubectl\nkubectl proxy --port=8080\n$ curl http://localhost:8080/version { \u0026#34;major\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;minor\u0026#34;: \u0026#34;28\u0026#34;, \u0026#34;gitVersion\u0026#34;: \u0026#34;v1.28.2\u0026#34;, \u0026#34;gitCommit\u0026#34;: \u0026#34;89a4ea3e1e4ddd7f7572286090359983e0387b2f\u0026#34;, \u0026#34;gitTreeState\u0026#34;: \u0026#34;clean\u0026#34;, \u0026#34;buildDate\u0026#34;: \u0026#34;2023-09-13T09:29:07Z\u0026#34;, \u0026#34;goVersion\u0026#34;: \u0026#34;go1.20.8\u0026#34;, \u0026#34;compiler\u0026#34;: \u0026#34;gc\u0026#34;, \u0026#34;platform\u0026#34;: \u0026#34;linux/amd64\u0026#34; } Get pods from kube-system namespace (truncated output)\n$ curl http://localhost:8080/api/v1/namespaces/kube-system/pods | les { \u0026#34;kind\u0026#34;: \u0026#34;PodList\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;resourceVersion\u0026#34;: \u0026#34;1555\u0026#34; \u0026#34;name\u0026#34;: \u0026#34;coredns-5dd5756b68-fcz42\u0026#34;, \u0026#34;generateName\u0026#34;: \u0026#34;coredns-5dd5756b68-\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;kube-system\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;326ba1b7-31b6-4d6c-9978-1057f6734154\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;553\u0026#34;, .. Check the openapi v3 specification (truncated output) on /openapi/v3, and v2 specification on /openapi/v2\n$ curl http://localhost:8080/openapi/v3 { \u0026#34;paths\u0026#34;: { \u0026#34;.well-known/openid-configuration\u0026#34;: { \u0026#34;serverRelativeURL\u0026#34;: \u0026#34;/openapi/v3/.well-known/openid-configuration?hash=4488--\u0026#34; }, \u0026#34;api\u0026#34;: { \u0026#34;serverRelativeURL\u0026#34;: \u0026#34;/openapi/v3/api?hash=929E--\u0026#34; }, \u0026#34;api/v1\u0026#34;: { \u0026#34;serverRelativeURL\u0026#34;: \u0026#34;/openapi/v3/api/v1?hash=5133--\u0026#34; }, \u0026#34;apis\u0026#34;: { \u0026#34;serverRelativeURL\u0026#34;: \u0026#34;/openapi/v3/apis?hash=27E0--\u0026#34; }, \u0026#34;apis/admissionregistration.k8s.io\u0026#34;: { \u0026#34;serverRelativeURL\u0026#34;: \u0026#34;/openapi/v3/apis/admissionregistration.k8s.io?hash=E8D5..\u0026#34; } } API Extensions Custom Resources\nThis is a way to make the API server recognize new non-standard Kubernetes objects.\nExample - Prometheus Operator uses a number of CRDs to manage the deployment in a cluster.\nAggregation Layer\nNeeds to be enabled and then runs in-process in the kube-apisever.\nYou first need to create an APIService object, say myawesomeapi, at a path, say apis/myawesomeapi/v1beta1/. The aggregation layer then proxies any requests API server receives for this API to the registered APIService.\nExample - metrics server\nCreate a Cluster Manually! Kubernetes releases before v1.24 included a direct integration with Docker Engine, using a component named dockershim.\nWhat is dockershim?\nTo provide support for multile container runtimes, CRI API/ specification was developed. But since docker was the first container runtime k8s supported, and to maintain backward compatibility, dockershim was developed which allowed kubelet to interact with docker runtime via the CRI API, sort of like a proxy?\ngraph LR; kb[kubelet] \u003c--cri--\u003e ds[dockershim] \u003c--\u003e dc[docker] \u003c--\u003e cd[containerd] --\u003e c1[container 1] cd --\u003e c2[container 2] cd --\u003e cn[container n] graph LR; kb[kubelet] \u003c--cri--\u003e ccd[cri-containerd] \u003c--\u003e cd[containerd] --\u003e c1[container 1] cd --\u003e c2[container 2] cd --\u003e cn[container n] Create a 3 node cluster - 1 control node, and 2 worker nodes.\nOn control node, kubeadm init On control node, Networking On worker node, kubeadm join Control node\nInstall a container runtime like docker, containerd, cri-o. Install kube tools like kubeadm, kubelet, kubectl kubeadm init Ref Setup $HOME/.kube/config Verify all hosts are present under /etc/hosts Install a pod network add-on - any one of calico, cilium, flannel etc. Worker nodes\nInstall a container runtime like docker, containerd, cri-o. Install kube tools like kubeadm, kubelet, kubectl kubeadm join --token xx --discovery-cert xx To create high availability - use 3 controller nodes, each running with etcd, or use a dedicated etcd cluster\nOlder, needs refining Pod Disruption Budgets https://kubernetes.io/docs/tasks/run-application/configure-pdb/ Based on the value of maxUnavailable for specific pods, cluster autoscaler will either ignore a node, or scale it down.\nPod Affinity https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/ uses pod labels\nexample\napiVersion: v1 kind: Pod metadata: name: label-demo labels: environment: production app: nginx spec: . . . Pods Anti Affinity https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\nlabels allow us to use selectors\nlabels are also useful to slice/dice resources when using kubectl\nkubectl get pods -Lapp -Ltier -Lrole -L displays an extra column in kubectl output -l either selects or update the label applied to a resourse.\nSelectors can be of 2 types\nEquality based (accelerator=nvidia-tesla-p100)\napiVersion: v1 kind: Pod metadata: name: cuda-test spec: containers: - name: cuda-test image: \u0026#34;registry.k8s.io/cuda-vector-add:v0.1\u0026#34; resources: limits: nvidia.com/gpu: 1 nodeSelector: accelerator: nvidia-tesla-p100 Set based\napiVersion: v1 kind: Pod metadata: name: cuda-test spec: containers: - name: cuda-test image: \u0026#34;registry.k8s.io/cuda-vector-add:v0.1\u0026#34; resources: limits: nvidia.com/gpu: 1 nodeSelector: environment: qa,qa1 # and condition accelerator in (nvidia, intel) service, replicationcontroller format for selector\nselector: component: redis daemonset, replicaset, deployment, job format for selector\nselector: matchLabels: component: redis matchExpressions: - {key: component, values: [redis]} Labels Standard or default kubernetes.io/arch kubernetes.io/hostname # cloud provider specific kubernetes.io/os node.kubernetes.io/instance-type # if available to kubelet topology.kubernetes.io/region # topology.kubernetes.io/zone # Labels and selectors [[K8S Scheduling]]\nTroubleshooting Pod Error Alerts sum (kube_pod_container_status_waiting_reason{reason=~\u0026#34;CrashLoopBackOff|ImagePullBackOff|ErrImagePull.+\u0026#34;}) by (namespace, container, reason) Questions 1. What is the value of kubernetes.io/hostname in [[eks]]? I know it\u0026rsquo;s part of standard labels #todo (link it) but, not seen this tag really on [[eks]]. Found following tags instead 🤷\nkubernetes.io/cluster/myawesomecluster=owned `aws:eks:cluster-name=myawesomecluster 2. How do pods communicate with each other in a cluster? 3. How will you control which pod runs on which node(s) Mix of scheduling options like node selector, affinity/anti-affinity, taints, tolerations etc.\n","permalink":"https://abiydv.github.io/notes/k8s-main/","summary":"Architecture graph LR; c1(Cluster) --\u003e n1(node1) --\u003e p11[pod 1] n1 --\u003e p12[pod 2] c1 --\u003e n2(node2) --\u003e p21[pod 1] API docs - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/\nInsert diagram about control nodes + worker nodes + api servers etc.\n#refine https://kubernetes.io/docs/concepts/architecture/\nComponents Metric Server Use HostNetwork\nHelps monitor pods Can run kubectl top pod to check resource usage for pods kubectl top nodes\nCNCF Project status graph LR; a(Sandbox\nNew) --\u003e b(Incubating\nMore wide-spread adoption, active development) --\u003e c[Graduated","title":"K8s Kubernetes"},{"content":"Amazon MQ is a managed message broker service for Apache ActiveMQ and RabbitMQ\nAmazon MQ supports industry standard APIs such as JMS and NMS, and protocols for messaging, including AMQP, STOMP, MQTT, and WebSocket.\ntwo types of broker storage – durability optimized (EFS), and throughput optimized (EBS).\nFor high volume applications, throughput optimized storage is better, can provide the required throughput and optimize costs by reducing the number of brokers required.\nNetwork of brokers\nactive/standby, where each broker has a standby node with a shared storage which can take over if the active fails.\nSingle-instance brokers can cause outage when the broker fails while a replacement broker is provisioned.\nUse network of brokers if you need high availability, fast reconnection or need to scale horizontally.\nOpen source project deployed on, and maintained by AWS.\n","permalink":"https://abiydv.github.io/notes/aws/mq/","summary":"Amazon MQ is a managed message broker service for Apache ActiveMQ and RabbitMQ\nAmazon MQ supports industry standard APIs such as JMS and NMS, and protocols for messaging, including AMQP, STOMP, MQTT, and WebSocket.\ntwo types of broker storage – durability optimized (EFS), and throughput optimized (EBS).\nFor high volume applications, throughput optimized storage is better, can provide the required throughput and optimize costs by reducing the number of brokers required.","title":"Amazon MQ"},{"content":"As an AWS Activate member, you can apply for AWS credits, accelerator programs offered by AWS, and browse the Showcase to explore the many startups that have built and launched a product on AWS as way to find inspiration, connect and collaborate with the AWS startup ecosystem. You can also explore the learning and training resources tailored to your startup.\nShowcase allows AWS Activate members to list and amplify their business to potential startup investors, customers, and partners. For investors, it’s an exciting way to find startups that have built and launched their startup on AWS.\nhttps://aws.amazon.com/activate/activate-landing/\n","permalink":"https://abiydv.github.io/notes/aws/activate/","summary":"As an AWS Activate member, you can apply for AWS credits, accelerator programs offered by AWS, and browse the Showcase to explore the many startups that have built and launched a product on AWS as way to find inspiration, connect and collaborate with the AWS startup ecosystem. You can also explore the learning and training resources tailored to your startup.\nShowcase allows AWS Activate members to list and amplify their business to potential startup investors, customers, and partners.","title":"AWS Activate"},{"content":"Athena Region scope 3x Replication Performance? DocumentDB\nQueries Parameterized Queries Add a prepared statement aws athena create-prepared-statement --statement-name SplitCostItem --query-statement \u0026#34;SELECT line_item_usage_start_date, line_item_usage_end_date, line_item_resource_id, line_item_blended_cost, split_line_item_net_split_cost, split_line_item_split_usage, split_line_item_net_unused_cost, line_item_line_item_type, line_item_usage_type, split_line_item_parent_resource_id FROM \u0026#34;athenacurcfn\u0026#34;.\u0026#34;amazon_athena\u0026#34; where line_item_line_item_type=\u0026#39;Usage\u0026#39; and line_item_resource_id=\u0026#39;?\u0026#39; and year=\u0026#39;?\u0026#39; and month=\u0026#39;?\u0026#39;\u0026#34; --work-group athena-engine-v2 Start query execution aws athena start-query-execution --query-string \u0026#34;Execute SplitCostItem\u0026#34; --query-execution-context \u0026#34;Database\u0026#34;=\u0026#34;default\u0026#34; --result-configuration \u0026#34;OutputLocation\u0026#34;=\u0026#34;s3://query-result-bucket/\u0026#34; --execution-parameters \u0026#34;resource-id\u0026#34; \u0026#34;2024\u0026#34; \u0026#34;1\u0026#34; ","permalink":"https://abiydv.github.io/notes/aws/athena/","summary":"Athena Region scope 3x Replication Performance? DocumentDB\nQueries Parameterized Queries Add a prepared statement aws athena create-prepared-statement --statement-name SplitCostItem --query-statement \u0026#34;SELECT line_item_usage_start_date, line_item_usage_end_date, line_item_resource_id, line_item_blended_cost, split_line_item_net_split_cost, split_line_item_split_usage, split_line_item_net_unused_cost, line_item_line_item_type, line_item_usage_type, split_line_item_parent_resource_id FROM \u0026#34;athenacurcfn\u0026#34;.\u0026#34;amazon_athena\u0026#34; where line_item_line_item_type=\u0026#39;Usage\u0026#39; and line_item_resource_id=\u0026#39;?\u0026#39; and year=\u0026#39;?\u0026#39; and month=\u0026#39;?\u0026#39;\u0026#34; --work-group athena-engine-v2 Start query execution aws athena start-query-execution --query-string \u0026#34;Execute SplitCostItem\u0026#34; --query-execution-context \u0026#34;Database\u0026#34;=\u0026#34;default\u0026#34; --result-configuration \u0026#34;OutputLocation\u0026#34;=\u0026#34;s3://query-result-bucket/\u0026#34; --execution-parameters \u0026#34;resource-id\u0026#34; \u0026#34;2024\u0026#34; \u0026#34;1\u0026#34; ","title":"AWS Athena"},{"content":"Relational Database, MySQL, PostgreSQL\nScope = AZ based, needs 3 subnets during creation.\nGlobal Database = Global, available across regions\nAuto-scaling upto 128 TB/db instance. Fault tolerant, Cross-AZ replication Self healing = divides data into 10GB chunk and each chuck is replicated 6 times, including across 3 AZ, can handle loss of 2 copies of data w/o affecting write ability, and 3 copies without affecting read ability. Continuous backups to S3 upto 15 low-latency read replicas\nMigration = mysqldump, mysqlimport utilities, RDS snapshot\nI/O Optimized = for high I/O apps that need predictable performance higher than what Standard offers\nTypes - open source MySQL compatible, PostgreSQL\nReplicas = read replicas within same region. Limit 15. Replicas for MySQL = cross region read replicas using MySQL binlog-based replication engine. Limit 5.\nRDS Proxy to reduce number of connections that are opened to the database.\nIAM authentication? Supported for Aurora MySql and PostgrSql. Use when the application makes fewer than 200 connections per second. Generate authentication token using DB cluster endpoint, custom domain does not work.\nBabelfish = Aurora PostgreSQL now understands T-SQL, Microsoft SQL Server\u0026rsquo;s proprietary SQL dialect,\n","permalink":"https://abiydv.github.io/notes/aws/aurora/","summary":"Relational Database, MySQL, PostgreSQL\nScope = AZ based, needs 3 subnets during creation.\nGlobal Database = Global, available across regions\nAuto-scaling upto 128 TB/db instance. Fault tolerant, Cross-AZ replication Self healing = divides data into 10GB chunk and each chuck is replicated 6 times, including across 3 AZ, can handle loss of 2 copies of data w/o affecting write ability, and 3 copies without affecting read ability. Continuous backups to S3 upto 15 low-latency read replicas","title":"AWS Aurora"},{"content":"Questions What is lifecycle tiering? Transition backups from warm storage to cold storage. Only available for EFS, DynamoDB, Timestream database, VMWare VMs\nBackup plan defines when and how to backup resources\nBackup vault Encrypted storage location (per region) that stores the backups\nAudit and report compliance of data protection policies Deploy central backup policies via AWS Organizations\ncloud S3 EFS FSx EBS EC2 Windows VSS services on EC2 SAP HANA on EC2 DynamoDB RDS Aurora DB Cluster Neptune DB DocumentDB Redshift manual snapshot Timestream database Cloudformation stacks\non-prem Storage gateway volumes VMWare on-prem VMWare cloud AWS VMWare cloud on outposts\n","permalink":"https://abiydv.github.io/notes/aws/aws-backup/","summary":"Questions What is lifecycle tiering? Transition backups from warm storage to cold storage. Only available for EFS, DynamoDB, Timestream database, VMWare VMs\nBackup plan defines when and how to backup resources\nBackup vault Encrypted storage location (per region) that stores the backups\nAudit and report compliance of data protection policies Deploy central backup policies via AWS Organizations\ncloud S3 EFS FSx EBS EC2 Windows VSS services on EC2 SAP HANA on EC2 DynamoDB RDS Aurora DB Cluster Neptune DB DocumentDB Redshift manual snapshot Timestream database Cloudformation stacks","title":"AWS Backup"},{"content":"Beanstalk az scope, auto-scaling in multi-az code source should be in same region\n","permalink":"https://abiydv.github.io/notes/aws/beanstalk/","summary":"Beanstalk az scope, auto-scaling in multi-az code source should be in same region","title":"AWS Beanstalk"},{"content":"Amazon Pinpoint Multi-channel communication with customers (SMS, voice, in-app etc) https://aws.amazon.com/pinpoint/\nAmazon Connect Cloud contact center - probably powers Amazon\u0026rsquo;s support function https://aws.amazon.com/connect/\n","permalink":"https://abiydv.github.io/notes/aws/amazon-business-apps/","summary":"Amazon Pinpoint Multi-channel communication with customers (SMS, voice, in-app etc) https://aws.amazon.com/pinpoint/\nAmazon Connect Cloud contact center - probably powers Amazon\u0026rsquo;s support function https://aws.amazon.com/connect/","title":"AWS Business Apps"},{"content":"Parameters AWS Things like AMI Id, Instance Id, Vpc Id etc. SSM AWS Doc Refer to a parameter (not secure string) saved in SSM parameter store Use parameter by name, don\u0026rsquo;t need to specify version Use cross account parameter by providing full arn. Dynamic AWS doc Refer to parameters dynamically in templates. Useful to obscure secrets. No drift detection SSM parameter store {{resolve:ssm-secure:parameter-name:version}} or {{resolve:ssm:parameter-name:version}} Can\u0026rsquo;t use secure string parameter for all resources. Very limited support, see this Can\u0026rsquo;t use cross-account parameters Can\u0026rsquo;t use public parameters or parameter labels Can\u0026rsquo;t use for custom resources Secrets Manager {{resolve:secretsmanager:secret-id:secret-string:json-key:version-stage:version-id}} Can use for ALL resources, well, almost. Can\u0026rsquo;t use for custom resources When changing a referred secret, cloudformation still makes a list call to the old secret. Template update will fail if the old secret is marked for deletion ahead of cloudformation run. Questions What is stack sets? How do you deploy cross-region cross-account? How do you customize parameter values for different regions and/or accounts? What is termination protection? What is deletion policy attribute? What resources support it? What are the valid vales? Snapshot/retain What is a stack policy? What is the use-case? What is a change-set? can use ssm appconfig? can use ssm parameter store? cloudformation does not support creating secrets parameter, supply parameter values in a separate file, like terraform tfvars files parameter types update behaviour How does it work with aws cdk? Reference: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html\n","permalink":"https://abiydv.github.io/notes/aws/cloudformation/","summary":"Parameters AWS Things like AMI Id, Instance Id, Vpc Id etc. SSM AWS Doc Refer to a parameter (not secure string) saved in SSM parameter store Use parameter by name, don\u0026rsquo;t need to specify version Use cross account parameter by providing full arn. Dynamic AWS doc Refer to parameters dynamically in templates. Useful to obscure secrets. No drift detection SSM parameter store {{resolve:ssm-secure:parameter-name:version}} or {{resolve:ssm:parameter-name:version}} Can\u0026rsquo;t use secure string parameter for all resources.","title":"AWS CloudFormation"},{"content":"Scope Global scope Supports multiple regions - multiple origins Health check for origin failovers - can be useful in disaster recovery supports active/active, active/standby architectures\nQuestions How is CloudFront different from Global Accelerator? https://docs.aws.amazon.com/global-accelerator/latest/dg/about-accelerators.html\nWhat is Origin Group? ORIGIN Group - configure 2 origins, mark one as primary, and the other as secondary. Configure status codes to trigger fail over. If primary origin returns one these configured status codes, CloudFront then sends the request to the secondary origin.\nCloudFront is sort of stateless in this failover mode since it sends a request to primary origin first irrespective of what happened with the previous request.\nLambda@Edge can be executed twice during the failover, once for each origin CloudFront sends the request to.\nHow to distribute a new file? Invalidate the cache or deploy the file with a versioned name. Versioning is better than invalidation\nVersioning can bypass local caches like browsers No cost implications as invalidation costs money Invalidation is chargeable beyond a certain number. Helpful while debugging since file names are different Easier to move forward or roll back between different versions Versioning can allow A/B testing - serving different versions for different requests Origin Access Identity to access S3 bucket without enabling website hosting on it? what is field level encryption? secure sensitive data between cloudfront and your origin to encrypt sensitive fields like credit card numbers so that only the origin is able to decrypt them. provide a public key, and the field (up to 10) to be encrypted in a POST request. the origin can keep the private key which it\u0026rsquo;ll use to decrypt the field later. Link this config to a cache behaviour to tell cloudfront when to encrypt this field. prevents the data from being leaked unintentially by one of the microservices, which had access to this field, but didn\u0026rsquo;t really need it.\nwhat is savings bundle? Commit to a monthly cloudfront usage for 1 year, and receive 30% discount for Cloudfront + free AWS WAF upto 10% of committed use\nwhat is price class 100? use price class to use only certain cloudfront locations and lower your bill. useful if your target customers are restrictricted to a particular geography or 2.\n100 - US, Mexico, Canada, EU and Israel 200 - 100 + Africa, Asia All - 200 + Sourth America, AU, NZ\nCaveats no charge for data transfer to cloudfront. cloudfront support HTTP or [[Web socket]] protocols use an Alias record on route53 to map an apex domain to cloudfront route53 doesn\u0026rsquo;t charge for alias record queries for cloudfront invalidate max 3000 objects if doing individually, or 15 paths if using wildcard POST, PUT, DELETE, PATCH requests are not cached, OPTIONS requests may be cached depending on the config\nLambda@Edge Backed by Lambda https://aws.amazon.com/blogs/networking-and-content-delivery/leveraging-external-data-in-lambdaedge/\nLambda@Edge v/s CloudFront functions CFF are 1/6th the cost, and execute in milliseconds, useful for things like normalizing requests for cache-keys, changing headers, redirects etc.\nLAE is a more heavyweight option that runs at the regional edge location. Supports network connection. Useful for things like heavier workloads - streaming media?\nCan you create a non-https api with api gateway? NO, API Gateway ONLY supports HTTPS APIs. So use a cloudfront distribution to setup a redirect http --\u0026gt; https\n","permalink":"https://abiydv.github.io/notes/aws/cloudfront/","summary":"Scope Global scope Supports multiple regions - multiple origins Health check for origin failovers - can be useful in disaster recovery supports active/active, active/standby architectures\nQuestions How is CloudFront different from Global Accelerator? https://docs.aws.amazon.com/global-accelerator/latest/dg/about-accelerators.html\nWhat is Origin Group? ORIGIN Group - configure 2 origins, mark one as primary, and the other as secondary. Configure status codes to trigger fail over. If primary origin returns one these configured status codes, CloudFront then sends the request to the secondary origin.","title":"AWS CloudFront"},{"content":"Trails No IAM roles needed for cross-account delivery. #todo What principal does it show if you enable data events on the log delivery bucket?\nLake Ingest events into an event data source, and run sql queries on this data source.\nData can be collected from AWS (like CloudTrail events), custom application or 3rd parties (like okta system log)\nPricing is based on data ingestion, analysis and retention. It can retain data up to a maximum of 10 years.\nQuery Run a SQL like query against the events recorded in the CloudTrail lake. It does not support some query types.\nExamples Find all actions taken by user across accounts and regions\nSELECT eventID, eventName, eventSource, eventTime, userIdentity.arn AS user FROM \u0026lt;cloudtrail-lake\u0026gt; WHERE userIdentity.arn LIKE \u0026#39;%username%\u0026#39; AND eventTime \u0026gt; \u0026#39;2023-11-11 00:00:00\u0026#39; AND eventTime \u0026lt; \u0026#39;2023-11-11 16:30:00\u0026#39; SELECT eventTime, eventName, eventSource, awsRegion, resources, userIdentity.arn AS user, errorCode, errorMessage, eventID FROM \u0026lt;cloudtrail-lake\u0026gt; WHERE userIdentity.arn = \u0026#39;arn:aws:sts::000123456789:assumed-role/some-role\u0026#39; AND NOT readOnly AND eventTime \u0026gt; \u0026#39;2023-11-11 11:00:00\u0026#39; AND errorCode IS NOT NULL ORDER BY eventTime DESC Find out which accounts are generating most events, and what the most frequent events are\nSELECT recipientAccountId, eventName, COUNT(*) AS eventCount FROM \u0026lt;cloudtrail-lake-id\u0026gt; WHERE eventTime \u0026gt; \u0026#39;2024-07-01 00:00:00\u0026#39; AND eventTime \u0026lt; \u0026#39;2024-07-10 00:00:00\u0026#39; GROUP BY recipientAccountId, eventName ORDER BY eventCount DESC ","permalink":"https://abiydv.github.io/notes/aws/cloudtrail/","summary":"Trails No IAM roles needed for cross-account delivery. #todo What principal does it show if you enable data events on the log delivery bucket?\nLake Ingest events into an event data source, and run sql queries on this data source.\nData can be collected from AWS (like CloudTrail events), custom application or 3rd parties (like okta system log)\nPricing is based on data ingestion, analysis and retention. It can retain data up to a maximum of 10 years.","title":"AWS CloudTrail"},{"content":"What is it? AWS Config records and saves configuration of supported resources.\nYou can use these records to audit whether the resources align to best practices, or org specific patterns/rules. It can also highlight 2-way relationships between different resources. For ex - an instance is related to a vpc, a subnet, and a security group. This relation will show up in both the instance, and vpc records under relationships.\nResource Schema https://github.com/awslabs/aws-config-resource-schema/tree/master\nAdvanced Query Run a sql like query against the records in AWS Config. It does not support some query types.\nIf you run the query against an aggregator you can query for resources across multiple accounts, even your entire organization.\nThis only works for current state query - does not work for historical search.\nNot supported advanced query limitations wildcard matches beginning with a wildcard character or having character sequences of less than length 3 are unsupported configuration.publicDnsName like \u0026ldquo;%11-12-13-14%\u0026rdquo; will not work configuration.publicDnsName like \u0026ldquo;11%\u0026rdquo; will not work configuration.publicDnsName like \u0026ldquo;ec2-11-12-13-14%\u0026rdquo; will work No way to kick off an on-demand evaluation of organization config rule without doing this for each account individually CloudFormation drift detection is not supported for config recorders and delivery channels Examples List all tags of instances in specific account SELECT resourceId, awsRegion, tags WHERE resourceType = \u0026#39;AWS::EC2::Instance\u0026#39; AND accountId = \u0026#39;12345678912\u0026#39; List all instances with a specific private IP note: configuration.networkInterfaces is an array, so all elements are searched. configuration.networkInterfaces.privateIpAddress will be an array of ip addresses as found\nSELECT resourceId, resourceType, configuration.instanceType, configuration.placement.tenancy, configuration.networkInterfaces.privateIpAddress, configuration.imageId, availabilityZone WHERE resourceType = \u0026#39;AWS::EC2::Instance\u0026#39; AND configuration.networkInterfaces.privateIpAddress = \u0026#39;1.1.1.1\u0026#39; public ip SELECT resourceId, resourceType, configuration.instanceType, configuration.placement.tenancy, configuration.imageId, availabilityZone WHERE configuration.ServerHostname = \u0026#39;ec2-11-12-13-14.compute-1.amazonaws.com\u0026#39; Search for instances with a specific tag select resourceId, tags where resourceType = \u0026#39;AWS::EC2::Instance\u0026#39; and tags.key = \u0026#39;Owner\u0026#39; and tags.value = \u0026#39;bruce.wayne\u0026#39; Wildcard search for instances select resourceId, tags.tag where resourceType = \u0026#39;AWS::EC2::Instance\u0026#39; and tags.tag like \u0026#39;Owner=bruce%\u0026#39; # matches bruce.wayne, bruce.willis Find all RDS instances not running with rds-ca-rsa2048-g1 CA SELECT configuration.cACertificateIdentifier, resourceId, resourceName, accountId, awsRegion, tags.tag WHERE resourceType = \u0026#39;AWS::RDS::DBInstance\u0026#39; and configuration.cACertificateIdentifier != \u0026#39;rds-ca-rsa2048-g1\u0026#39; Find all resources created in an account between specific dates select resourceType, count(*) where accountId = \u0026#39;123456789000\u0026#39; and configurationItemCaptureTime between \u0026#39;2024-01-01T00:00:000Z\u0026#39; and \u0026#39;2024-02-01T00:00:000Z\u0026#39; GROUP BY resourceType Find all buckets in 2 accounts select resourceId where accountId in (\u0026#39;123456789001\u0026#39;, \u0026#39;123456789002\u0026#39;) and resourceType = \u0026#39;AWS::S3::Bucket\u0026#39; Find all security groups which contain a particular cidr range Note: CIDR notation is converted to IP ranges for search. This means that \u0026ldquo;=\u0026rdquo; and \u0026ldquo;BETWEEN\u0026rdquo; search for any range that includes the provided IP, instead of for an exact one. To search for an exact IP range, you need to add in additional conditions to exclude IPs outside of the range.\nSELECT * WHERE resourceType = \u0026#39;AWS::EC2::SecurityGroup\u0026#39; AND configuration.ipPermissions.ipRanges BETWEEN \u0026#39;10.0.0.0\u0026#39; AND \u0026#39;10.0.0.255\u0026#39; AND NOT configuration.ipPermissions.ipRanges \u0026lt; \u0026#39;10.0.0.0\u0026#39; AND NOT configuration.ipPermissions.ipRanges \u0026gt; \u0026#39;10.0.0.255\u0026#39; Aggregator Aggregates resource records from different accounts and regions. A default set up is done by [[AWS Control Tower]], if you don\u0026rsquo;t want to set it up from scratch yourself.\nConformance Packs Recorder Delivery Channel Rules What is a rule? Triggers Custom rules Implement custom rules in Lambda to validate the resource record.\nManaged rules Use AWS managed rules to validate the resource record. Full list of managed rules is here.\nRecord software changes on EC2 You can use AWS Config to record software inventory changes on Amazon EC2 instances and on-premises servers\nTurn on recording for the managed instance inventory resource type in AWS Config. Configure EC2 and on-premises servers as managed instances in AWS Systems Manager. A managed instance is a machine that has been configured for use with Systems Manager. Initiate collection of software inventory from your managed instances using the Systems Manager Inventory capability. References https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html https://docs.aws.amazon.com/config/latest/developerguide/querying-AWS-resources.html ","permalink":"https://abiydv.github.io/notes/aws/config/","summary":"What is it? AWS Config records and saves configuration of supported resources.\nYou can use these records to audit whether the resources align to best practices, or org specific patterns/rules. It can also highlight 2-way relationships between different resources. For ex - an instance is related to a vpc, a subnet, and a security group. This relation will show up in both the instance, and vpc records under relationships.\nResource Schema https://github.","title":"AWS Config"},{"content":"Consolidated Billing Cost Allocation Tags Cost Categories Cost and Usage Report v1 v2 Compute Optimizer Trusted Advisor Cost Explorer CUR-Athena-QuickSight Alerts Example here https://github.com/abiydv/aws-cost-alerts\nBudgets Anomaly Detector ","permalink":"https://abiydv.github.io/notes/aws/billing-and-cost/","summary":"Consolidated Billing Cost Allocation Tags Cost Categories Cost and Usage Report v1 v2 Compute Optimizer Trusted Advisor Cost Explorer CUR-Athena-QuickSight Alerts Example here https://github.com/abiydv/aws-cost-alerts\nBudgets Anomaly Detector ","title":"AWS Cost and Billing"},{"content":"NoSql, MongoDB compatible\nQuestions What is on-demand mode for DocumentDB ","permalink":"https://abiydv.github.io/notes/aws/document-db/","summary":"NoSql, MongoDB compatible\nQuestions What is on-demand mode for DocumentDB ","title":"AWS DocumentDB"},{"content":"Internals Depends on single leader replication, do not confuse with Amazon Dynamo which uses leaderless replication and is primarily used in S3.\nScope standard tables - regional global tables - global (cross-region replication) Table classes Standard Standard-IA infrequently accessed data. Similar to S3 storage classes? consider standard-IA if the storage cost \u0026gt; 50% throughput cost (read/write units) Capacity Modes on-demand provisioned throughput Cache DAX, DAX cluster - Dynamodb accelerator provisioned in a VPC.\nproviding in-memory cache, and increases performance by 10x\nAutoscaling Use auto scaling policy for [[AWS AutoScaling]] to adjust the provisioned read/write throughput to meet demand. Target tracking algo based on specified target utilization or based on schedule. If a table uses Global secondary index, ensure to enable the same auto scaling properties on that as well.\nReservation Cost savings compared to on-demand prices if you commit to an hourly use rate. Good option to reduce cost if the traffic pattern is predictable, well-understood and builds up gradually.\nPerformance Read Size Read Request Unit strongly consistent \u0026lt;= 4 KB 1 eventually consistent \u0026lt;= 4 KB 1.5 transactional ACID \u0026lt;= 4 KB 2 Write Size Write Request Unit ? \u0026lt;= 1 KB 1 transactional ACID \u0026lt;= 1 KB 2 Issue Performance issues when scaling are usually caused by selecting inefficient partition keys that do not load the partitions equally and therefore do not spread write or read load properly across the partitions. When this happens, it is called “hot keys” or “hot partitions.” Evaluate partition keys to understand if this is causing issues.\nFine grained access Provide access to principals on specific attributes or keys\nhttps://aws.amazon.com/blogs/aws/fine-grained-access-control-for-amazon-dynamodb/ - Using IAM policy\nKinesis Data Streams for DynamoDB Captures and stores information about data changes in a particular table. These records are held for upto 1 year.\nDuplicate records might appear in stream,\nDynamoDB streams Captures and stores information about data changes in a particular table. These records are held for 24 hours before being automatically removed.\nNO duplicate records appear in this stream.\nIt is very similar to Kinesis Data Streams, so you can use the KCL (Kinesis Client Library) to process streams. Using Streams Kinesis Adapter allows interacting with the low level streams api by using KCL.\ngraph LR a(My App) --\u003e k1(KCL) --\u003e k2(Kinesis Adapter) k3(Kinesis Adapter) --\u003e s1(AWS SDK) --\u003e s2(Streams API) --\u003e s(Streams) Streams can trigger synchronous lambda functions as new records are added to it. This lambda can be used to perform actions based on a record change in the table. Maybe save before/after to S3 to create an audit trail.\n","permalink":"https://abiydv.github.io/notes/aws/dynamodb/","summary":"Internals Depends on single leader replication, do not confuse with Amazon Dynamo which uses leaderless replication and is primarily used in S3.\nScope standard tables - regional global tables - global (cross-region replication) Table classes Standard Standard-IA infrequently accessed data. Similar to S3 storage classes? consider standard-IA if the storage cost \u0026gt; 50% throughput cost (read/write units) Capacity Modes on-demand provisioned throughput Cache DAX, DAX cluster - Dynamodb accelerator provisioned in a VPC.","title":"AWS DynamoDB"},{"content":"Scope AZ scope - create in 1 AZ\n5 9s SLA\nManage snapshots using Amazon Data Lifecycle Manager, which uses AWS Systems Manager internally. Supports custom or default policies.\nDefault encryption can be enable under Account Attributes \u0026gt; EC2 settings \u0026gt; EBS encryption.\n","permalink":"https://abiydv.github.io/notes/aws/ebs/","summary":"Scope AZ scope - create in 1 AZ\n5 9s SLA\nManage snapshots using Amazon Data Lifecycle Manager, which uses AWS Systems Manager internally. Supports custom or default policies.\nDefault encryption can be enable under Account Attributes \u0026gt; EC2 settings \u0026gt; EBS encryption.","title":"AWS Elastic Block Storage (EBS)"},{"content":"Scope az scope - specify 1 az\nMulti region replication? AMI = replicate via lambda, or life cycle rules or AWS Backup\nAutoScaling AutoScaling groups - can use launch templates or launch configs.\nLaunch templates allow using latest EC2 features including selecting instances based on defined attributes rather than adding a fixed instance type.\nLifecycle hooks Lifecycle hooks provide a way to trigger custom workflows based on the scale-in or scale-out activity in an auto scaling group. By default, the hooks provide a 1 hr window before transitioning to the next stage. Alternatively, the custom workflow can post either ABANDON or CONTINUE using complete-lifecycle-action cli command.\naws autoscaling complete-lifecycle-action --lifecycle-action-result CONTINUE --lifecycle-hook-name my-launch-hook --auto-scaling-group-name my-asg --lifecycle-action-token alpha-numeric-token When the instance is being terminated, both ABANDON ang CONTINUE result in termination. ABANDON only prevents any more subsequent lifecycle hooks to run.\nWhen the instance is being created, ABANDON prevents the instance from being launched, while CONTINUE allows the instance to be launched.\nBy default, the lifecycle hooks publish events to eventbridge, from where, rules can be created to send these events to any supported destination like SQS, SNS, Lambda etc.\nUsecase capture an instance termination event, copy files or logs from the instance before they get lost! use the lifecycle hook command in the instance userdata script to signal to auto-scaling group all initialization steps are complete, and the instance can be marked in service ready to accept traffic. Termination Protection Enable EC2 termination protection - not applicable to services like Autoscaling\nSuspend Autoscaling ReplaceUnhealthy scaling process\nInstance scale-in protection on Autoscaling group\nSystems Manager Placement Groups Cluster – Packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of high-performance computing (HPC) applications. Partition – Spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. Spread – Strictly places a small group of instances across distinct underlying hardware to reduce correlated failures. Purchasing Options On-Demand Reserved Instances Standard RI provides you with a significant discount on EC2 usage (a particular instance family) when you commit to a one-year or three-year term.\nConvertible RI Region specific, allows for customization to instance properties down the road, or even exchange for a new convertible RI\nZonal RI When a Standard or Convertible RI is scoped to a specific Availability Zone (AZ), instance capacity matching the exact RI configuration is reserved for your use (these are referred to as “zonal RIs”). Zonal RIs give you additional confidence in your ability to launch instances when you need them.\nRegional RI Regional RIs provide AZ and instance size flexibility but no capacity reservation\nSpot Dedicated Hosts Dedicated Instances Capacity Reservations Capacity Blocks for ML Reserve GPU capacity for ML workloads on UltraClusters.\nreserve upto 14 days in 1 day increments\ninstances - 1, 2, 4, 8, 16, 32 and 64\ninstance type region p5.48xlarge us-east-1, us-east-2 p4d.24xlarg us-east-2, us-west-2 On-demand capacity reservation Reserve capacity for Prime day like events to ensure you get all the capacity you need on your big day.\nFleet Launch mix of instances cross-az with on-demand, spot, reserved ri billing models.\nSavings Plans Compute Savings Plans = EC2, Fargate, Lambda\nEC2 Instance Savings Plans = EC2 family in a region\nQuestions What are Burstable Instances\nuses and differences between reserved instances, compute savings plans, capacity reservations? Is there anything called instance savings plan?\n","permalink":"https://abiydv.github.io/notes/aws/ec2/","summary":"Scope az scope - specify 1 az\nMulti region replication? AMI = replicate via lambda, or life cycle rules or AWS Backup\nAutoScaling AutoScaling groups - can use launch templates or launch configs.\nLaunch templates allow using latest EC2 features including selecting instances based on defined attributes rather than adding a fixed instance type.\nLifecycle hooks Lifecycle hooks provide a way to trigger custom workflows based on the scale-in or scale-out activity in an auto scaling group.","title":"AWS Elastic Cloud Compute (EC2)"},{"content":"Scope az-scope\nauto-scaling in multi-az\nimages from anywhere, any region\n","permalink":"https://abiydv.github.io/notes/aws/ecs/","summary":"Scope az-scope\nauto-scaling in multi-az\nimages from anywhere, any region","title":"AWS Elastic Container Service (ECS)"},{"content":"Questions How does it compare with other AWS tools like VM import/export, Server Migration Service, AWS Migration Service (MGN) Unique property others don\u0026rsquo;t have - failover orchestration.\nElastic Disaster Recovery (DRS) The full name is Elastic Disaster Recovery. However, the short form is DRS (Disaster Recovery Service).\nWhat does it do? Failover from on-prem to AWS Failover from any cloud to AWS Failover from 1 AWS region to another AWS region No need to maintain a duplicate recovery site Agent based - need to install an agent on source server to replicate data to AWS. DRS launches replication servers on AWS side for replicating data from source, these servers are lightweight. During replication, EBS is created corresponding to the source server disk. Snapshots are taken at regular intervals and customers can define a retention period. Any recovery drill carried out on AWS side, has no impact on source apps or replication. Converts the source server boot disk to boot successfully on EC2, customers can provide custom scripts to complement this automatic conversion too. Replicate source network (only if source server is on AWS). Basically captures all configuration related to the source network (possibly from config) and creates a {{ relref \u0026ldquo;cloudformation\u0026rdquo; }} template to be deployed into a new region as part of recovery.\nWhat can it NOT do? Recover to Outposts that CloudEndure Disaster Recovery (discontinued for all regions except China, GovCloud) can do. Is it Regional or Global? Regional, does not support China or Gov Cloud.\nKey Concepts RPO RTO Staging Account Target Account Source Replication\nLimits? Default limit of 3000 servers into a single target account. Customers can use multiple target accounts, or request a limit increase. Limit of 300 servers into a single staging account. To recover say 1000 servers into a single target account, customers can use multiple staging accounts.\n","permalink":"https://abiydv.github.io/notes/aws/drs/","summary":"Questions How does it compare with other AWS tools like VM import/export, Server Migration Service, AWS Migration Service (MGN) Unique property others don\u0026rsquo;t have - failover orchestration.\nElastic Disaster Recovery (DRS) The full name is Elastic Disaster Recovery. However, the short form is DRS (Disaster Recovery Service).\nWhat does it do? Failover from on-prem to AWS Failover from any cloud to AWS Failover from 1 AWS region to another AWS region No need to maintain a duplicate recovery site Agent based - need to install an agent on source server to replicate data to AWS.","title":"AWS Elastic Disaster Recovery Service (DRS)"},{"content":"Scope AZ scope - multi-AZ (provide a list of subnets (AZs) to create)\nReplication across AZ\nTypes Regional One zone Features 4 9s SLA\nOnly for Linux\nSupports NFS v4.1 - higher performance Supports Portable Operating System Interface (POSIX) permissions\nAccess Control POSIX IAM Network controls like security groups, access control lists etc. Throughput Modes General Purpose Elastic ","permalink":"https://abiydv.github.io/notes/aws/efs/","summary":"Scope AZ scope - multi-AZ (provide a list of subnets (AZs) to create)\nReplication across AZ\nTypes Regional One zone Features 4 9s SLA\nOnly for Linux\nSupports NFS v4.1 - higher performance Supports Portable Operating System Interface (POSIX) permissions\nAccess Control POSIX IAM Network controls like security groups, access control lists etc. Throughput Modes General Purpose Elastic ","title":"AWS Elastic File Storage (EFS)"},{"content":"Workshop - https://www.eksworkshop.com/docs/introduction/\nPricing Static cost of a cluster is $0.1 per hour, so ~$70 per month.\nIn extended support (old k8s versions), each cluster costs $0.6 per hours (6x), so ~$420 per month.\nUpdate kubeconfig to access EKS aws eks update-kubeconfig --name my-cluster --region us-east-1 If cluster admin role is different from current role used to aut\naws eks update-kubeconfig --name my-cluster --role-arn arn:aws:iam::123456789000:role/eksrole --region us-east-1 More here - https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html\nWorkload AWS access Provide access to AWS workloads like S3 buckets, SNS topics etc via short lived temporary credentials.\nIRSA (IAM roles for service accounts) Pods can use service accounts which was backed by IAM roles and their permissions. Needs an annotation on k8s service account which refers to the IAM role arn. Principal in the role\u0026rsquo;s trust policy is the EKS cluster\u0026rsquo;s oidc provider.\nYou can add conditions to lock down which service accounts are able to use this role.\nannotation: eks.amazonaws.com/role-arn: arn:aws:iam::111122223333:role/my-role Example trust policy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::111122223333:oidc-provider/oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE:sub\u0026#34;: \u0026#34;system:serviceaccount:default:my-service-account\u0026#34;, \u0026#34;oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE:aud\u0026#34;: \u0026#34;sts.amazonaws.com\u0026#34; } } } ] } More here https://docs.aws.amazon.com/eks/latest/userguide/associate-service-account-role.html\nPod Identities Pods can assume an IAM role directly via k8s service account, without the need of an annotation on the service account. It need a pod identity association at cluster level (needs namespace, service account name). An agent runs on each node which makes this possible. The role\u0026rsquo;s trust policy is different than when using IRSA. The principal here is service: pods.eks.amazonaws.com\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowEksAuthToAssumeRoleForPodIdentity\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;pods.eks.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;sts:TagSession\u0026#34; ] } ] } Following cdk code snippet (typescript) creates such a role, and adds a pod identity association.\nconst podIdentityServicePrincipal = new iam.SessionTagsPrincipal(new iam.ServicePrincipal(\u0026#34;pods.eks.amazonaws.com\u0026#34;)) const workloadPodIdentityRole = new iam.Role(this, \u0026#34;WorkloadPodIdentityRole\u0026#34;, { assumedBy: podIdentityServicePrincipal, }); workloadPolicy.attachToRole(WorkloadPodIdentityRole); new eks.CfnPodIdentityAssociation(this, \u0026#34;WorkloadPodIdentityRoleAssociation\u0026#34;, { clusterName: cluster.clusterName, roleArn: workloadPodIdentityRole.roleArn, namespace: \u0026#34;workload\u0026#34;, serviceAccount: \u0026#34;workload-podidentity\u0026#34;, }); More here https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-eks-podidentityassociation.html\nNetworking https://aws.github.io/aws-eks-best-practices/networking/\nhttps://github.com/aws/amazon-vpc-cni-k8s\nAdvanced Networking https://aws.amazon.com/blogs/containers/eks-vpc-routable-ip-address-conservation/\ncan use both Fargate and EC2 clusters to launch nodes. STS endpoint for the deployment region must be enabled. worker nodes must have a private DNS entry or a node not found error will occur. node IAM role must be present in the aws-auth-cm.yaml file when using self-managed Amazon Linux nodes. Ref: https://docs.aws.amazon.com/eks/latest/userguide/troubleshooting.html, https://docs.aws.amazon.com/eks/latest/userguide/eks-connector.html\nAdd-ons Fetch available config options via aws cli\n$ aws eks describe-addon-versions --addon-name vpc-cni # provides available versions $ aws eks describe-addon-configuration --addon-name vpc-cni --addon-version v1.12.0-eksbuild.1 # provides config options under configurationSchema { \u0026#34;addonName\u0026#34;: \u0026#34;vpc-cni\u0026#34;, \u0026#34;addonVersion\u0026#34;: \u0026#34;v1.12.0-eksbuild.1\u0026#34;, \u0026#34;configurationSchema\u0026#34;: \u0026#34;{\\\u0026#34;$ref\\\u0026#34;:\\\u0026#34;#/definitions/VpcCni\\\u0026#34;,\\\u0026#34;$schema\\\u0026#34;:\\\u0026#34;http://json-schema.org/draft-06/schema#\\\u0026#34;,\\\u0026#34;definitions\\\u0026#34;:{\\\u0026#34;Cri\\\u0026#34;:{\\\u0026#34;additionalProperties\\\u0026#34;:false,\\\u0026#34;properties\\\u0026#34;:{\\\u0026#34;hostPath\\\u0026#34;:{\\\u0026#34;$ref\\\u0026#34;:\\\u0026#34;#/definitions/HostPath\\\u0026#34;}},\\\u0026#34;title\\\u0026#34;:\\\u0026#34;Cri\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;object\\\u0026#34;},\\\u0026#34;Env\\\u0026#34;:{\\\u0026#34;additionalProperties\\\u0026#34;:false,\\\u0026#34;properties\\\u0026#34;:{\\\u0026#34;ADDITIONAL_ENI_TAGS\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;AWS_VPC_CNI_NODE_PORT_SUPPORT\\\u0026#34;:{\\\u0026#34;format\\\u0026#34;:\\\u0026#34;boolean\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;AWS_VPC_ENI_MTU\\\u0026#34;:{\\\u0026#34;format\\\u0026#34;:\\\u0026#34;integer\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;AWS_VPC_K8S_CNI_CONFIGURE_RPFILTER\\\u0026#34;:{\\\u0026#34;format\\\u0026#34;:\\\u0026#34;boolean\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG\\\u0026#34;:{\\\u0026#34;format\\\u0026#34;:\\\u0026#34;boolean\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;AWS_VPC_K8S_CNI_EXTERNALSNAT\\\u0026#34;:{\\\u0026#34;format\\\u0026#34;:\\\u0026#34;boolean\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;AWS_VPC_K8S_CNI_LOGLEVEL\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;AWS_VPC_K8S_CNI_LOG_FILE\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;AWS_VPC_K8S_CNI_RANDOMIZESNAT\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;AWS_VPC_K8S_CNI_VETHPREFIX\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;AWS_VPC_K8S_PLUGIN_LOG_FILE\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;AWS_VPC_K8S_PLUGIN_LOG_LEVEL\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;DISABLE_INTROSPECTION\\\u0026#34;:{\\\u0026#34;format\\\u0026#34;:\\\u0026#34;boolean\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;DISABLE_METRICS\\\u0026#34;:{\\\u0026#34;format\\\u0026#34;:\\\u0026#34;boolean\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;DISABLE_NETWORK_RESOURCE_PROVISIONING\\\u0026#34;:{\\\u0026#34;format\\\u0026#34;:\\\u0026#34;boolean\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;ENABLE_POD_ENI\\\u0026#34;:{\\\u0026#34;format\\\u0026#34;:\\\u0026#34;boolean\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;ENABLE_PREFIX_DELEGATION\\\u0026#34;:{\\\u0026#34;format\\\u0026#34;:\\\u0026#34;boolean\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;WARM_ENI_TARGET\\\u0026#34;:{\\\u0026#34;format\\\u0026#34;:\\\u0026#34;integer\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;WARM_PREFIX_TARGET\\\u0026#34;:{\\\u0026#34;format\\\u0026#34;:\\\u0026#34;integer\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;}},\\\u0026#34;title\\\u0026#34;:\\\u0026#34;Env\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;object\\\u0026#34;},\\\u0026#34;HostPath\\\u0026#34;:{\\\u0026#34;additionalProperties\\\u0026#34;:false,\\\u0026#34;properties\\\u0026#34;:{\\\u0026#34;path\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;}},\\\u0026#34;title\\\u0026#34;:\\\u0026#34;HostPath\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;object\\\u0026#34;},\\\u0026#34;Limits\\\u0026#34;:{\\\u0026#34;additionalProperties\\\u0026#34;:false,\\\u0026#34;properties\\\u0026#34;:{\\\u0026#34;cpu\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;},\\\u0026#34;memory\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;string\\\u0026#34;}},\\\u0026#34;title\\\u0026#34;:\\\u0026#34;Limits\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;object\\\u0026#34;},\\\u0026#34;Resources\\\u0026#34;:{\\\u0026#34;additionalProperties\\\u0026#34;:false,\\\u0026#34;properties\\\u0026#34;:{\\\u0026#34;limits\\\u0026#34;:{\\\u0026#34;$ref\\\u0026#34;:\\\u0026#34;#/definitions/Limits\\\u0026#34;},\\\u0026#34;requests\\\u0026#34;:{\\\u0026#34;$ref\\\u0026#34;:\\\u0026#34;#/definitions/Limits\\\u0026#34;}},\\\u0026#34;title\\\u0026#34;:\\\u0026#34;Resources\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;object\\\u0026#34;},\\\u0026#34;VpcCni\\\u0026#34;:{\\\u0026#34;additionalProperties\\\u0026#34;:false,\\\u0026#34;properties\\\u0026#34;:{\\\u0026#34;cri\\\u0026#34;:{\\\u0026#34;$ref\\\u0026#34;:\\\u0026#34;#/definitions/Cri\\\u0026#34;},\\\u0026#34;env\\\u0026#34;:{\\\u0026#34;$ref\\\u0026#34;:\\\u0026#34;#/definitions/Env\\\u0026#34;},\\\u0026#34;resources\\\u0026#34;:{\\\u0026#34;$ref\\\u0026#34;:\\\u0026#34;#/definitions/Resources\\\u0026#34;}},\\\u0026#34;title\\\u0026#34;:\\\u0026#34;VpcCni\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;object\\\u0026#34;}}}\u0026#34; } Find all available versions, along with supported k8s versions\n$ aws eks describe-addon-versions --addon-name aws-efs-csi-driver | jq -cr \u0026#39;.addons[0].addonVersions[]| [.addonVersion,.compatibilities[].clusterVersion] | @csv\u0026#39; \u0026#34;v2.0.7-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v2.0.6-eksbuild.2\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v2.0.6-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v2.0.5-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v2.0.4-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v2.0.3-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v2.0.2-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v2.0.1-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v2.0.0-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v1.7.7-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v1.7.6-eksbuild.2\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v1.7.6-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v1.7.5-eksbuild.2\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v1.7.5-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v1.7.4-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v1.7.3-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v1.7.2-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v1.7.1-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v1.7.0-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v1.5.9-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34; \u0026#34;v1.5.8-eksbuild.1\u0026#34;,\u0026#34;1.30\u0026#34;,\u0026#34;1.29\u0026#34;,\u0026#34;1.28\u0026#34;,\u0026#34;1.27\u0026#34;,\u0026#34;1.26\u0026#34;,\u0026#34;1.25\u0026#34;,\u0026#34;1.24\u0026#34;,\u0026#34;1.23\u0026#34;,\u0026#34;1.22\u0026#34; Refer\nKubecost Mix of enterprise and opensource features at no additional cost.\nComparison here.\n","permalink":"https://abiydv.github.io/notes/aws/eks/","summary":"Workshop - https://www.eksworkshop.com/docs/introduction/\nPricing Static cost of a cluster is $0.1 per hour, so ~$70 per month.\nIn extended support (old k8s versions), each cluster costs $0.6 per hours (6x), so ~$420 per month.\nUpdate kubeconfig to access EKS aws eks update-kubeconfig --name my-cluster --region us-east-1 If cluster admin role is different from current role used to aut\naws eks update-kubeconfig --name my-cluster --role-arn arn:aws:iam::123456789000:role/eksrole --region us-east-1 More here - https://docs.","title":"AWS Elastic Kubernetes Service (EKS)"},{"content":"EMR Fleet mix of spot and on-demand instances Provide a list of up to 5 instance types with corresponding weighted capacities and spot bid prices (including spot blocks)! EMR will automatically provision On-Demand and Spot capacity across these instance types when creating your cluster. This can make it easier and more cost effective to quickly obtain and maintain your desired capacity for your clusters. EMR States EMR Alarms, Metrics https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html\n","permalink":"https://abiydv.github.io/notes/aws/emr/","summary":"EMR Fleet mix of spot and on-demand instances Provide a list of up to 5 instance types with corresponding weighted capacities and spot bid prices (including spot blocks)! EMR will automatically provision On-Demand and Spot capacity across these instance types when creating your cluster. This can make it easier and more cost effective to quickly obtain and maintain your desired capacity for your clusters. EMR States EMR Alarms, Metrics https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html","title":"AWS Elastic Map Reduce (EMR)"},{"content":"open-source project, deployed on, and managed by AWS\nIn-memory key-value NoSql database. An in-memory cache can help improve application performance and user experience.\nScope AZ scope\nSupports Multi-AZ (read replicas)\nSupports Auto-scaling (shards)\nFeatures Async Replication\nEnable in-transit encryption TransitEncryptionEnabled:true\nEnable at-rest enryption AtRestEncryptionEnabled:true\nIt can only be enabled at the time of creation, can have some performance impact due to encryption/decryption\nRedis Redis is the most popular option for caching relational database queries.\nAuthentication Redis Auth, for self-designed clusters using in-transit encryption only.\nSetup during replication group creation by passing the argument --auth-token authtoken. Token can be changed by using either ROTATE or SET.\nTo set auth on an existing cluster, provide the token, and use --auth-token-update-strategy=ROTATE.\nRBAC has superseeded AUTH for Redis 6.0 onwards.\nAn access string applied to the user or group provides the least priviledge access. Example string ~objects:* ~items:* ~public:*\nMore here https://docs.redis.com/latest/rs/security/access-control/rbac/\nMemcached ","permalink":"https://abiydv.github.io/notes/aws/elasticache/","summary":"open-source project, deployed on, and managed by AWS\nIn-memory key-value NoSql database. An in-memory cache can help improve application performance and user experience.\nScope AZ scope\nSupports Multi-AZ (read replicas)\nSupports Auto-scaling (shards)\nFeatures Async Replication\nEnable in-transit encryption TransitEncryptionEnabled:true\nEnable at-rest enryption AtRestEncryptionEnabled:true\nIt can only be enabled at the time of creation, can have some performance impact due to encryption/decryption\nRedis Redis is the most popular option for caching relational database queries.","title":"AWS ElastiCache"},{"content":"Scope AZ scope\nFeatures 4 9s SLA\nConnect to S3 using \u0026ldquo;Data Repository\u0026rdquo;\nAuto import, auto export to keep data in FSx in sync with data in S3\nLazy loads the files as they are accessed so there could be latency when the file is first accessed\nFSx, and S3 bucket should be in same region to setup auto-import\nPOSIX permissions are maintained when moving data between S3 and FSx\nAWS File Cache is also a thing where it provides milisecond access to data\nSupports Windows (FSx for Windows)\nSupports Linux (FSx for Lustre)\nQuestions Auto-export works cross region too? Which protocols are supported by which FSx?\nFSx for - Windows File Server Lustre ONTAP OpenZFS ","permalink":"https://abiydv.github.io/notes/aws/fsx/","summary":"Scope AZ scope\nFeatures 4 9s SLA\nConnect to S3 using \u0026ldquo;Data Repository\u0026rdquo;\nAuto import, auto export to keep data in FSx in sync with data in S3\nLazy loads the files as they are accessed so there could be latency when the file is first accessed\nFSx, and S3 bucket should be in same region to setup auto-import\nPOSIX permissions are maintained when moving data between S3 and FSx","title":"AWS File Storage (FSx)"},{"content":"AWS Firewall Manager simplifies your AWS WAF administration and helps you enforce WAF rules on the resources across all the accounts in an AWS Organization by using AWS Config in the background. AWS Firewall Manager also enables you to selectively apply the rules to specific resources.\nAWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to CloudFront or Application Load Balancer.\nAWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses from which requests originate or the values of query strings.\nCloudFront or Application Load Balancer will respond to requests either with the requested content or with an HTTP 403 status code (Forbidden). You also can configure CloudFront to return a custom error page when a request is blocked.\n","permalink":"https://abiydv.github.io/notes/aws/firewall-manager/","summary":"AWS Firewall Manager simplifies your AWS WAF administration and helps you enforce WAF rules on the resources across all the accounts in an AWS Organization by using AWS Config in the background. AWS Firewall Manager also enables you to selectively apply the rules to specific resources.\nAWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to CloudFront or Application Load Balancer.","title":"AWS Firewall Manager"},{"content":"Scope Global service\nFeatures Provides anycast static IP addresses from AWS Edge Network 2 IPv4, 2 IPv6\nSupports BYOIP\nStatic IP are released only when an accelerator is deleted\nTCP termination at edge\nControl traffic % directed to a region\nS3 Multi-Region Access Points\nTypes Standard routes traffic based on client location, backend health, and custom policies Endpoints - NLB, ALB, EC2, Elastic IPs across multiple regions Custom routes traffic to private IPs in a subnet Endpoints - vpc subnet. ","permalink":"https://abiydv.github.io/notes/aws/global-accelerator/","summary":"Scope Global service\nFeatures Provides anycast static IP addresses from AWS Edge Network 2 IPv4, 2 IPv6\nSupports BYOIP\nStatic IP are released only when an accelerator is deleted\nTCP termination at edge\nControl traffic % directed to a region\nS3 Multi-Region Access Points\nTypes Standard routes traffic based on client location, backend health, and custom policies Endpoints - NLB, ALB, EC2, Elastic IPs across multiple regions Custom routes traffic to private IPs in a subnet Endpoints - vpc subnet.","title":"AWS Global Accelerator"},{"content":"Access Analyzer https://aws.amazon.com/blogs/security/how-to-prioritize-iam-access-analyzer-findings/\nIn addition to roles, it also analyzes resource policies on supported resources\nMonitors resources for external access AWS Simple Storage(S3) buckets AWS Identity and Access Management roles AWS Key Management Service Keys AWS Lambda functions and layers AWS Simple Queue Service queues AWS Secrets Manager secrets\nIdentity Center SSO for Org\nPolicy Conditions Useful to allow/deny actions based on certain conditions in addition to resource arn or patterns.\nCan be used for ABAC\nTag based access control\nUse the Service Authorization page to find which service or actions support which condition keys.\nContext keys can be single valued or multi valued, more here\nCondition Operators\nEvaluation logic - AND, OR conditions\nGlobal Keys These are available for most services and actions\nAll keys need an aws: suffix unless the prefix is specifically mentioned for a condition, liek ec2:RoleDelivery\nPrincipal Example Value Why use it? Notes PrincipalArn PrincipalAccount PrincipalOrgPaths PrincipalOrgID PrincipalTag/TagKey PrincipalIsAWSService PrincipalServiceName PrincipalServiceNamesList PrincipalType userid username Role Session Example Value Why use it? Notes FederatedProvider TokenIssueTime MultiFactorAuthPresent Ec2InstanceSourceVpc Ec2InstanceSourcePrivateIPv4 SourceIdentity ec2:RoleDelivery ec2:SourceInstanceArn glue:RoleAssumedBy glue:CredentialIssuingService lambda:SourceFunctionArn ssm:SourceInstanceArn identitystore:UserId Network Example Value Why use it? Notes SourceIp SourceVpc SourceVpce VpcSourceIp Resource Properties Example Value Why use it? Notes ResourceAccount ResourceOrgPaths ResourceOrgID ResourceTag/TagKey Request Properties Example Values Why use it? CalledVia CalledViaFirst CalledViaLast ViaAWSService CurrentTime EpochTime referer RequestedRegion us-east-1 Implement region deny controls by using this condition in an IAM policy or an SCP RequestTag/TagKey Implement standard tags. Deny action if tag not present. Not all resources support request tags (RDS doesn\u0026rsquo;t!), so needs to be applied with other controls. TagKeys SecureTransport true, false Implement encryption-in-transit by denying actions which do not have this enable. SourceArn arn:aws:sns:us-east-2:123456789012:MyTopic Use in a SQS queue resource policy to allow SNS to send message to the queue. Since principal is sns in this case, without the condition it can lead to the [[confused deputy problem]]. SourceAccount SourceOrgPaths SourceOrgID UserAgent Refer AWS docs\nExamples Not working\n\u0026#34;Condition\u0026#34;: {\u0026#34;StringEquals\u0026#34;: {\u0026#34;aws:TagKeys\u0026#34;: [\u0026#34;Department\u0026#34;]}} Working\n\u0026#34;Condition\u0026#34;: {\u0026#34;ForAllValues:StringEquals\u0026#34;: {\u0026#34;aws:TagKeys\u0026#34;: [\u0026#34;Department\u0026#34;]}} Working\n\u0026#34;Condition\u0026#34;: {\u0026#34;ForAnyValue:StringEquals\u0026#34;: {\u0026#34;aws:TagKeys\u0026#34;: [\u0026#34;Department\u0026#34;]}} Managed Policy Refer https://docs.aws.amazon.com/aws-managed-policy/latest/reference/policy-list.html\n","permalink":"https://abiydv.github.io/notes/aws/iam/","summary":"Access Analyzer https://aws.amazon.com/blogs/security/how-to-prioritize-iam-access-analyzer-findings/\nIn addition to roles, it also analyzes resource policies on supported resources\nMonitors resources for external access AWS Simple Storage(S3) buckets AWS Identity and Access Management roles AWS Key Management Service Keys AWS Lambda functions and layers AWS Simple Queue Service queues AWS Secrets Manager secrets\nIdentity Center SSO for Org\nPolicy Conditions Useful to allow/deny actions based on certain conditions in addition to resource arn or patterns.","title":"AWS Identity and Access Management (IAM)"},{"content":"If you create a transit gateway attachment using the public subnet, the instance traffic is routed to the internet gateway, but the internet gateway drops the traffic because the instances don\u0026rsquo;t have public IP addresses\nEgress Only Internet Gateway is IPv6 only.\nhttps://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-nat-igw.html\n","permalink":"https://abiydv.github.io/notes/aws/internet-gateway/","summary":"If you create a transit gateway attachment using the public subnet, the instance traffic is routed to the internet gateway, but the internet gateway drops the traffic because the instances don\u0026rsquo;t have public IP addresses\nEgress Only Internet Gateway is IPv6 only.\nhttps://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-nat-igw.html","title":"AWS Internet Gateway"},{"content":"Iot Core Region specific endpoints\nDevice Gateway - pub/sub model\nDevice Shadow - properties of connected devices, could have been called fingerprint?\nSupports MQTT\nRegistry - Organize and track devices. Each device has it\u0026rsquo;s unique handle in the registry @myawesomelightbulb.\nRules Engine - SQL like rules to filter/transform and forward incoming device metrics/data to other AWS/3rd party services\nThing Type - Categorize similar devices with common attributes (up tp 50, 3 searchable)\nSimplified Permission Management - x.509 certificates\nIoT Greengrass Edge runtime\nProcess data locally\nPerform inferences based on ML models\nDeploy lambda (with Greengrass SDK) to these devices\nIoT Message Broker IoT devices\u0026rsquo; telemetry, device management, device security and IoT Analysis.\nSupports industry standard lightweight protocols such as MQTT, HTTP and MQTT over WebSocket.\n","permalink":"https://abiydv.github.io/notes/aws/iot/","summary":"Iot Core Region specific endpoints\nDevice Gateway - pub/sub model\nDevice Shadow - properties of connected devices, could have been called fingerprint?\nSupports MQTT\nRegistry - Organize and track devices. Each device has it\u0026rsquo;s unique handle in the registry @myawesomelightbulb.\nRules Engine - SQL like rules to filter/transform and forward incoming device metrics/data to other AWS/3rd party services\nThing Type - Categorize similar devices with common attributes (up tp 50, 3 searchable)","title":"AWS IoT"},{"content":"Refer https://docs.aws.amazon.com/vpc/latest/ipam/what-it-is-ipam.html\nRefer https://aws.amazon.com/blogs/networking-and-content-delivery/collecting-aws-networking-information-in-large-multi-account-environments/\n","permalink":"https://abiydv.github.io/notes/aws/ipam/","summary":"Refer https://docs.aws.amazon.com/vpc/latest/ipam/what-it-is-ipam.html\nRefer https://aws.amazon.com/blogs/networking-and-content-delivery/collecting-aws-networking-information-in-large-multi-account-environments/","title":"AWS IPAM"},{"content":"Questions What is a cloudhsm in kms? Use AWS CloudHSM when you need to manage the hardware security modules (HSMs) that generate and store your encryption keys. In AWS CloudHSM, you create and manage HSMs, including creating users and setting their permissions. You also create the symmetric keys and asymmetric key pairs that the HSM stores. Features Think of KMS as an additional layer of authentication and authorization. If a client cannot access the AWS KMS, the public key from an asymmetrical key pair can be used to encrypt the data. The data can then be encrypted by the AWS KMS that has the private key. Ref https://docs.aws.amazon.com/kms/latest/developerguide/symmetric-asymmetric.htm ","permalink":"https://abiydv.github.io/notes/aws/kms/","summary":"Questions What is a cloudhsm in kms? Use AWS CloudHSM when you need to manage the hardware security modules (HSMs) that generate and store your encryption keys. In AWS CloudHSM, you create and manage HSMs, including creating users and setting their permissions. You also create the symmetric keys and asymmetric key pairs that the HSM stores. Features Think of KMS as an additional layer of authentication and authorization. If a client cannot access the AWS KMS, the public key from an asymmetrical key pair can be used to encrypt the data.","title":"AWS Key Management Service (KMS)"},{"content":"Scope region scope - specify multiple az\ncode in the same region\nFeatures max runtime 15 mins\nmax resource - vcpu ?, memory ?\nCan be triggered by CloudWatch alarm directly now - cross-region\nAlias - allows to test a new version without impacting live traffic which is served via an alias. If testing is successful, point the live alias to the new function version. Sort of, allows you perform [[devops#Canary deployments|canary deployments]] for Lambda.\nRouting to a specific alias? #question\nError Error: Timed Out If some of your TCP connections are timing out, this may be due to packet fragmentation. Lambda functions cannot handle incoming fragmented TCP requests, since Lambda does not support IP fragmentation for TCP or ICMP.\nDeployment package Container images Upto 10 GB in size\nZip file archives Upload directly to Lambda (\u0026lt;50 MB) or to S3 (\u0026gt; 50 MB), and provide the S3 url to the Lambda service.\nLayers Use for reusable code, cache session objects, shared libraries, telemetry, etc.\nShare dependencies across multiple functions\nSeparate core app logic from dependecies\nReduce the app deployment package size\nUseful for zip file archives only. When using containers, these dependencies can be added to the image directly.\nCross Service APIs Lambda Functions manage EC2 ENIs for functions created in a [[vpc]]. Initially, terraform aws provider helped cleanup these ENIs on function removal since AWS api did not. Later, AWS updated the API to delete these lingering ENIs and that started causing failures in terraform as it could no longer find the lingering ENI when trying to clean-up.\nHashiCorp and AWS had to do a large scale outreach to fix these configs, and help practitioners migrate.\nRefer terraform-provider-aws Run deleteLingeringLambdaENIs when subnets are destroyed ec2: Prevent \u0026ldquo;InvalidNetworkInterfaceID.NotFound\u0026rdquo; errors in deleteLingeringLambdaENIs ","permalink":"https://abiydv.github.io/notes/aws/lambda/","summary":"Scope region scope - specify multiple az\ncode in the same region\nFeatures max runtime 15 mins\nmax resource - vcpu ?, memory ?\nCan be triggered by CloudWatch alarm directly now - cross-region\nAlias - allows to test a new version without impacting live traffic which is served via an alias. If testing is successful, point the live alias to the new function version. Sort of, allows you perform [[devops#Canary deployments|canary deployments]] for Lambda.","title":"AWS Lambda"},{"content":"AWS Application Load Balancer AWS Network Load Balancer AWS Gateway Load Balancer GWLB’s ability to use 5-tuples or 3-tuples of an IP packet to select specific appliance behind it for life of that flow combined with Transit Gateway appliance mode, provides session stickiness irrespective of source and destination AZ. 1\nA Gateway Load Balancer operates at the third layer of the Open Systems Interconnection (OSI) model, the network layer.\nIt maintains stickiness of flows to a specific target appliance using 5-tuple (for TCP/UDP flows) or 3-tuple (for non-TCP/UDP flows). The Gateway Load Balancer and its registered virtual appliance instances exchange application traffic using the GENEVE protocol on port 6081.\nFeatures Stickiness Cross-AZ load balancing Health Checks Alternatives Modern applications can make use of Service Discovery to eliminate the need of load balancers, reducing latency, and cross zone data transfer costs\nQuestions Routing Algorithms? and which load balancer supports which algo? References https://docs.aws.amazon.com/whitepapers/latest/modern-application-development-on-aws/service-discovery-and-service-registries.html 1 https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-inspection-architecture-with-aws-gateway-load-balancer-and-aws-transit-gateway/\n","permalink":"https://abiydv.github.io/notes/aws/load-balancers/","summary":"AWS Application Load Balancer AWS Network Load Balancer AWS Gateway Load Balancer GWLB’s ability to use 5-tuples or 3-tuples of an IP packet to select specific appliance behind it for life of that flow combined with Transit Gateway appliance mode, provides session stickiness irrespective of source and destination AZ. 1\nA Gateway Load Balancer operates at the third layer of the Open Systems Interconnection (OSI) model, the network layer.\nIt maintains stickiness of flows to a specific target appliance using 5-tuple (for TCP/UDP flows) or 3-tuple (for non-TCP/UDP flows).","title":"AWS Load Balancers"},{"content":"What is local zones? AWS Local Zones are a new type of AWS infrastructure designed to run workloads that require single-digit millisecond latency, like video rendering and graphics intensive, virtual desktop applications. Local Zones allow customers to gain all the benefits of having compute and storage resources closer to end-users, without the need to own and operate their own data center infrastructure. Local Zones are available in 33 metros globally. enable the AWS Local Zones for your AWS account before you can deploy resources to them.\n","permalink":"https://abiydv.github.io/notes/aws/local-zones/","summary":"What is local zones? AWS Local Zones are a new type of AWS infrastructure designed to run workloads that require single-digit millisecond latency, like video rendering and graphics intensive, virtual desktop applications. Local Zones allow customers to gain all the benefits of having compute and storage resources closer to end-users, without the need to own and operate their own data center infrastructure. Local Zones are available in 33 metros globally. enable the AWS Local Zones for your AWS account before you can deploy resources to them.","title":"AWS Local Zones"},{"content":"Features Stateful Fully managed IPS (Intrusion Prevention Service) DPI (Deep Packet Inspection) References https://aws.amazon.com/blogs/networking-and-content-delivery/deployment-models-for-aws-network-firewall https://aws.amazon.com/blogs/networking-and-content-delivery/deployment-models-for-aws-network-firewall-with-vpc-routing-enhancements ","permalink":"https://abiydv.github.io/notes/aws/network-firewall/","summary":"Features Stateful Fully managed IPS (Intrusion Prevention Service) DPI (Deep Packet Inspection) References https://aws.amazon.com/blogs/networking-and-content-delivery/deployment-models-for-aws-network-firewall https://aws.amazon.com/blogs/networking-and-content-delivery/deployment-models-for-aws-network-firewall-with-vpc-routing-enhancements ","title":"AWS Network Firewall"},{"content":"Features cross-zone disabled by default\nNLB provides a network interface in each AZ backed by a static IP. This static IP can be used to configure AZ specific DNS records.\nNLB forwards the requests to the same AZ target in which it receives the request. This is useful to maintain AZ affinity and reduce costs due to cross AZ traffic.\nNLB is a layer-4 device, works on layer-4 protocols, and is inherently sticky. Applications should close connections to not overload a single server behind NLB.\nRef Multi AZ architectures\n","permalink":"https://abiydv.github.io/notes/aws/nlb/","summary":"Features cross-zone disabled by default\nNLB provides a network interface in each AZ backed by a static IP. This static IP can be used to configure AZ specific DNS records.\nNLB forwards the requests to the same AZ target in which it receives the request. This is useful to maintain AZ affinity and reduce costs due to cross AZ traffic.\nNLB is a layer-4 device, works on layer-4 protocols, and is inherently sticky.","title":"AWS Network Load Balancer (NLB)"},{"content":"Multi-account architectures New account - does not inherit all required properties from an org. Ensure details are updated before leaving the org.\nPayment method, Tax information, Contact info, Root password etc.\nConsolidated billing cannot use cost-allocation tags in member accounts, only possible in payer account, which is the management account for an organization. aws support can not do this.\ncannot disable cost savings plan, and reservations in member accounts. only possible to limit it to a particular account from the org management/payer account.\nwhat is compliance report? uses? runs every 48 hrs\nPrivate Marketplace?\nEnable All Features or Consolidated Billing Only All Features is the default when creating new Organization Invited accounts must accept the reqeust to enable all features before it can be enabled. Not applicable for accounts created via the Organization You cannot revert to Consolidated Billing only mode after switching to All features SCPs depend on all features enabled SSO depends on all features enabled Tag Policy Stanardize on tags\nDoesn\u0026rsquo;t do anything if a resource is missing a tag key 🤷‍♂️, best to use in combination of SCP\nTo find all resources without any tag consider using Resource Manager. This is not possible to get from organization compliance report. example - aws resource-explorer-2 search --query-string \u0026quot;tag:none\u0026quot;\nresource manager report can be run every 36 hours but only supports 1000 resources max.\nService Control Policy How does Inheritance work for SCP?\nIt behaves differently depending on whether the policy uses allow or deny statements.\nDeny policy - any deny statement from the root org to the account OU will result in a deny action\nAllow policy - ALL SCP from the root of the org, must have an allow statement for an action to be allowed in the account. Because of this, you will find a AWSFullAccess SCP applied at the root of an Org by default.\nDeny public IP\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:RunInstances\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;Bool\u0026#34;: { \u0026#34;ec2:AssociatePublicIpAddress\u0026#34;: \u0026#34;true\u0026#34; } }, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:ec2:*:*:network-interface/*\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:AssociateAddress\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34; } ] } Should you use Bool or BoolIfExists?\nSCP does not support Principal, NotPrincipal, NotResource\nSCP supports NotAction, however, it can only be used with Effect:Deny\nhttps://stackoverflow.com/questions/70027448/aws-organization-scp-policy-to-deny-any-ec2-instances-in-any-vpcs-with-public-ip\nControl Tower Account Factory Delegated Admin ","permalink":"https://abiydv.github.io/notes/aws/organizations/","summary":"Multi-account architectures New account - does not inherit all required properties from an org. Ensure details are updated before leaving the org.\nPayment method, Tax information, Contact info, Root password etc.\nConsolidated billing cannot use cost-allocation tags in member accounts, only possible in payer account, which is the management account for an organization. aws support can not do this.\ncannot disable cost savings plan, and reservations in member accounts. only possible to limit it to a particular account from the org management/payer account.","title":"AWS Organizations"},{"content":"AWS Outposts are fully managed and configurable compute and storage racks built with AWS-designed hardware that allow customers to run compute and storage on-premises, while seamlessly connecting to AWS’s broad array of services in the cloud. Features Run AWS services on-prem\nLow latency local compute\nData residency requirements for compliance\nLocal data processing for M/L workloads\nTypes Racks Servers ","permalink":"https://abiydv.github.io/notes/aws/outposts/","summary":"AWS Outposts are fully managed and configurable compute and storage racks built with AWS-designed hardware that allow customers to run compute and storage on-premises, while seamlessly connecting to AWS’s broad array of services in the cloud. Features Run AWS services on-prem\nLow latency local compute\nData residency requirements for compliance\nLocal data processing for M/L workloads\nTypes Racks Servers ","title":"AWS Outposts"},{"content":"Keywords - SAAS, Private connectivity\nConnect to services outside your VPC using private addresses as if the external service is within your VPC.\nUses Network load balancer to connect an interface VPC endpoint to a service.\nIn service provider VPC (right) Create a VPC endpoint service (VPC \u0026gt; Endpoint services) which points to either a NLB or Gateway Load Balancer. The load balancer then forwards the traffic to the service compute. Assign a unique name to the service endpoint, consumers will need this name to connect. Share this with the consumer VPC account or Org. It can use multiple subnets across AZs to improve service resilience\nIn service consumer VPC (left) Create a VPC interface endpoint (VPC \u0026gt; Endpoints) and point to the endpoint service configured above.\nA number of AWS Services support using VPC endpoints to keep traffic within the AWS network.\nUseful to establish unidirectional connectivity from a consumer to provider.\nBest suited for client/server kind of architectures. VPC range overlap is not an issue as the consumer ENIs are created such that it does not conflict with provider ENI.\nWhitepaper https://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/aws-privatelink.html #todo\nInteresting read about how PrivateLink came to be? https://aws.amazon.com/blogs/apn/enabling-new-saas-strategies-with-aws-privatelink/\n","permalink":"https://abiydv.github.io/notes/aws/private-link/","summary":"Keywords - SAAS, Private connectivity\nConnect to services outside your VPC using private addresses as if the external service is within your VPC.\nUses Network load balancer to connect an interface VPC endpoint to a service.\nIn service provider VPC (right) Create a VPC endpoint service (VPC \u0026gt; Endpoint services) which points to either a NLB or Gateway Load Balancer. The load balancer then forwards the traffic to the service compute.","title":"AWS Private Link"},{"content":"Scope AZ scope\nFeatures Data warehouse for VERY LARGE amounts of data MPP (Massively parallel processing) sql analytics on exabytes of data for business insights - complex BI workloads provisioned and serverless managed storage - scale storage and compute needs independently Replication to S3 3 9s SLA redshift spectrum to query data lake in S3 data exchange to share/use data in standard formats easily with 3rd parties manual scaling options increase query concurrency = Concurrency Scaling - no downtime increase query throughput, add nodes = Elastic resize - 4 to 8 mins Cloud offering based on ParAccel\u0026rsquo;s solution Redshift Spectrum Query data in S3\nFaster than Athena?\nRefer https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-techniques-for-amazon-redshift/ ","permalink":"https://abiydv.github.io/notes/aws/redshift/","summary":"Scope AZ scope\nFeatures Data warehouse for VERY LARGE amounts of data MPP (Massively parallel processing) sql analytics on exabytes of data for business insights - complex BI workloads provisioned and serverless managed storage - scale storage and compute needs independently Replication to S3 3 9s SLA redshift spectrum to query data lake in S3 data exchange to share/use data in standard formats easily with 3rd parties manual scaling options increase query concurrency = Concurrency Scaling - no downtime increase query throughput, add nodes = Elastic resize - 4 to 8 mins Cloud offering based on ParAccel\u0026rsquo;s solution Redshift Spectrum Query data in S3","title":"AWS Redshift"},{"content":"Scope az-scope - provide rds subnet group, collection of multiple az\nmulti-az\ncross-region read replicas\ncross-region backup snapshots to help disaster recovery\nEngines Questions RDS MySql vs Aurora MySql? Options group? Deployment Options Which deployment option to choose? https://aws.amazon.com/blogs/database/choose-the-right-amazon-rds-deployment-option-single-az-instance-multi-az-instance-or-multi-az-database-cluster/\nSingle AZ Single instance running in a single AZ. Best suited for development workloads.\nA read replica can be configured in the same or a different region. This read replica can be manually promoted if the primary instance fails.\nTypical RPO is ~ 5 mintues, and RTO could be several hours depending on the nature of the disaster.\nMulti AZ Instance This is an active-passive setup.\nDo note that in case of a multi-AZ deployment, both instances are NOT in use at the same time. The secondary instance is merely kept up-to-date by synchronous replication, and is promoted to be the primary instance if the primary fails.\nRPO is ZERO due to the synchronous replication.\nRTO could be longer due to variety of factors like -\ncopying transaction logs\nlazy loading from S3 to EBS\ninstance class\u0026rsquo;s I/O throughput\nrollback uncommitted transaction\nrollforward in-memory committed transactions\nDuring failover, active queries or transactions are cancelled, so it is a recommended to maintain some mechanism to monitor query cancellations.\nReplication is not supported from both sides, that is to say, it cannot really be used for an active/active setup.\nFor multi-AZ deployments, there is possiblity of outage during the time it takes for the instance to fail over (~ 60 seconds).\nOS Maintenance happens on the multi-AZ deployments in steps, secondary first, followed by primary failing over to secondary, then the update continuing on primary.\nDB engine version upgrade however, happens on both both primary and secondary at the same time, causing an outage during this operation. Upgrade on read replica happens independently from the source instance.\nFalling back to primary is difficult.\nBest suited for business critical applications that need high availability and low RPO/RTO.\nDue to the nature of the setup, however, it is not the best scaling option for high read scenarios. Better to use in conjuction with Read replicas or use Multi AZ DB cluster instead.\nAvoid caching DNS data of the DB instances, set a TTL to less than 30 seconds. During a failover, a higher TTL might result in application still trying to use the failed instance.\nMulti AZ DB Cluster Backups Automated and manual snapshots.\nAutomated snapshots can be copied cross-region in the same account but not cross-account.\nManual snapshots can be copied cross-region, cross-account provided they are are not using option groups with persistant or permanent options like TDE, timezone etc.\nSnapshots using default encryption key can also be not shared, they would need to be encryted with custom key which is then shared along with the snapshot.\nChange Data Capture (CDC) Only for MySQL.\nCaptures metadata about all changes in a table. Use for on-going replication during migration.\nRef MS doc\nRDS Proxy Supports Aurora, RDS\nWhen to use RDS Proxy? Too many connections to database Offload the logic to prepare a database connection from your lambda. Make use of the IAM permissions instead to provide access to the database. SaaS or eCommerce applications which prioritize low latency, often need a ready pool of data connections to work with. It makes failover transparent for applications. Since it bypasses DNS (how?, why?), it routes the preserved connections to the new database endpoint thereby reducing failover times for aurora by 66%! RDS Proxy can help avoid out-of memory errors on databases using smaller instance classes like T2, T3.\n","permalink":"https://abiydv.github.io/notes/aws/rds/","summary":"Scope az-scope - provide rds subnet group, collection of multiple az\nmulti-az\ncross-region read replicas\ncross-region backup snapshots to help disaster recovery\nEngines Questions RDS MySql vs Aurora MySql? Options group? Deployment Options Which deployment option to choose? https://aws.amazon.com/blogs/database/choose-the-right-amazon-rds-deployment-option-single-az-instance-multi-az-instance-or-multi-az-database-cluster/\nSingle AZ Single instance running in a single AZ. Best suited for development workloads.\nA read replica can be configured in the same or a different region. This read replica can be manually promoted if the primary instance fails.","title":"AWS Relational Database Service (RDS)"},{"content":"DNS works on port 53, hence the name route 53\nIn a VPC, AWS reserves the network range +2 address for DNS server.\nWhat are R53 resolvers? Inbound endpoint\nFor queries from on-prem systems to resolve AWS hosted domains.\nOutbound endpoint\nFor queries from AWS workloads for on-prem systems.\nYou can specify rule for directing queries from inbound/outbound EP.\nLimit - 10,000 queries per second per IP Address for an EP. For higher performance, use DNS configured on an EC2. With EC2 based DNS, VPC perring is required.\nDNS-VPC primary Amazon DNS server (IP - VPC CIDR +2) Resolver Endpoints (EP) - Inbound, Outbound R53 Resolver EP rules awscloud.private on inbound resolver EP, forwards queries for this domain to the DNS-VPC primary Amazon DNS server. onprem.private on outbound resolver EP, forwards queries for this domain to the on-prem DNS server. R53 Resolver \u0026gt; Rules are shared to all the Org accounts using [[AWS Resource Access Manager]]. Unique private hosted zone in each account, using a sub-domain of aws.private. Each zone should be associated with the shared rule from step 4. On-prem DNS server with conditional forwarders to forward AWS domain queries to the Inbound Resolver EP of AWS DNS-VPC. No VPC peering is necessary in this setup!\nWARNING If using Private Link or EFS, make sure to have rules in a way the these service endpoints are able to resolve locally in the same account, and are not forwarded to the central DNS server. #todo expand on this with example\nTo associate a VPC with a hosted zone created in a different account, use AWS CLI, not possible via console.\nDelegating responsibility for a subdomain You cannot create NS records in a private hosted zone to delegate responsibility for a subdomain. #question\nRouting Policies Simple\nFailover - for active-passive setups to failover to healthy resources\nGeolocation - based on location of users\nGeoproximity - based on location of resources\nLatency - region specific records\nIP-based\nMultivalue answer - upto 8, random healthy records\nWeighted - similar effect is also possible by using weighted rules for load balancer listners - use different target groups for different weights. In fact the ALB approach is preferred if there is a need to maintain session stickiness as well.\nRecords an alias record cannot point to a record in a domain in another hosted zone.\ndns failover active-active https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html#dns-failover-types-active-active\nReferences https://aws.amazon.com/blogs/security/simplify-dns-management-in-a-multiaccount-environment-with-route-53-resolver/\nhttps://aws.amazon.com/blogs/security/how-to-centralize-dns-management-in-a-multi-account-environment/\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-with-amazon-route-53-and-aws-transit-gateway/\nhttps://aws.amazon.com/blogs/architecture/using-route-53-private-hosted-zones-for-cross-account-multi-region-architectures/\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-private-hosted-zones.html\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-considerations.html\n","permalink":"https://abiydv.github.io/notes/aws/route53/","summary":"DNS works on port 53, hence the name route 53\nIn a VPC, AWS reserves the network range +2 address for DNS server.\nWhat are R53 resolvers? Inbound endpoint\nFor queries from on-prem systems to resolve AWS hosted domains.\nOutbound endpoint\nFor queries from AWS workloads for on-prem systems.\nYou can specify rule for directing queries from inbound/outbound EP.\nLimit - 10,000 queries per second per IP Address for an EP.","title":"AWS Route53"},{"content":"Security Hub Add details\nAWS Inspector automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2)\nAWS Lambda functions, and container images in Amazon ECR and within continuous integration and continuous delivery (CI/CD) tools, in near-real time for software vulnerabilities and unintended network exposure.\nCommon Vulnerability Scoring System (CVSS) v2 and v3 scores from both National Vulnerability Database (NVD) and vendors\nmanage software bill of materials (SBOM) exports for all monitored resources.\nintegrates with Security Hub, AWS EventBridge to automate workflows and ticket routing\nintegrates with AWS Organizations for central management\nintegrates with SSM, and uses the same agent for it\u0026rsquo;s findings\nMacie discovers sensitive data using machine learning and pattern matching, provides visibility into data security risks, and enables automated protection\nenabled on a per-region basis\nmanaged data identifier = built-in criteria and techniques that are designed to detect a specific type of sensitive data\ncustom data identifier = define custom data patterns using regex for your unique usecase, for example, a particular format of employee email addresses or ids. You can also specify character sequences, such as words and phrases, and a proximity rule to refine the results.\nintegrates with Security Hub, AWS EventBridge\nintegrates with AWS Organizations to discover sensitive data across all accounts\nKMS Highly available key generation, storage, management, and auditing solution to encrypt or digitally sign data within applications or control the encryption of data across AWS services.\nEnable Private DNS Name makes the standard AWS KMS DNS hostname (https://kms.region.amazonaws.com) resolve to VPC endpoint.\nSupports Symmetric (256-bit key that is used for encryption and decryption) and Asymmetric CMKs (an RSA key pair that is used for encryption and decryption or signing and verification but not both, or an elliptic curve (ECC) key pair that is used for signing and verification).\nAsymmetric customer managed CMKs - the key material can only be generated within AWS KMS HSMs and no option for automatic key rotation.\nSymmetric CMKs and the private keys of Asymmetric CMKs never leave AWS KMS unencrypted and AWS KMS does not store, manage, or track data keys.\nCryptographic Operations\nDecrypt, Encrypt, GenerateDataKey, GenerateDataKeyPair, GenerateDataKeyPairWithoutPlaintext, GenerateDataKeyWithoutPlaintext, GenerateRandom, ReEncrypt, Sign, Verify GenerateRandom does not use any CMK. Customer Master Key (CMK)\nLogical representation of a master key. Includes metadata, such as the key ID, creation date, description, and key state. Contains the key material used to encrypt and decrypt data. Cannot be changed. Alias - friendly name. Can be used in KMS operations (including cryptographic). Use single alias to refer to different CMKs in different Region. Can add, change, and delete an alias without affecting the associated CMK. AWS Managed CMK\nPartial control. Track in account, choose which ones to use, track usage via Cloudtrail. Mandatory rotation in 1095 days/3 yrs. No monthly fee. Do not count against resource quotas on the number of CMKs in each Region. DescribeKey API response, the KeyManager value is AWS. Cannot edit the key policy. Customer Managed CMK\nFull control. Define access control and usage policy for each key, grant permissions to other accounts and services. Key material can be generated in KMS, CloudHSM or be from outside AWS. Optional rotation 365 days/1 yr. Incur a monthly fee and a fee for use in excess of the free tier. Counted against quota for number of CMKs in each Region. DescribeKey API response, the KeyManager value is Customer. Use key policy to add, remove, or change permissions at any time. AWS owned CMK\nCMKs that an AWS service owns and manages for use in multiple AWS accounts. Not in your AWS account but AWS service can use it to protect the resources in account. Key Rotation\nManual only - asymmetric, CMK with imported key material, CMKs in custom key stores (ex- using CloudHSM). Automatic - yearly for customer managed CMK (disabled by default). Key Deletion\nSchedule key deletion - 7 days (min) to 30 days (max, default). Key scheduled for deletion cannot be used for cryptographic operations. CMK with imported key material - can delete the key material immediately. CMK can be re-enabled by reimporting the key material. Deleted CMK may not impact mounted encrypted EBS immediately as EC2 uses plain data-key in memory to manage I/O. The EBS cannot be reused as next mount option will fail. Grants - very specific, easy to create/revoke. Useful to provide temporary or granular permissions on CMKs.\n$ aws kms create-grant \\ --key-id 1234abcd-12ab-34cd-56ef-1234567890ab \\ --grantee-principal arn:aws:iam::111122223333:user/exampleUser \\ --retiring-principal arn:aws:iam::111122223333:role/adminRole \\ --operations GenerateDataKey \\ --constraints EncryptionContextEquals={Purpose=Test} Key Policies - This policy delegates all access to IAM, root does not necessarily represent the root user here. Refer\n{ \u0026#34;Sid\u0026#34;: \u0026#34;Enable IAM User Permissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: {\u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::111122223333:root\u0026#34;}, \u0026#34;Action\u0026#34;: \u0026#34;kms:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } Envelope Encryption\nEncrypting plaintext data with a data key, and then encrypting the data key under another key. Protects keys - safe to store the encrypted data key alongside the encrypted data. Encrypting the same data under multiple master keys - instead of re-encrypting raw data multiple times with different keys, re-encrypt only the data keys that protect the raw data. Combining the strengths of multiple algorithms. Encryption Context\n(Optional) Key–value pairs containing additional contextual information (AAD) about the data. Same encryption context is required to decrypt (or decrypt and re-encrypt) the data as provided while encypting it. Not secret, appears in plaintext in CloudTrail. Do not use for sensitive data. Use to limit access to CMKs. Use as a constraint in grants and as a condition in policy statements. Example - encrypted EBS snapshots use volume-id as encryption context. CloudHSM Securely generate, store, and manage cryptographic keys used for data encryption. Keys are accessible only by you (owner). Each HSM appears as a network resource in VPC. Can only be provisioned within a VPC. No free tier. No reserved instance pricing. Physical and digital tamper detection. IAM Always pick temporary credential option like IAM Role over long term access like Access Key/Secret Key. Least Permission model - never assign more than the bare minimum required permissions. MFA on root - must. AWS Org SCP policy can deny actions to member account root users. Policy evaluation flow = deny all \u0026gt; org scp \u0026gt; resource based policy \u0026gt; iam permission boundary \u0026gt; session policy \u0026gt; identity based policy. Confused Deputy - always use an externalId when allowing third party to assume IAM role in your account. S3 Bucket ACL\nGrant basic read/write permissions to other AWS accounts. Limited permissions available. Cannot grant permissions to users in same account. Cannot grant conditional permissions. Cannot explicitly deny permissions. Object ACL\nBucket Policy\nIAM Policy\nSSE-S3\nS3 manages the data and master encryption keys. SSE-C\nCustomer manages the encryption key. SSE-KMS\nAWS manages the data key but customer manages the master key (CMK) in AWS KMS.\nThe AWS KMS CMK must be in the same Region as the bucket.\nAll GET and PUT requests for an object protected by AWS KMS fail if they are not made via SSL or TLS, or without SigV4.\nBucket Keys\nUsed to create unique data keys for objects. Used for a time-limited period within Amazon S3, reducing the need to make requests to KMS to complete encryption operations. Reduces the cost of KMS operations. S3 uses the bucket ARN as the encryption context instead of the object ARN if using bucket keys. Replication\nIf source object uses S3 Bucket Keys \u0026amp; destination bucket uses default encryption, replica object maintains its S3 Bucket Key encryption settings in the destination bucket. If source object is not encrypted \u0026amp; destination bucket uses S3 Bucket Key with SSE-KMS, replica object is encrypted with an S3 Bucket Key using SSE-KMS. This results in the ETag of the source object being different from the ETag of the replica object. ACM Public/private SSL/TLS certs for AWS services and other internal connected resources. Auto renewals for ACM issued certs. Not for imported 3rd party certs. Cannot issue or renew certificates using DNS validation if CNAME record is removed. Contact support to revoke a public issued cert via ACM. ACM issued certs cannot be copied between regions. ACM does not have an SLA. Public ACM certificates can be used only with specific AWS services. To use an ACM certificate with Amazon CloudFront, request the certificate in the US East (N. Virginia). Private Certs Secure communication between connected resources on private networks. Private CA renews the cert, but user needs to retrieve and depoy to the private resources. Steps - Install OpenSSL \u0026amp; Generate private key. Generate CSR with the new key. Call ACM issue-certificate with CSR. Call ACM get-certificate to get the issued private cert. Required IAM Permissions - acm-pca:GetCertificate and acm:IssueCertificate. AWS Web Application Firewall (WAF) Configure rules to allow, block, or monitor (count) web requests based on defined conditions - IP addresses, HTTP headers, HTTP body, URI strings, SQL injection and cross-site scripting. Runs on edge if applied to CloudFront. Runs in region if applied to API GW, ALB, AppSync etc. Rate-based rule - allow spefic number of hits for particular condition (IP, Path, Headers or a combination). Mitigate DDoS. Configure a “count” action for rules, which counts the number of web requests that meet rule conditions. Managed rules - OWASP, bots, or CVEs. Amazon Macie Detects sensitive data like including personally identifiable information (PII) such as names, addresses, and credit card numbers. Supports custom-defined data types using regular expressions. Constant visibility of the data security and data privacy of data stored in Amazon S3. Regional service - needs to be enabled per region. Can also be enabled from AWS Org for multiple accounts. AWS Trusted Advisor Automated weekly checks for business/enterprise support accounts and email notifications. Basic support - S3 Bucket Permissions, Security Groups - Specific Ports Unrestricted, IAM Use, MFA on Root Account, EBS Public Snapshots, RDS Public Snapshots. Other supports - full access. Does not monitor continuously - periodic refreshes. AWS CloudTrail GUI - Last 90 days\u0026rsquo; events (management events) for current region.\nFor \u0026gt;90 days visibility configure a CloudTrail trail.\nA CloudTrail trail can deliver CloudTrail events to Amazon S3, Amazon CloudWatch Logs, and Amazon CloudWatch Events.\nData Events, Management Events, Read-Only Events.\nTrail for all regions will capture any newly created regions as well.\n5 Trails/region.\nCaptures auto/manual trusted advisor check refreshes.\nUnsuccessful sign-in events by the root user are not logged by CloudTrail.\nDoes not log HSM device or access logs. These are available in CloudWatch Logs.\nAWS does not log the entered IAM user name text when the sign-in failure is caused by an incorrect user name. The user name text is masked by the value HIDDEN_DUE_TO_SECURITY_REASONS.\nHas a history of all AWS WAF API calls made on account.\nDeliver logs to S3\nEncrypted with SSE-S3 by default. Can be configured to use SSE-KMS too. CloudTrail must have the required permissions on bucket. Attach a policy for using existing bucket. It cannot be a Requester Pays bucket. CloudTrail automatically attaches the required permissions to a new bucket when creating or updating a trail from console. Older bucket policy using CloudTrail account IDs for each region (New use service principle cloudtrail.amazonaws.com) can cause log delivery issues. A bucket policy with an incorrect prefix can prevent cloudtrail from delivering logs. AWS Config Fully managed service that provides AWS resource inventory, configuration history, and configuration change notifications to enable security and governance.\nConfig rules\nDesired config for a resource. Evaluated against config changes on relevant resources, as recorded by AWS Config. AWS managed or Customer managed. 150 rule limit - can submit an extension request. Change-triggered rule (based on tag, resource type or resource id) or a periodic rule (1,3,6,12,24 hrs). Try to update the rule (\u0026amp; lambda) instead of deleting it and creating a new rule. New rule result in charges. Conformance pack\nCollection of Config rules and remediation actions. A framework to build and deploy compliance packages. Deploy at scale across accounts. Resource configuration history\nRelate config changes with AWS CloudTrail events that made the change. Useful to get info like “Who made the change?”, “From what IP address?” AWS Security Hub vs AWS Config conformance packs - If compliance is already present in AWS Security Hub, then it is the easiest way to operationalize it. Go with Config conformance pack if need is to have something custom.\nAggregate AWS Config data from multiple accounts and regions into a single account and a single region. However it cannot be used for provisioning rules across multiple accounts. It is purely a reporting capability.\nAlso record configuration changes to Operating System updates, network configuration, installed applications, etc within EC2 instances, virtual machines (VMs), or on-prem servers.\nAWS Config sends notifications only when the compliance status changes. If a resource was previously non-compliant and is still non-compliant, Config will not send a new notification. If the compliance status changes to “compliant”, you will receive a notification for the change in status.\nDoes not evaluate rules prior to provisioning a resource or prior to making configuration changes on the resource.\nDoes not guarantee that resources will be compliant or prevent users from taking non-compliant actions. Can make them compliant after the change though via remediation actions.\nCan connect to ServiceNow and Jira Service Desk instances to AWS Config using AWS Service Management Connector.\nAmazon Inspector Automated security assessment service that can test the network accessibility of EC2 instances and the security state of applications running on the EC2. Agent needed on EC2 for Host assessment. Rules packages include Common Vulnerabilities and Exposures (CVE), Center for Internet Security (CIS) Operating System configuration benchmarks, and security best practices. No agent necessary for the network reachability rules package that checks for network accessibility of EC2. Only the pre-defined rules are currently allowed for assessment runs. No custom rules possible. Generates a findings report (exec summary) or a full report (complete details) after assessment run. Periodic - Set up custom schedules with either a fixed recurring rate or a more detailed Cron expression through CloudWatch Events. Event based - Use Amazon CloudWatch Events to create event patterns which monitor other AWS services for actions to trigger an assessment. Cloudwatch EC2 instance agent to send application logs to cloudwatch. Errors Log rotation method not supported. No internet connectivity to cloudwatch API. Fingerprinting - log-identifying hash. Permission - no access via role, no role etc. Timestamp - events do not begin with timestamp, log path incorrect, cannot push from multiple logs to a single log stream. AWS GuardDuty Analyzes continuous streams of meta-data generated and network activity in AWS CloudTrail, Amazon VPC Flow Logs, and DNS Logs. Threat intelligence such as known malicious IP addresses, anomaly detection etc. Identify threats such as attacker reconnaissance, instance compromise, account compromise, and bucket compromise. GuardDuty security findings remain in the same region where the underlying data was generated. Aggregation - self managed via CloudWatch Events, S3 etc. Multiple account feature - All security findings are aggregated to master account for review and remediation. CloudWatch Events are also aggregated to master account in this case. GuardDuty cannot access and process data from a 3rd party DNS resolver (non Route53). VPC Security Groups\nStateful - need only inbound rule, outbound is allowed if inbound is allowed. Use if need to allow an IP. Network Access Control Lists (NACL)\nStateless - need both inbound/outbound rule to allow traffic Use if need to block an IP Flow Logs\nAbstracted data - do not allow for full package reconstruction Capture source/destination IP, ports, protocol, start/end time, no of packets, no of bytes, action etc. EC2 Disable source/destination check attribute for instances running services such as network address translation, routing, or a firewall. Enabled by default. IPtables can be used to restrict access to ec2 metadata service. Prefer access rules over deny rules. Launch the instance with --metadata-options set to HttpEndpoint=disabled to disable metadata service. Packet capture - Marketplace AMI, route all traffic though it. AWS does not support virtual instances running in promiscuous mode. Select ciphers to use for SSL connections by creating a custom ELB security policy (cannot edit default). AD Authentication Reference DynamoDB Encryption Server-side encryption\nBy default - encrypts all tables, cannot be disabled. Uses AWS owned CMK. Encrypts all table and related data incl primary keys, local/secondary indexes, backups, global tables etc. DynamoDB Encryption Client\nDoes not encrypt entire items. primary keys, attributes are still in plain text. Customer managed CMK (incl custom crypographic data from cloudHSM). Use with Java/Python. Others AWS Artifact - audit artifact retrieval portal that provides AWS’ compliance documentation and AWS agreements. REFERENCES AWS Security Best Practices AD Authentication FLow Security related trainings ","permalink":"https://abiydv.github.io/notes/aws/security-services/","summary":"Security Hub Add details\nAWS Inspector automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2)\nAWS Lambda functions, and container images in Amazon ECR and within continuous integration and continuous delivery (CI/CD) tools, in near-real time for software vulnerabilities and unintended network exposure.\nCommon Vulnerability Scoring System (CVSS) v2 and v3 scores from both National Vulnerability Database (NVD) and vendors\nmanage software bill of materials (SBOM) exports for all monitored resources.","title":"AWS Security Services"},{"content":"Features Send via SMTP - does not support temporary credentials (sts), must create and use an IAM user credential.\nSend via API - use normally with temporary credentials (sts)\nWhen using SMTP - use STARTTLS (ports 25, 587, 2587) or TLS wrapper (ports 465, 2465)\nSave and use Handlebar templates for sending emails. If using AWS SDK, use the SendTemplatedEmail or SendBulkTemplatedEmail API actions. Data used to render the template is provided via the TemplateData field.\nSES accepts the message if the payload for aboce actions is valid. However the rendering can still fail due to errors in the template. In this case the email is not sent, and neither is a message returned to the requestor about such an error.\nTo avoid such silent failures, setup notifications for rendering failures.\n","permalink":"https://abiydv.github.io/notes/aws/ses/","summary":"Features Send via SMTP - does not support temporary credentials (sts), must create and use an IAM user credential.\nSend via API - use normally with temporary credentials (sts)\nWhen using SMTP - use STARTTLS (ports 25, 587, 2587) or TLS wrapper (ports 465, 2465)\nSave and use Handlebar templates for sending emails. If using AWS SDK, use the SendTemplatedEmail or SendBulkTemplatedEmail API actions. Data used to render the template is provided via the TemplateData field.","title":"AWS Simple Email Service (SES)"},{"content":"Wavelength is designed to deliver ultra-low latency applications to 5G devices by extending AWS infrastructure, services, APIs, and tools to 5G networks.\nembeds storage and compute inside telco providers 5G networks to help developers build new applications for 5G end users that require single-digit millisecond latency, like IoT devices, game streaming, autonomous vehicles, and live media production.\n","permalink":"https://abiydv.github.io/notes/aws/wavelength/","summary":"Wavelength is designed to deliver ultra-low latency applications to 5G devices by extending AWS infrastructure, services, APIs, and tools to 5G networks.\nembeds storage and compute inside telco providers 5G networks to help developers build new applications for 5G end users that require single-digit millisecond latency, like IoT devices, game streaming, autonomous vehicles, and live media production.","title":"AWS Wavelength"},{"content":"Introduction Pillars 6 pillars of AWS Well Architected framework, as of 05-Dec-2023\nS Security C Cost Optimization O Operational Excellence R Reliability P Performance Efficiency S Sustainability Patterns Static Stability https://aws.amazon.com/builders-library/static-stability-using-availability-zones/\nBe ready for impairments before they happen\nOver-provisioning is not necessarily a bad thing. It can provide some static stability in the sense that an AZ failure will not cause a full outage, the existing capacity in other AZ can continue to handle the load while the fault recovers or additional capacity is provisioned reactively to the AZ failure event.\nActive Active Useful for stateless workloads\nActive Passive Useful for stateful workloads\nAZ Affinity Keep resources within the same AZ?\nCustom Lens In the well architecred framework, it is possible to create a custom lens for a specific need for compliance or just ensuring an org-wide standard.\nA custom lens is usually a json file, and can be run from the well architected tool console. Ref\n","permalink":"https://abiydv.github.io/notes/aws/aws-well-architected/","summary":"Introduction Pillars 6 pillars of AWS Well Architected framework, as of 05-Dec-2023\nS Security C Cost Optimization O Operational Excellence R Reliability P Performance Efficiency S Sustainability Patterns Static Stability https://aws.amazon.com/builders-library/static-stability-using-availability-zones/\nBe ready for impairments before they happen\nOver-provisioning is not necessarily a bad thing. It can provide some static stability in the sense that an AZ failure will not cause a full outage, the existing capacity in other AZ can continue to handle the load while the fault recovers or additional capacity is provisioned reactively to the AZ failure event.","title":"AWS Well Architected"},{"content":"Deployments Methodologies Blue/Green Canary Like a canary in a coal mine, this is used to assess if there is a danger deploying the latest changes. Initial deployment to a limited set of resources, followed by tests to ensure there are no issues. If successful, deployment can continue to all other resources.\n","permalink":"https://abiydv.github.io/notes/devops/","summary":"Deployments Methodologies Blue/Green Canary Like a canary in a coal mine, this is used to assess if there is a danger deploying the latest changes. Initial deployment to a limited set of resources, followed by tests to ensure there are no issues. If successful, deployment can continue to all other resources.","title":"DevOps"},{"content":"Strategies 6 R https://docs.aws.amazon.com/prescriptive-guidance/latest/strategy-migration/welcome.html\nRehosting Lift and shift\nHomogeneous (made of same things before/after) No changes to the applications, simply move the location from on-prem to cloud.\nTools = VM Import/Export, [[#Application Migration Service (MGN)]]\nReplatforming Lift, tinker and shift\nHomogeneous (made of same things before/after) No big changes to the applications, simply move the location from on-prem to cloud, while choosing some low hanging fruits. Like moving to a database-as-a-service offering.\nExample = Migrate on-prem Oracle DB to RDS for Oracle\nTools = VM Import/Export, [[#Application Migration Service (MGN)]]\nRepurchasing Move to a different product Heterogeneous (quite different before/after) Usually a move to a SaaS platform\nExample = Tools =\nRefactoring Heterogeneous (quite different before/after) Strong business need to add features, scale, or performance\nExample = Move from monolith to micro-services, Migrate on-prem Oracle DB to Aurora, RDS (MySql, PostgreSql), MariaDB etc.\nTools = [[#Schema Conversion Tool (SCT)]], [[#App2container]]\nRetire No use, retire\nRetain Keep as-is, no change\nsource\nMigration Evaluator Build a business case\nUpload data from a [[CMDB]] export or gather data via an agentless collector agent. Agentless collector requires a Windows VM deployed on the source system like VMWare vSphere Provides projected costs to rehost on AWS based on resource configs and usage patterns Does not support Citirix XenServer Application Discovery Service (ADS) Discover details\nUse this to discover your application\u0026rsquo;s needs prior to any planned migration. It can collect metrics (cpu/memory/disk/network), configurations (number of disks, os version etc) either via an agent (can be installed on any windows/linux host), or agentlessly by launching a collector in vSphere (reads the metrics from outside the vms).\nAgentless collection does not collect data about processes! Available as an OVA file to be deployed in the VMWare environment.\nPrevious verions - AWS Agentless Discovery Connector (Discovery Connector) is a VMware appliance. Retired, use ADS connector instead.\nMigration Hub Plan and manage\nIntegrates well with ADS Presents a simplified view about all the applications on a single dashboard. You can group the servers together for migration, export the utilization stats to do cost modelling on AWS etc.\nApplication Migration Service (MGN) Rehost, or lift and shift\nContinuous block-level replication of on-premises source servers to a staging area in AWS Account. 2160 hrs free use period ~90d Auto converts source servers from physical, virtual, or cloud infrastructure to run natively on AWS\nPrevious version = CloudEndure Migration\nApp2container Replatform apps to run on containers\nAWS App2Container (A2C) is a tool for replatforming .NET and Java web-based applications directly into containers. A2C packages the application artifact and identified dependencies into container images, configures the network ports, and generates the needed definitions. A2C provisions the cloud infrastructure and CI/CD pipelines required to deploy the containerized application into production.\nDatabase Migration Service (DMS) Replatform and sync data\nMigrate to RDS or EC2 hosted databases Switch DB platforms - Oracle to Postgres\nPerformance best practices Turn off backups and transaction logging\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/strategy-database-migration/cut-over.html\nCutover Flash Cut Minimal downtime SCT DMS continuous data replication (CDC)\nOffline Major downtime SCT DMS full-load\nActive/Active No downtime Bi-directional replication\nIncremental No downtime Example = migrate one service at a time if possible, while re-factoing the application\nSchema Conversion Tool (SCT) Migrate database schema from source to destination SCT replication agent supports asynchronous data replication using [[#Snowball Edge Storage Optimized]] device. Helpful for very large scale migrations.\nDataSync Move data between storage.\nEndpoints - [[efs]], [[fsx]], [[s3]]\n#question When to pick [[#DataSync]] over [[#Storage Gateway]] ?\nSetup a task to periodically transfer data between storage locations. The scheduled task runs with a minimal interval of 1 hr (even with a custom cron). So, if the need is to replicate/transfer data faster than 1 hr, DataSync won\u0026rsquo;t work. Ensures file integrity Handles synchronization Install as VM on-prem\nSnow Family Manage using OpsHub application, which can be installed on Mac/Linux/Windows OS.\nNFS Encryption Edge compute Offline data migration Online data migration using [[#DataSync]] End to end tracking Secure erasure\nSnowcone Small form factor for maximum portability, smallest device in the family Best suited to use in harsh environments to run workloads locally on EC2 8 TB HDD or 14 TB SSD storage, use as NFS v4 mounts. Transfer data to AWS offline by shipping the device back or online by using inbuild [[#DataSync]]\nSnowball Edge Compute Optimized Best suited for edge compute needs before transferring data to AWS ~100 vCPU, 28 TB [[NVMe SSD]] storage 100 Gbps network connection IoT data analytics (think IoT Greengrass) Local compute with Lambda or EC2 instances (creatively called, SBE1) File transfer using NFS GUI GPU workloads for optimizing M/L models locally with limited connectivity (SBE-G instances)\nSnowball Edge Storage Optimized Transfer TB to PB of data to AWS IoT data aggregation and analysis 80 TB or 210 TB storage 1, 10 or 100 Gbps network connection Max duration - 360 days\nSnowmobile A 45 foot container to transfer up to 100 PB of data. It starts to make sense using this over [[#Snowball Edge Storage Optimized]] devices if most data is at a single location, and exceeds 10 PB.\nTransfer Family Transfer data directly to [[#S3]] or [[#EFS]] using protocols such as [[SFTP]], [[FTPS]], [[FTP]], [[AS2]]\nFor FTP, you cannot create a public endpoint, it has to be a VPC hosted endpoint. As a workaround for using FTP over internet, you can route traffic through a Network load balancer, and run the FTP server in PASV mode.\nFor FTPS, only explicit mode is supported.\nFor FTPS, and FTP only Passive mode is supported.\nCan the endpoints be assigned fixed public IP addresses? Yes, create VPC hosted \u0026ldquo;internet-facing\u0026rdquo; endpoint, and assign an Elastic IP to the endpoint directly. This is not possible for Public endpoints.\nCan you restrict who is able to connect to the endpoint? Yes, for VPC hosted endpoints, this can be done via security groups. For public endpoints using [[API Gateway]] for identity management, WAS can also be used.\nhttps://aws.amazon.com/blogs/storage/simplify-your-aws-sftp-structure-with-chroot-and-logical-directories/\n","permalink":"https://abiydv.github.io/notes/aws/migration/","summary":"Strategies 6 R https://docs.aws.amazon.com/prescriptive-guidance/latest/strategy-migration/welcome.html\nRehosting Lift and shift\nHomogeneous (made of same things before/after) No changes to the applications, simply move the location from on-prem to cloud.\nTools = VM Import/Export, [[#Application Migration Service (MGN)]]\nReplatforming Lift, tinker and shift\nHomogeneous (made of same things before/after) No big changes to the applications, simply move the location from on-prem to cloud, while choosing some low hanging fruits. Like moving to a database-as-a-service offering.","title":"Migration to AWS"},{"content":"Questions What is the scope? AZ, Regional, Global\nWhen or why will you choose both CloudFront AND Load balancer?\nWhat is an AZ? AZ or availability zone is an independent data center within a specific geographic region. An AWS Region is a collection of multiple availability zones. Regarded as a fault boundary. Data transfer across AZ is not free. Cross AZ traffic should be controlled for cost optimization and performance efficiency. Why use multiple AZ? Improves reliability since failures in one AZ do not cause full outage of the workload. Improves performance by reducing latency. Helpful in disaster recovery? Is Route53 AZ aware? In the example with app servers trying to reach the AD domain controller, how does the name resolution help keep the traffic in the same AZ?\nCreate AZ specific DNS entries using network interfaces in the same AZ. For ex - NLB provides a static IP for network interface in each AZ. Use this to create endpoints such as euw-az1.myservice.com, instead of letting clients use myservice.com.\nChallenges Random AZ name to AZ id mapping AWS maps AZ names to different AZ ids in each account, to distribute load more evenly across all it\u0026rsquo;s data centers. AZ name format is eu-west-1a, where as AZ id format is euw1-az1. AZ name to AZ id mapping can differ between accounts. AZ id is the same everywhere. Example from an AWS Account (eu-west-1 region) -\u0026gt; EC2 Dashboard AZ name AZ ID eu-west-1a euw1-az3 eu-west-1b euw1-az1 eu-west-1c euw1-az2 No visibility to others\u0026rsquo; AZ choices Multi-account setups usually have multiple VPC peering and/or transit gateway attachments, often with differing AZ choices.\nOne team may not be aware of the other teams\u0026rsquo; AZ choices. This can cause traffic to cross the AZ boundary when workloads deployed in different accounts/VPCs need to interact.\nGuidelines Due to the above mentioned challenges of multi-AZ architectures, it is important to follow some guidelines when designing.\nManage AZ assignments centrally Just like managing VPC CIDR ranges centrally with IPAM, manage the preferred AZ centrally as well.\nExample, when using VPC with CIDR range of /16, it can be divided further into /20 blocks. The first two /20 blocks can then be used in the first two AZs in all accounts. This greatly simplifies the routing table of the Transit Gateway in the shared network account.\nVPC range AZ 1 range AZ 2 range 10.1.0.0/16 10.1.0.0/20 10.1.16.0/20 AZ aware shared networking AWS Transit Gateway is AZ aware (except in appliance mode) and keeps the traffic in the same AZ as source, until it reaches the destination or is routed to a different AZ. Refer examples.\nWhen attaching a TGW to a VPC, use the same AZ that subnets in the VPC use.\nDisable cross-zone load balancing Load Balancers route traffic across AZ, by default (cross-zone load balancing = enabled). Consider disabling cross-zone load balancing when using cross-account load balancer. Be aware of the following trade-offs\nCapacity planning in each AZ to ensure adequate redundancy No support for stickiness AWS Network Load Balancer does not route traffic across AZ, by default (cross-zone load balancing = disabled).\nUse AZ local resources Use local resources within the AZ, if possible.\nExample, if there are AD controllers an app needs to reach, consider configuring the app instances in each AZ with the address of the AD controller of that specific AZ. This would ensure there is no risk of traffic crossing the AZ boundary.\nMonitor cost and performance metrics Monitoring relevant metrics is essential to continually improve the workload design and performance. Some things to keep an eye on -\nAnalyse network traffic patterns using vpc flow logs to identify cross-AZ traffic. Use network manager to monitor inter/intra-AZ latency specific information. Analyse data transfer charges using cost and usage reports to identify cross-AZ traffic, and optimise workloads. Examples Using transit gateway to inspect all ingress/egress traffic A workload in VPC A (application account) connects to the AD controller deployed in VPC B (shared services account) through the transit gateway (shared networking account)\n[East-West-VPC-to-VPC-traffic-inspection-scenario.png]\nCentralized egress with ingress/egress inspection All egress traffic is routed through the transit gateway, and AZ specific NAT gateway. Separate NAT gateways should be provisioned in each AZ to avoid cross traffic.\nNote the route table in the central egress TGW subnet. While sending traffic to internet, care should be taken to prevent [[Work/Network#Private IPs|private IPs]] from hitting the public internet. So, be sure to add routes like 10.0.0.0/8, 100.64.0.0/18, 192.168.0.0, 172.168.0.0\n[Routing-traffic-through-NAT-Gateway.png]\nCentralized ingress with ingress/egress inspection Incoming internet is accepted by the internet gateway, and passed to the ALB, which in turn routes it to the destination app workload via the transit gateway, and inspection appliance.\nThe key steps to note here are 2, and 10. Load balancer (cross-zone disabled) picks a network interface in the same AZ (AZ B) as the destination. If cross-zone is enabled, it can pick any AZ, so a scenario like 10a becomes possible/probable.\n[AZ-aware-load-balancing-scenario.png]\nStorage service spanning multiple AZ Figure shows a storage service which has 3 different components. A request router, worker nodes (lead, tail, and middle), and a load balancer (cross-zone load balancing = enabled).\nThis architecture results in data transfer across AZ about 8 times. This can add performance overhead as cross AZ latency is in single digit ms compared to communication within an AZ, where latency is sub ms (\u0026lt; 1 ms).\nImprovements Switch from ALB to NLB (cross-zone disabled, static ip in each AZ) Create AZ specific DNS endpoints using the NLB network interface in AZ, like euw1-az1.foo.com Clients use the AZ specific DNS name above. This ties the clients to a specific AZ and reduce the benefit of using multiple AZ. Implement a client library to include logic for retries with exponential backoff, circuit breakers etc for cases when an AZ fails. The clients should be able to switch to a healthy AZ. Implement service discovery to reduce the need of LB, clients can directly connect to their AZ specific request routers (see figure above). Full post\nReferences https://community.aws/posts/improving-availability-and-performance-with-multi-az-architecture\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/centralized-inspection-architecture-with-aws-gateway-load-balancer-and-aws-transit-gateway/\nhttps://aws.amazon.com/builders-library/static-stability-using-availability-zones/\n","permalink":"https://abiydv.github.io/notes/aws/multi-az-rchitectures/","summary":"Questions What is the scope? AZ, Regional, Global\nWhen or why will you choose both CloudFront AND Load balancer?\nWhat is an AZ? AZ or availability zone is an independent data center within a specific geographic region. An AWS Region is a collection of multiple availability zones. Regarded as a fault boundary. Data transfer across AZ is not free. Cross AZ traffic should be controlled for cost optimization and performance efficiency.","title":"Multi-AZ Architectures"},{"content":" info FYI AWS use “CloudWatch” in docs, “cloudwatch“ in cli/sdk, “monitoring” as the API endpoint, and “cloud-watch” in other service API calls like when setting flow logs for VPC, log_destination_type = \u0026ldquo;cloud-watch-logs\u0026rdquo;\nCustom log groups Now it\u0026rsquo;s possible to use a custom log group for a lambda function. Use this to send logs from multiple lambda functions to the same log group - each function gets it\u0026rsquo;s own log stream! Previously, every lambda function created it\u0026rsquo;s own cloudwatch log group (with a random suffix). This log group wasn\u0026rsquo;t removed up if the function was deleted. A log expiry duration like 7 days would only delete the log streams inside the log group. This resulted in proliferation of dead log groups which had to be deleted/cleaned up manually. The problem is severe in accounts where there are lots for short lived lambda deployments, like lower development environments.\nRef\ncloudwatch log query fields @timestamp, eventSource, eventName, @message | filter ispresent(errorCode) and readOnly = 0 and userIdentity.accountId = \u0026#39;000123456789\u0026#39; fields @timestamp, recipientAccountId, eventName, eventSource, userIdentity.arn, @message | filter eventName in [\u0026#34;RunInstances\u0026#34;,\u0026#34;AttachInternetGateway\u0026#34;,\u0026#34;CreateEgressOnlyInternetGateway\u0026#34;,\u0026#34;AssociateAddress\u0026#34;,\u0026#34;CreateVpcPeeringConnection\u0026#34;,\u0026#34;AcceptVpcPeeringConnection\u0026#34;] and not ispresent(errorCode) fields @timestamp, recipientAccountId, eventName, eventSource, userIdentity.arn, @message | filter eventName in [\u0026#34;RunInstances\u0026#34;] and not ispresent(errorCode) | filter requestParameters.networkInterfaceSet.items.0.associatePublicIpAddress=0 cloudwatch Composite alarms can send Amazon SNS notifications when they change state, and can create Systems Manager OpsItems or incidents when they go into ALARM state, but can\u0026rsquo;t perform EC2 actions or Auto Scaling actions.\n","permalink":"https://abiydv.github.io/notes/aws/cloudwatch/","summary":"info FYI AWS use “CloudWatch” in docs, “cloudwatch“ in cli/sdk, “monitoring” as the API endpoint, and “cloud-watch” in other service API calls like when setting flow logs for VPC, log_destination_type = \u0026ldquo;cloud-watch-logs\u0026rdquo;\nCustom log groups Now it\u0026rsquo;s possible to use a custom log group for a lambda function. Use this to send logs from multiple lambda functions to the same log group - each function gets it\u0026rsquo;s own log stream!","title":"AWS CloudWatch"},{"content":"While VPN connections are OK, for some critical workloads, VPN over internet may not be stable or available enough.\nIf a customer doesn\u0026rsquo;t want to connect to AWS over the internet, they can alternatively use Direct Connect. This ensures the traffic to/from AWS flows through the customer\u0026rsquo;s network directely on to AWS backbone network without going over the internet.\nCost effective, secure, performant private network connectivity between on-prem private network and AWS.\nLocation These are the AWS provided physical locations customers connect their on-prem data centers to, to establish direct connectivity with AWS back bone network.\nAll such locations have a default region associated with them depending on their location. However, the Direct Connect connection established using a particular location can be used for any region (except some exceptions for China).\nSome locations support MACSec for some port speeds (mostly for 10 and 100 Gbps). Most widely available port speed is 10 Gbps which is offered at every location, unlike 1 Gbps and 100 Gbps which some locations don\u0026rsquo;t support.\nConnection Establish a connection to AWS Direct Connect location.\nDedicated A physical ethernet connection associated with a single customer, can be a 1 Gbps, 10 Gbps, or 100 Gbps Ethernet port.\nHosted A physical ethernet connection privisioned by an AWS partner for a customer. Only supports ONE [[#Virtual Interface (VIF)]]\nVirtual Interface (VIF) A virtual interface is the VLAN that transports Direct Connect traffic. It can be connected to either a Virtual Private Gateway (VGW) on a VPC, or Direct Connect Gateway (DXGW)\nIf connected to a VGW, it enables connectivity to a single VPC in a single region.\nIf connected to a DXGW, it enables connectivity to multiple VPCs, across regions.\nPrivate Virtual Interface (Private VIF) Access Amazon VPC resources using private IP addresses.\nPublic Virtual Interface (Public VIF) Access AWS public services using public IPs.\nTransit Virtual Interface (Transit VIF) Transports traffic from a Direct Connect Gateway (DXGW) to one or more Transit Gateway.\nOnly available for Dedicated or Hosted connection with speeds of 1 Gbps or more.\nDirect Connect Gateway (DXGW) Use DXGW to connect Direct Connect connection to one or many vpc over a Private Virtual Interface (Private VIF)\nYou can attach multiple Private VIF to a DXGW, this enables North-South connectivity (between on-prem and AWS) BUT does not enable East-West connectivity (between VPCs on AWS).\nThis is strictly for private connectivity. You cannot connect a Public Virtual Interface (Public VIF) to a DXGW.\nConnection Patterns [!info] Q The DXGW is a control plane-only device. In other words, no actual traffic goes over it, it is just a Route-Reflector, it only carries the routing table.\nSource - AWS Blog\nQuestions The DXGW is a control plane-only device. In other words, no actual traffic goes over it, it is just a Route-Reflector, it only carries the routing table?\nGiven the above, what physical medium does the traffic use? And how does it compare to the Transit Gateway?\nConnect DC location to VGW over Private VIF 1 virtual interface can connect to only 1 VPC (via virtual-private-gateway) 1 direct connect connection supports max 50 VIF 1 BGP peering session per VPC. Region specific to Direct Connect region.\nConnect DC location via DXGW to VGW over Private VIF Direct Connect Gateway (DXGW) is a global resource. It can connect to upto 10 VPCs (via VGW) over 1 VIF. It allows North-South (on-prem to AWS) connectivity but not East-West (between VPCs). 1 BGP peering session per DXGW per Direct Connect connection. Good fit if the size of the landing zone on AWS is small, less than 10 VPCs.\nConnect DC location via DXGW to TGW over Transit VIF Use Transit Virtual Interface to connect Direct Connect Gateway to Transit Gateway.\n1 DXGW can connect upto 3 TGW over 1 Transit VIF. Simplest, scalable setup to establish reliable connectivity with AWS without going through the Internet. TGW can only advertise 20 Prefix over the Transit IVF to on-prem router. Cost = Direct Connect pricing + TGW attachment + TGW data transfer charges\nConnect DC location via VPN to TGW over Public VIF Use to encrypt your Direct Connect traffic at Network layer.\nThe Public VIF advertises all AWS public IP to on-prem router including the public IP of VPN endpoints created when adding a VPN attachment to Transit Gateway.\nAdvisable to use some firewall routing at the on-prem router since AWS advertises all public IP ranges.\nConnect DC location via GRE tunnel to TGW over Transit VIF Use Transit Gateway Connect attachment type to connect on-prem SD-WAN virtual appliances directly to Transit Gateway without IPSec VPN using GRE tunnel. This enables high bandwidth performance compared to a VPN.\nWhich pattern to pick? Decide based on the requirements, and the advantages/disadvantages of each pattern.\nThe simplest (not cheapest) approach is to use the 3rd pattern. Connect DC location via DXGW to Transit Gateway over Transit VIF. This helps centralize all your connectivity needs to a single Transit Gateway.\nConsider the 1st pattern, connecting to a VPC directly using a private VIF for large amounts of data transfer. This will save Transit Gateway data processing charges applicable when using the 3rd pattern.\nConsider using the 4th pattern, Transit Gateway Connect + GRE tunnel to connect the on-prem SD-WAN appliances/infrastructure to AWS without VPN.\nResilience To maintain production grade resilience, it is recommended to use at-least 2 Direct Connect connections terminating at 2 appliances/devices in each of 2 different customer sites. This provides 4 connections in total with a high degree of resilience.\nFor development workloads, you can configure 2 connections terminating at 2 appliances/devices at a single customer site.\nBackup Connection It is also advisable to have a high performance backup connection to the Direct Connect connections.\nVirtual Private Gateway (site to site VPN) is a simple pattern to enable backup connectivity between customer sites and AWS. However, due to limited bandwidth of 1.25 Gbps/tunnel, and no support of ECMP, it is generally not favoured.\nTo maintain high performance over the backup connection, it is advisable to use Transit Gateway VPN which supports ECMP. Even thought the individual tunnels still operate at 1.25 Gbps speeds, due to ECMP, it is possible to achieve much higher speeds.\nInspect all traffic coming into AWS? https://aws.amazon.com/blogs/networking-and-content-delivery/announcing-amazon-virtual-private-gateway-ingress-routing-support-for-gateway-load-balancer/\nFurther Reading https://aws.amazon.com/blogs/networking-and-content-delivery/influencing-traffic-over-hybrid-networks-using-longest-prefix-match/\n","permalink":"https://abiydv.github.io/notes/aws/direct-connect/","summary":"While VPN connections are OK, for some critical workloads, VPN over internet may not be stable or available enough.\nIf a customer doesn\u0026rsquo;t want to connect to AWS over the internet, they can alternatively use Direct Connect. This ensures the traffic to/from AWS flows through the customer\u0026rsquo;s network directely on to AWS backbone network without going over the internet.\nCost effective, secure, performant private network connectivity between on-prem private network and AWS.","title":"AWS Direct Connect"},{"content":"Managed service which offers a hub and spoke model for connecting AWS VPCs and on prem networks. Low operational overhead but not the cheapest.\nFeatures Multiple VPCs can connect to a single transit gateway, which can allow on-prem connectivity via direct connect or cross region connectivity via transit gateway peering.\nWhen using a VPN, or direct-connect, each VPC must only be connected to the TGW, which in turn is coonected to the DC or VPN.\n3 transit gateway connections are supported over 1 direct connect.\nIf isolation is required between VPCs connected to the TGW, it can be achieved using route table rules.\nWorks cross-account only. For cross-regions, use Transit Gateway peering.\nAWS Transit Gateway works in AZ affinity mode, so keeps the traffic in the specific AZ as long as the destination is in the same AZ.\nMultiple Transit Gateways may be used in the same region for operational ease, restricting the blast radius. Centralized design vs distributed design choices. Limit of 5000 connections per region, this may also dictate if additional TGWs are required.\nLaunch in a dedicated account like networking, and share across the organization accounts using AWS Resource Access Manager. One caveat, it does not share the tags from the source account.\ntransit gateway vpc attachement has an hourly charge\nIf you have a multi-Region deployment, we recommend that you use a unique ASN for each of your transit gateways.\nMix of static and auto-propagated routes Administrative distance Static routes take precedence over auto-propagated routes on tgw Attachment Types #todo Add information about different attachments that TGW supports\nAWS Transit Gateway Connect Attachment type - Transit Gateway Connect Use AWS Transit Gateway Connect to connect AWS infrastructure with on-prem SD-WAN appliances. Use Transit Gateway Connect BGP for dynamic routing and GRE tunnel protocol for high performance, delivering up to 20 Gbps total bandwidth per Connect attachment (up to four Transit Gateway Connect peers per Connect attachment). VPN Type of VPC edge consolidation Supports IPSec termination for site-to-site VPN, customers can create tunnels terminating at TGW and connect to VPCs connected to the TGW. Supports ECMP for VPN attachements which allows you to scale beyond the default limit of 1.25 Gbps per VPN connection. More details - Scaling VPN throughput. Appliance mode Without appliance mode, the cross AZ traffic might be dropped when destination is across AZ due to the TGW AZ affinity property. In appliance mode, transit gateway picks a network interface in the appliance VPC and uses that same one for the return traffic as well.\nUsing 4-tuples of an IP packet, TGW selects a single TGW ENI in the Appliance VPC for the life of a flow to send traffic to.\nRef https://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-appliance-scenario.html\nReferences https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-inspection-architecture-with-aws-gateway-load-balancer-and-aws-transit-gateway/ https://aws.amazon.com/blogs/networking-and-content-delivery/building-a-global-network-using-aws-transit-gateway-inter-region-peering/ https://aws.amazon.com/blogs/security/simplify-dns-management-in-a-multiaccount-environment-with-route-53-resolver/ https://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-appliance-scenario.html https://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-nat-igw.html ","permalink":"https://abiydv.github.io/notes/aws/transit-gateway/","summary":"Managed service which offers a hub and spoke model for connecting AWS VPCs and on prem networks. Low operational overhead but not the cheapest.\nFeatures Multiple VPCs can connect to a single transit gateway, which can allow on-prem connectivity via direct connect or cross region connectivity via transit gateway peering.\nWhen using a VPN, or direct-connect, each VPC must only be connected to the TGW, which in turn is coonected to the DC or VPN.","title":"AWS Transit Gateway"},{"content":"What is a VPC? Virtual Private Cloud\nAn isolated virtual network present in your AWS account. There is a default VPC present. However, it is recommended to create your own custom VPC based on your environment.\nYou can create a IPv4 only or Dual stack (IPv4 + IPv6) VPC.\nDefault VPC You can modify, delete, recreate a default VPC Creating a default VPC does not need VPC specific permissions as all resources are created by AWS\nCustom A custom VPC can not be marked as default even if there is no default VPC in the region.\nVPC CIDR Blocks When creating a VPC you must specify an IPv4 CIDR range. This becomes the VPC\u0026rsquo;s primary CIDR range. All private ranges defined in RFC#1918 are valid to be used -\n10.0/8 172.16/12 192.168/16 100.64/10 (CGNAT) The largest and smallest VPC CIDR ranges that can be specified are /16 and /28 respectively. Yes, you cannot create a /8 VPC, and the smallest VPC /28 only has 14 usable IPs.\nSubnet mask for prefix /28 = 255.255.255.240 Total IPs = 10.0.0.0 to 10.0.0.15 (16 IPs). Usable IPs = 10.0.0.1 to 10.0.0.14 (14 IPs)\nDANGER Avoid using the Docker default bridge range 172.17.0.0/16 as a CIDR range for a VPC, since it may cause conflicts with some AWS services that use the same range like SageMaker.\nConstraints You can assign a secondary CIDR range to a VPC with some constraints as follows\nIP range Restricted Allowed 10.0/8 198.19/16172.16/12192.168/16 Non-overlapping range from 10.0/8100.64/10Any other public range 172.16/12 198.19/1610.0/8192.168/16 Non-overlapping range from 172.16/12100.64/10Any other public range 192.168/16 198.19/1610.0/8172.16/12 Non-overlapping range from 192.168/16100.64/10Any other public range 198.19/16 10.0/8172.16/12192.168/16 100.64/10Any other public range Any other public range, or 100.64/10 198.19/1610.0/8172.16/12192.168/16 Non-overlapping range from 100.64/10Any other public range Subnet CIDR Blocks Subnet CIDR block can be the same as VPC CIDR block - 1 subnet in a VPC. Or, it can be a subset of the VPC CIDR block to create multiple subnets in a VPC.\nSubnet CIDR blocks cannot overlap each other.\nSimilar to VPC CIDR block, the largest, and smallest CIDR block for subnets are /16 (~65 IPs ) and /28 (~14 IPs).\nAWS reserves 5 IPs in each subnet block. For example, in a subnet with CIDR block 10.0.0.0/24, following IPs are reserved by AWS\n10.0.0.0 Network Address 10.0.0.1 VPC Router 10.0.0.2 DNS server, usually, network range + 2 route53 10.0.0.3 Future use 10.0.0.255 Network broadcast address. No broadcasr support in VPC, so reserved. Prefix Lists A collection of CIDR block entries (and an optional description).\nProperties Supports both IPv4, IPv6, but these cannot be mixed in the same list. Region specific Needs a maximum entry count while creation. Can be expanded later. Resources referencing them always use the latest version Can be shared with other account using AWS Resource Access Manager Weight of an AWS managed prefix list is the number of entries it takes up in a resource. For ex - cloudfront prefix list weight is 55, so it takes up 55 entries in a security group. Types AWS Managed AWS service CIDR blocks, you can not edit, share, delete, add these.\nCustomer managed Useful for sharing commonly used CIDR blocks in a customer\u0026rsquo;s environment.\nSecurity Groups Endpoints VPC endpoints are virtual devices enabling connectivity for compute or services within a VPC.\nTypes Interface Collection of managed ENIs with private IP to access AWS services (except dynamodb, s3), custom services, services from marketplace privately.\nGateway Load Balancer Route traffic to a fleet of virtual appliances using private IPs.\nGateway Targets specific IP routes Prefix Lists for traffic destined for AWS DynamoDB and AWS S3\nIngress Routing Here, the custom route table attached to the [[internet-gateway|AWS Internet Gateway]] forwards all incoming traffic to the network interface (eni0) of an EC2 instance. This instance could be running a security appliance like network-firewall to inspect the traffic.\nSimilarly, the route table for application subnet has a route which forwards all internet bound traffic to the instance\u0026rsquo;s network interface. Note: The appliance doesn\u0026rsquo;t do any NAT, so the internet communication is based on the public IP of the workload instance.\nHow to distribute workloads in VPCs? There are multiple ways to distribute workloads across different VPCs. Each approach has it\u0026rsquo;s pros and cons, so they should be evaluated based on the customer\u0026rsquo;s specific requirements.\nBased on Environment (dev, prod, shared services). Based on Compliance needs like public facing services, internal only, PCI compliant resources etc. Based on data sovernity restrictions (US, EU etc.) - across different regions. Based on company structure, these can be seggregated further into their own specific accounts for each company division and so on. VPC to VPC Connectivity As the number of VPCs increase in a customer\u0026rsquo;s environment, the next obvious question that arises is, how to maintain connectivity between different VPCs, between VPCs and on-prem systems etc.\nThere are 2 broad approaches to establishing such connections. They are -\nPoint to point - Traffic flows between specific VPCs. Hub and spoke - Traffic flows via a central resource based on certain defined rules. VPC Peering Point to point connectivity model.\nThis is a point to point connectivity method in which 2 VPCs are connected directly. It allows bidirectional traffic to flow between them. It does not support transitive connectivity.\nVPC A peered with VPC B, and VPC B peered with VPC C. Traffic from VPC A cannot reach VPC C via B. To enable connectivity between A \u0026amp; C, a new peering connection must be estabished.\nMaximum limit of 125 peering connections per VPC, so this does not scale very well.\nLowest cost for connecting VPCs since the charges are only for the traffic passing through the connection, there is no standing cost.\nWhen using a VPN, or [[direct-connect|Direct Connect]], each VPC must be connected individually to the DC or VPN. Peering does not support transitive routing, remember?\nWorks cross-region, cross-account.\nBest suited for VPCs less than 10.\nTraffic isolation can be achieved using security groups on specific resources (or NACLs as well?).\nTransit Gateway A hub-and-spoke model using Transit Gateway. Cost implications.\nTransit VPC A hub-and-spoke model.\nIn this solution, a central VPC with customer managed software appliances is used to route traffic between various connected networks. All VPCs can connect to this central VPC to gain access to the connected network. This is similar to Transit Gateway solution but is managed by customer.\nTransitive connectivity is provided by a VPN overlay network using BGP IPSec.\nThe central VPC contains EC2 instances running software appliances that route the traffic to their destination using the VPN overlay network.\nCustomers can use the same L7 Firewall, IDP, IPS products for their on-prem/cloud workloads if using a vendor provided product as the software appliance.\nNot very high performance - limited throughput of 1.2 Gbps/VPN tunnel.\nIt brings in additional complexity, cost, administrative and performance challenges.\n[[private-link|AWS Private Link]] Access AWS services, 3rd party SAAS apps directly in your VPC without traversing the internet.\nVPC Lattice Abstracts away the network complexity for application developers.\nIt creates logical application network layer called service network which allows service to service communication across VPC, regions, accounts over HTTP/HTTPS, or gRPC. It operates on a data plane in the VPC using the link local address layer 169.254/16 RFC#3927. This data plane is exposed via an endpoint within the VPC. Once a service network is associated with a VPC, services deployed within the VPC can discover other services made available by the service network.\nQUESTION In the above diagram, while VPC1, 3 are associated with the service network, VPC2 isn\u0026rsquo;t. So what\u0026rsquo;s the impact of this? All 3 services are associated with the service network meaning they are discoverable by resources running in VPCs associted with the service network i.e. resources in VPC1, 3. But, they aren\u0026rsquo;t discoverable from VPC2. This means the service in VPC2 doesn\u0026rsquo;t really depend on any other service and just responds to requests received.\nService An independent software unit that performs a specific function. While configuring a service with a service network, you use similar resources like a load balancer - listners, routing rules and target groups.\nService directory Central directory (at account level) of registered services, either deployed in the same account or shared via AWS Resource Access Manager.\nService network Logical grouping of services to enable connectivity and apply common policies\nAuth Policy Resource policy to ensure only authenticated and authorized clients use the services\nVPC Sharing VPC and/or subnets can be shared across accounts in an Org using AWS Resource Access Manager.\nThis is a low cost, low operational overhead solution, however, quite permissive in terms to network isolation.\nNetwork isolation can be improved by using grouping apps by behaviour and using separate subnets with relevant NACL for each group. Further isolation can be achieved by using security groups.\nPrivate NAT Gateway Connect networks with overlapping CIDR blocks.\nINFO Here, the 2 VPCs have non-routable overlapping range of 100.64/16. Based on secondary CIDR [[#Constraints| constraints, select a non-overlapping routable CIDR block for both VPC. VPC A = 10.0.1.0/24, VPC B = 10.0.2.0/24 Now, subnets are created within this secondary range in both VPCs - 10.0.1.0/25, and 10.0.2.0/25. These new subnets are used to launch private NAT gateways which will do the address translation. The new subnets are attached to the transit gateway with appropriate routes defined to refer to each other. In the non-routable sunets, routes are updated to send all non-local requests to the NAT.\nAs per the VPC A non-routable subnet route table, the packet with destination of ALB IP (10.0.2.x) is headed to the private NAT in VPC A routable subnet. Private NAT does the network translation, and forwards the packet to the TGW attachement as per the VPC A routable subnet route table. As per the TGW route table, the packet is then forwarded to the ALB in VPC B routable subnets. The ALB then forwards the packet to the destination in VPC B non-routable subnet. Response follows a similar path back - 5, 6, 7, 8. Reference\nVPC peering vs. Transit VPC vs. Transit Gateway Comparison of VPC peering, Transit VPC, and Transit Gateway\nCriteria VPC peering Transit VPC Transit Gateway Architecture Full mesh VPN-based hub-and-spoke Attachments-based hub-and-spoke. Can be peered with other TGWs. Complexity Increases with VPC count Customer needs to maintain EC2 instance/HA AWS-managed service; increases with Transit Gateway count Scale 125 active Peers/VPC Depends on virtual router/EC2 5000 attachments per Region Segmentation Security groups Customer managed Transit Gateway route tables Latency Lowest Extra, due to VPN encryption overhead Additional Transit Gateway hop Bandwidth limit No limit Subject to EC2 instance bandwidth limits based on size/family Up to 50 Gbps (burst)/attachment Visibility VPC Flow Logs VPC Flow Logs and CloudWatch Metrics Transit Gateway Network Manager, VPC Flow Logs, CloudWatch Metrics Security group cross-referencing Supported Not supported Not supported Cost Data transfer EC2 hourly cost, VPN tunnels cost and data transfer Hourly per attachment, data processing, and data transfer Hybrid Connectivity There are 2 approaches to establish hybrid connectivity. Both utilize VPN, but differ in where and how the VPN connection terminates.\nOne to One In this case, each VPC is connected to the VPN individually using Direct Connect/Virtual Private Gateway.\nEdge Consolidation Connect the VPN to a central transit-gateway and then attach VPCs to this central Transit Gateway.\nOnce the approach is decided, there are 4 different ways to terminate VPN connection on AWS\nTransit Gateway Recommended option, to connect VPN to a central Transit Gateway. Cost = AWS Transit Gateway Pricing + AWS VPN pricing.\nEC2 Some customers might want to use 3rd party software products, so terminating the VPN on an EC2 instance is also an option. This is basically one to one connectivity.\nTo enable Edge consolidation, you can use the [[#Transit VPC]] pattern. However, the same drawbacks as highlighted in that section remain.\nThis is also an option when wanting to use GRE.\nVirtual Private Gateway VGW Great option when starting out, since this is a [[#One to One]] connectivity design where you need to configure this on a per VPC basis.\nOffers redundancy due to 2 tunnels/connection design.\nDoes not support ECMP for VPN connections so throughput is limited to 1.25 Gbps per tunnel.\nCost = AWS VPN pricing only, no charge for Virtual Private Gateway\nQuestion How are Virtual Private Gateway and Customer Gateways related?\non-prem ------ aws customer ------ aws virtual device ------ gateway ------ private gw ---- aws vpc |\u0026lt;--------- site to site VPN connection ---------\u0026gt; Question How are Virtual Private Gateway and Direct Connect related?\non-prem ---- direct ---- virtual --- aws virtual device ---- connect ---- interface --- private gw --- aws vpc Client VPN Endpoint Another option when starting out. This is a One to One connectivity design where you need to configure this on a per VPC basis, or make use of other VPC connectivity features to achieve an Edge Consolidation connectivity design.\nTo configure client VPN endpoint, you need -\nTarget Network - subnets in a VPC, 1 per AZ. Additional routes - local, to peered VPC or VPGW etc. Authentication Mechanism SAML 2.0 (Single IdP only) Mutual authentication, cert based - each client needs a new cert Active Directory Client CIDR block between /12 and /22 prefix/subnet mask. Server certificate in [[AWS Certificate Manager]] regardless of authentication mechanism selected Enable self service portal (optional) Enable split tunnel (optional) Once configured, download the client VPN configuration file, and use it to connect to the VPN endpoint. This file contains info like the DNS name of the VPN endpoint, authorization mechanism, client CIDR blocks to use etc.\nAuthorization Security groups (attached to the client VPN endpoint) and network based authorization rules using either AD attributes or IdP provided metadat like groups etc.\nSplit Tunnel Use this to send specific traffic to the AWS tunnel. By default, the client endpoint route adds a 0.0.0.0 route to the client which sends all traffic from the client machine to the AWS tunnel. This can increase data transfer costs if everything goes via AWS.\nSplit tunnel adds specific routes to the client device route table which ensures only the traffic destined for AWS is sent over the tunnel. When using split tunnel ensure to not have a 0.0.0.0 route in the client VPN, otherwise it will still send traffic not destined for AWS to the tunnel.\nDirect Connect Connect On-prem network to AWS backbone network.\nLimits 250 network interfaces per VPC.\nReferences https://aws.amazon.com/blogs/aws/new-vpc-ingress-routing-simplifying-integration-of-third-party-appliances/ https://aws.amazon.com/blogs/containers/eks-vpc-routable-ip-address-conservation/ https://aws.amazon.com/blogs/networking-and-content-delivery/designing-hyperscale-amazon-vpc-networks/ https://aws.amazon.com/blogs/networking-and-content-delivery/how-to-solve-private-ip-exhaustion-with-private-nat-solution/ https://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-appliance-scenario.html https://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-nat-igw.html https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html#gateway-route-table https://repost.aws/knowledge-center/vpc-analyze-inbound-traffic-nat-gateway ","permalink":"https://abiydv.github.io/notes/aws/vpc/","summary":"What is a VPC? Virtual Private Cloud\nAn isolated virtual network present in your AWS account. There is a default VPC present. However, it is recommended to create your own custom VPC based on your environment.\nYou can create a IPv4 only or Dual stack (IPv4 + IPv6) VPC.\nDefault VPC You can modify, delete, recreate a default VPC Creating a default VPC does not need VPC specific permissions as all resources are created by AWS","title":"AWS Virtual Private Cloud (VPC)"},{"content":"REST API CloudFormation resource AWS::ApiGateway::RestApi Regional service Edge-optimized Can be made private Integration types AWS Use this to create AWS API proxies for virtually any service AWS_PROXY lambda HTTP_PROXY HTTP MOCK Offers caching HTTP API CloudFormation resource AWS::ApiGatewayV2::Api Lightweight version of the REST API Regional service NOT Edge-optimized NOT Private Integration types AWS_PROXY Lambda, SQS, Kinesis Data Stream, AppConfig, Step Functions, EventBridge with proper integration sub-types. Ref HTTP_PROXY WebSocket API Integration types AWS AWS_PROXY Lambda HTTP_PROXY HTTP MOCK Usage plan Set throttling limits and send a 429 Too many requests HTTP response to clients\nThrottling rule evaluation sequence -\nPer client, per method limit set on a API stage Per client limit on the API Default per method limit or individual per method limit set on API stage Per region account level limit Error Note API gateway does not support HTTP endpoints, it is HTTPS only. Use CloudFront to add (http -\u0026gt; https) redirection.\nFAQ https://aws.amazon.com/api-gateway/faqs/\n","permalink":"https://abiydv.github.io/notes/aws/api-gateway/","summary":"REST API CloudFormation resource AWS::ApiGateway::RestApi Regional service Edge-optimized Can be made private Integration types AWS Use this to create AWS API proxies for virtually any service AWS_PROXY lambda HTTP_PROXY HTTP MOCK Offers caching HTTP API CloudFormation resource AWS::ApiGatewayV2::Api Lightweight version of the REST API Regional service NOT Edge-optimized NOT Private Integration types AWS_PROXY Lambda, SQS, Kinesis Data Stream, AppConfig, Step Functions, EventBridge with proper integration sub-types. Ref HTTP_PROXY WebSocket API Integration types AWS AWS_PROXY Lambda HTTP_PROXY HTTP MOCK Usage plan Set throttling limits and send a 429 Too many requests HTTP response to clients","title":"AWS API Gateway"},{"content":"Retired, not available for new customers. Succeeded by Glue\n","permalink":"https://abiydv.github.io/notes/aws/data-pipeline/","summary":"Retired, not available for new customers. Succeeded by Glue","title":"AWS Data Pipeline (Retired)"},{"content":"Formerly called CloudWatch Events\n","permalink":"https://abiydv.github.io/notes/aws/eventbridge/","summary":"Formerly called CloudWatch Events","title":"AWS EventBridge"},{"content":" Data integration service to process/prepare data before analysis. Data catalog to manage data from different sources in a central place. ETL pipelines to load data into data warehouses/data lakes etc. ","permalink":"https://abiydv.github.io/notes/aws/glue/","summary":" Data integration service to process/prepare data before analysis. Data catalog to manage data from different sources in a central place. ETL pipelines to load data into data warehouses/data lakes etc. ","title":"AWS Glue"},{"content":"What is Kinesis Data Firehose? Streaming ETL or streamig data pipeline. Not this [[data-pipeline|Data Pipeline]] though! Load streaming data into OpenSearch, Redshift, S3, or 3rd party HTTP endpoints. Batch, compress, buffer and encrypt data before loading to minimize storage needs at destination Read streaming data from [[kinesis-data-streams|Kinesis data streams]] Scales elastically with demand Replicates data across AZ for high availability and durability. Use Kinesis Agent installed servers to stream data to Firehose. Supports both linux/windows. Built-in data format conversions into parquet ot ORC Use-cases Say you want to ingest lots of data from S3 to OpenSearch cluster. The incoming data pattern is not linear, so there can be spikes. When ingesting via lambda, there could be timeouts or errors from OpenSearch if a large amount of data is pushed in a short window. Kinesis data firehose can smoothen out these spikes. ","permalink":"https://abiydv.github.io/notes/aws/kinesis-data-firehose/","summary":"What is Kinesis Data Firehose? Streaming ETL or streamig data pipeline. Not this [[data-pipeline|Data Pipeline]] though! Load streaming data into OpenSearch, Redshift, S3, or 3rd party HTTP endpoints. Batch, compress, buffer and encrypt data before loading to minimize storage needs at destination Read streaming data from [[kinesis-data-streams|Kinesis data streams]] Scales elastically with demand Replicates data across AZ for high availability and durability. Use Kinesis Agent installed servers to stream data to Firehose.","title":"AWS Kinesis Data Firehose"},{"content":" Real-time data ingestion, aggregation and then loading the data into a data-warehouse like [[#Redshift]] or [[#EMR]] cluster. Replicated across 3-AZ, so high availability, and data durability. Auto-scaling Delay between data ingestion, and retrieval is typically less than a second. Multiple applications can consume data from the same stream and do different things in parallel, like for ex - processing and archiving. Use cases Logs intake, processing Real-time metrics and reporting Real-time streaming Complex stream processing like DAG (Directed Acyclic Graphs) by ingesting from multiple [[#Kinesis#Data Streams]]. ","permalink":"https://abiydv.github.io/notes/aws/kinesis-data-streams/","summary":"Real-time data ingestion, aggregation and then loading the data into a data-warehouse like [[#Redshift]] or [[#EMR]] cluster. Replicated across 3-AZ, so high availability, and data durability. Auto-scaling Delay between data ingestion, and retrieval is typically less than a second. Multiple applications can consume data from the same stream and do different things in parallel, like for ex - processing and archiving. Use cases Logs intake, processing Real-time metrics and reporting Real-time streaming Complex stream processing like DAG (Directed Acyclic Graphs) by ingesting from multiple [[#Kinesis#Data Streams]].","title":"AWS Kinesis Data Streams"},{"content":"Scope Region\nFeatures Multi region - replicate easily Automatic key rotation, uses lambda Encrypted in-transit, at-rest Supports SecretString, SecretBinary. Within SecretString, it\u0026rsquo;s advisable to save data as json. Ref Alternatives SSM Parameter store\n","permalink":"https://abiydv.github.io/notes/aws/secrets-manager/","summary":"Scope Region\nFeatures Multi region - replicate easily Automatic key rotation, uses lambda Encrypted in-transit, at-rest Supports SecretString, SecretBinary. Within SecretString, it\u0026rsquo;s advisable to save data as json. Ref Alternatives SSM Parameter store","title":"AWS Secrets Manager"},{"content":"Scope Region\nFeatures Apps PUSH messages to a topic.\nYou can subscribe a queue (SQS) or Lambda to a topic cross-region, cross-account. When doing so it is important to use the global condition key SourceArn or SourceAccount in queue resource policy to restrict which topic can publish to the queue.\nOften the first building block of a fan out architectural pattern.\ngraph LR src(source) --\u003e sns sns(sns use1) --\u003e q1(sqs1 usw1) --\u003e app1 sns --\u003e q2(sqs2 use1) --\u003e app2 sns --\u003e q3(sqs3 euw1) --\u003e app3 ","permalink":"https://abiydv.github.io/notes/aws/sns/","summary":"Scope Region\nFeatures Apps PUSH messages to a topic.\nYou can subscribe a queue (SQS) or Lambda to a topic cross-region, cross-account. When doing so it is important to use the global condition key SourceArn or SourceAccount in queue resource policy to restrict which topic can publish to the queue.\nOften the first building block of a fan out architectural pattern.\ngraph LR src(source) --\u003e sns sns(sns use1) --\u003e q1(sqs1 usw1) --\u003e app1 sns --\u003e q2(sqs2 use1) --\u003e app2 sns --\u003e q3(sqs3 euw1) --\u003e app3 ","title":"AWS Simple Notification Service (SNS)"},{"content":"Scope Region\nFeatures Apps PULL messages from a queue, process it and DELETE it. If the message is not deleted by a consumer, it is returned back to the queue after visibility timeout expires.\nFIFO preserves order and ensure once only delivery. Also deduplication. Visibility timeout Retention period - max 14 days, by default 4 days. Delivery delay Redrive allow policy Redrive Move messages from DLQ to the main queue for re-preocessing. Low value of maxReceiveCount can move messages to dead letter queue before any processing occurs in the main queue. Set it high to allow for retries in the main queue before a message is moved to the DLQ. Visibility Timeout Ideally, set the visibility timeout to roughly the same time as it takes a consumer to process and delete the message.\nShort Polling vs Long Polling In Short polling, SQS polls some of its servers to fech messages. Returns found messages, or empty if no message where found on the polled servers. On the next invocation, it fetches messages from other servers, finds the missed messages and sends it across. Eventually all messages get delivered, but empty responses or false empty responses are sent frequently.\nWaitTimeSeconds=0 specified for ReceiveMessage call or ReceiveMessageWaitTimeSeconds=0 queue property triggers short polling.\nIn Long polling, SQS collects messages from all servers before sending the response back. In this case, very rarely is the response empty or false empty. This also reduces overall cost of using SQS queues.\nWaitTimeSeconds\u0026gt;0 specified for ReceiveMessage call or ReceiveMessageWaitTimeSeconds\u0026gt;0 queue property triggers long polling. Max long polling time is 20 seconds.\nWaitTimeSeconds for ReceiveMessage is honoured (if specified) over queue property.\nAuto-scaling .. based on SQS message queue depth. CloudWatch metric does not handle auto scaling at the extremes quite as well - too few messages or too many messages.\nIn this case, a custom metric can be created like backlog per resource which is just message in the queue/current resources. This can be done by a cron based lambda executing every 60s. This custom metric can then be used for a more accurate auto-scaling EC2 or ECS.\nBackend Architecture This blog post from AWS provides some insights into SQS backend.\nIt consists of several micro-services, 2 of the important ones being front-end, and storage-backend.\nPre 2024 frontend service accepts API calls, handles authorization, and forwards the request to the storage-backend service.\nThe system consists of multiple clusters. Each cluster has multiple hosts and handles multiple customer queues. A customer queue can also be assigned to multiple clusters.\nSo, frontend system\u0026rsquo;s hosts connect to multiple hosts of the storage backend service. Using connection pools helps reuse some of these connections, however, given the scale of messages being transferred (100 mil/sec at peak times) this can hit hardware limits of how many connections can be open at a time.\nPost 2024 New frame1 transfer protocol between these 2 service deployed. It uses multiplexing, and can handle multiple requests/responses over a single connection. Reduced the load on the system, and it can now handle 20% more requests with the same specs. Performance times also imporved, reducing the time a message spends \u0026ldquo;in the backend\u0026rdquo; by almost 20%.\nQuestions How does auto-scaling based on message age in queue behave? References 1https://book.systemsapproach.org/direct/framing.html ","permalink":"https://abiydv.github.io/notes/aws/sqs/","summary":"Scope Region\nFeatures Apps PULL messages from a queue, process it and DELETE it. If the message is not deleted by a consumer, it is returned back to the queue after visibility timeout expires.\nFIFO preserves order and ensure once only delivery. Also deduplication. Visibility timeout Retention period - max 14 days, by default 4 days. Delivery delay Redrive allow policy Redrive Move messages from DLQ to the main queue for re-preocessing.","title":"AWS Simple Queue Service (SQS)"},{"content":"Object storage offering 99.999999999% (11 nines) data durability.\nInternals Based on Amazon Dynamo (not the same as DynamoDB) - technology developed internally at amazon for a incrementally scalable, highly available key-value storage system.\nThis article by DR. Werner Vogels, is an interesting read on how S3 came to be!\nScope Regional, although AWS does call it global at places.\nCan you create a bucket without specifying a region?\nConsistency read-after-write consistency for PUT, and DELETE requests Strongly consistent for reads, always returns the most recent data. Note, a read op initiated before a write op is finished can return either old or new data. No out of the box object locking for concurrent writes (manage in app code), so be mindful of write conflicts last-writer-wins for conflict resolution of concurrent writes More here\nStorage class Standard Intelligent tiering Express One-Zone (High Performance) Standard Infrequent Access One-Zone Infrequent Access Glacier Instant Retrieval Glacier Flexible Retrieval Glacier Deep Archive S3 on [outposts] More https://aws.amazon.com/s3/storage-classes/\nLifecycle rules Use to transition between storage classes or deleting/expiring objects etc. Be sure to not operate on too many small objects as the transition cost will be more than any cost gains due to storage class change.\nObject lock Use to retain particular objects for compliance reasons.\nEncryption AWS encrypts all objects uploaded to S3 using SSE-S3 by default. This can be changed in bucket settings.\nOther encryption options (SSE = server side encryption) -\nSSE-C (customer managed keys) SSE-KMS (kms keys - customer or aws managed) # DSSE-KMS (dual-layer SSE using KMS keys) # With SSE-KMS there can be cost implications (KMS charges) if too many encrypted files are uploaded/downloaded. Always use a bucket key with SSE-KMS to reduce KMS costs.\nBucket Key Source\nUses the bucket ARN as the encryption context instead of the object ARN if using bucket keys, thus reducing calls to KMS, and eventually cost Create unique data keys for objects Used for a time-limited period within S3 Encrypt existing objects? S3 Batch Operations - copy objects to same bucket. Use AWS API (SDK/CLI) to copy objects to same bucket. Replication If source object uses S3 Bucket Keys \u0026amp; destination bucket uses default encryption, replica object maintains its S3 Bucket Key encryption settings in the destination bucket. If source object is not encrypted \u0026amp; destination bucket uses S3 Bucket Key with SSE-KMS, replica object is encrypted with an S3 Bucket Key using SSE-KMS. This results in the ETag of the source object being different from the ETag of the replica object. Options for data replication - https://aws.amazon.com/blogs/storage/considering-four-different-replication-options-for-data-in-amazon-s3/ 2-way sync for S3 objects https://aws.amazon.com/about-aws/whats-new/2020/12/amazon-s3-replication-adds-support-two-way-replication/ Access Logs Useful for audit and security purposes. Deliver to a different S3 bucket. Alternative to switching on cloudtrail data events, and incurring higher cost.\nLogs can be queried via athena.\nEndpoints Use endpoints to keep the data transfer within AWS backbone network, and reduce data transfer charges. S3 supports both gateway endpoints and interface endpoints.\nGateway Think of it as the bare minimum for enabling private access to S3 from a vpc. Traffic does not travel via NAT or internet gateways avoiding the data transfer costs associated with them.\nInterface This is a more heavyweight solution, use this to access S3 from on-prem, peered VPCs in other AWS Regions, or even through a transit gateway.\nAccess Points Network endpoints connected to a SINGLE bucket. A single bucket can have multiple access points, preferrebly for each service/app that needs access to the bucket. This design allows for keeping permissions modular and scoped to a particular application without impacting any other app. Unlike bucket policies which tend to be a monolith managing access for ALL apps together.\nMulti-Region Access Points Uses Global Accelerator under the hood to optimize S3 traffic. Ensures low latency access to data from anywhere in the world irrespective of the home region of the bucket and data.\nIt can centrally configure replication rules between buckets in different regions, and use the nearest bucket to fulfil a request.\nTransfer Acceleration S3TA Speeds up S3 transfers for apps where users are geographically far away from the bucket\u0026rsquo;s region. It uses cloudfront edge and AWS backbone network to speed up the data transfer by as much as 50 to 500%. Only accelerated transfers are billed. Enable in S3 bucket properties.\nPerformance Dependent on keys (path)\nEvents File operations can directly trigger events for services\nlambda sns sqs eventbridge If the service isn\u0026rsquo;t supported directly, create a data events only cloudtrail, this will push the event to eventbridge. Add a rule with detail-type:AWS API Call via CloudTrail and target the required service.\nSecurity A public bucket can be tracked by IAM Access Analyzer findings.\nEnable server access logging and send logs to a different bucket. Use these logs for security audit, understanding usage patterns etc. Query via athena.\nSELECT requestdatetime, remoteip, requester, key FROM s3_access_logs_db.mybucket_logs WHERE key = \u0026#39;images/picture.jpg\u0026#39; AND operation like \u0026#39;%DELETE%\u0026#39;; ","permalink":"https://abiydv.github.io/notes/aws/s3/","summary":"Object storage offering 99.999999999% (11 nines) data durability.\nInternals Based on Amazon Dynamo (not the same as DynamoDB) - technology developed internally at amazon for a incrementally scalable, highly available key-value storage system.\nThis article by DR. Werner Vogels, is an interesting read on how S3 came to be!\nScope Regional, although AWS does call it global at places.\nCan you create a bucket without specifying a region?\nConsistency read-after-write consistency for PUT, and DELETE requests Strongly consistent for reads, always returns the most recent data.","title":"AWS Simple Storage Service (S3)"},{"content":"Features Think of it as a device which exposes the S3 storage as iSCSI, SMB or NFS mounts to on-prem workloads.\nLaunch it as a VM in your on-prem data center, or install the [[#Hardware Appliance]]. Reduces the need to re-engineer these workloads to work with S3. Helps with replicating on-prem data to S3 for backups or archival needs. On-prem workloads get access to low latency access to S3 data Data lake access for pre and post processing tasks. Types File Gateway Access S3 via SMB or NFS shares Max 64 TB FSx File Gateway Access FSx via SMB Volume Gateway Access block storage using iSCSI. EBS? Supports upto 32 volumes Cached mode: Max 32 x 32 TB volumes (1 PB / gateway) Stored mode: Max 32 X 16 TB volumes (512 TB / gateway) Tape Gateway iSCSI virtual tape library (VTL) interface to store backups on S3 or Glacier.\nHardware Appliance There is a storage gateway hardware appliance for locations that do not have an existing VMWare infrastructure. This appliance comes in 2 sizes 5 TB or 12 TB.\nQuestions When to pick storage gateway over data sync? ","permalink":"https://abiydv.github.io/notes/aws/storage-gateway/","summary":"Features Think of it as a device which exposes the S3 storage as iSCSI, SMB or NFS mounts to on-prem workloads.\nLaunch it as a VM in your on-prem data center, or install the [[#Hardware Appliance]]. Reduces the need to re-engineer these workloads to work with S3. Helps with replicating on-prem data to S3 for backups or archival needs. On-prem workloads get access to low latency access to S3 data Data lake access for pre and post processing tasks.","title":"AWS Storage Gateway"},{"content":" SSM Agent Most SSM functionality depends on this agent being present on the instance to be managed. Pre-installed on Amazon Linux Needs permission AmazonSSMManagedInstanceCore Use VPC endpoint Application Manager Logical grouping of resources to help provide context around resources, and while troubleshooting issues. Discovers related resources automatically, for ex, using CloudFormation templates.\nAppConfig Store and manage configs for environment separate from app code.\nParameter Store No support for multi region No Rotation Optional encryption in-transit, at-rest Region scope Similar to Secrets Manager Change Management Change Manager Change Calendar: Decide, plan and enforce when changes, can or cannot be made to resources. Maintenance Window Automation: Automate common tasks, uses SSM agent, runbooks etc. Node Management Compliance Fleet Manager Inventory Session Manager Run Command State Manager Patch Manager Distributor Hybrid Activations Operations Incident Manager Explorer OpsCenter ","permalink":"https://abiydv.github.io/notes/aws/systems-manager/","summary":"SSM Agent Most SSM functionality depends on this agent being present on the instance to be managed. Pre-installed on Amazon Linux Needs permission AmazonSSMManagedInstanceCore Use VPC endpoint Application Manager Logical grouping of resources to help provide context around resources, and while troubleshooting issues. Discovers related resources automatically, for ex, using CloudFormation templates.\nAppConfig Store and manage configs for environment separate from app code.\nParameter Store No support for multi region No Rotation Optional encryption in-transit, at-rest Region scope Similar to Secrets Manager Change Management Change Manager Change Calendar: Decide, plan and enforce when changes, can or cannot be made to resources.","title":"AWS Systems Manager (SSM)"},{"content":"A virtual private gateway is the VPN endpoint on a specific VPC.\nUse a virtual private gateway to connect a site-to-site VPN connection to a AWS VPC.\nAttaches to -\nONE VPC ONE Direct Connect Gateway (DXGW) ONE Virtual Interface (VIF) ","permalink":"https://abiydv.github.io/notes/aws/virtual-private-gateway/","summary":"A virtual private gateway is the VPN endpoint on a specific VPC.\nUse a virtual private gateway to connect a site-to-site VPN connection to a AWS VPC.\nAttaches to -\nONE VPC ONE Direct Connect Gateway (DXGW) ONE Virtual Interface (VIF) ","title":"AWS Virtual Private Gateway"},{"content":"Introduction Authenticating to AWS EKS (Elastic Kubernetes Service) using AWS IAM (Identity and Access Management) credentials is a crucial aspect of managing and securing these clusters. However, IAM is not the only option when it comes to authenticating with these clusters. You can also configure additional IDP (identity providers) for this purpose. In this post today, I\u0026rsquo;ll explain the steps necessary to authenticate with Okta.\nPre-requisities Before diving into the specifics, please ensure you have the following -\nAn Okta app, with it\u0026rsquo;s corresponding client id/secret. Terraform configs which create the cluster, and have admin level access to configure the cluster. Configuring the EKS OIDC IDP Module Use this module in your Terraform configuration, specifying the required input variables such as cluster name, cluster email, IDP client ID, issuer URL, group claims, username claims, and admin/readonly groups and users. A sample config will look like this -\n1module eks_oidc_idp { 2 source = \u0026#34;git::ssh://git@github.com/abiydv/aws-eks-okta-auth.git?ref=v1.0.0\u0026#34; 3 4 cluster_name = \u0026#34;my-cluster\u0026#34; 5 cluster_email = \u0026#34;user\u0026#34; 6 idp_client_id = \u0026#34;client-id\u0026#34; 7 idp_config_name = \u0026#34;okta\u0026#34; 8 idp_issuer_url = \u0026#34;https://example.okta.com\u0026#34; 9 idp_group_claim = \u0026#34;groups\u0026#34; 10 idp_username_claim = \u0026#34;username\u0026#34; 11 idp_cluster_admin_groups = [\u0026#34;okta-admins\u0026#34;] 12 idp_cluster_admin_users = [\u0026#34;user1\u0026#34;, \u0026#34;user2\u0026#34;] 13 idp_cluster_readonly_groups = [\u0026#34;okta-devs\u0026#34;] 14 idp_cluster_readonly_users = [\u0026#34;user3\u0026#34;, \u0026#34;user4\u0026#34;] 15 tags = var.tags 16} Apply the Terraform configuration to provision the OIDC provider for your EKS cluster. This might take upto 20 mins to complete.\nInstalling the Kubelogin Plugin To authenticate with Okta, we need to install the Kubelogin plugin. Follow these steps:\nInstall the Kubelogin plugin using the appropriate package manager for your system. Verify the installation by running the kubectl oidc-login --help command. Configuring the Kubectl User In the ~/.kube/config file, add a new user configuration under the users section, specifying the necessary details such as name, exec command, OIDC issuer URL, client ID, client secret, and additional scopes.\n1- name: okta 2 user: 3 exec: 4 apiVersion: client.authentication.k8s.io/v1beta1 5 args: 6 - oidc-login 7 - get-token 8 - --oidc-issuer-url=https://okta.com/oauth2/ 9 - --oidc-client-id=clientid 10 - --oidc-client-secret=clientsecret 11 - --oidc-extra-scope=profile 12 command: kubectl 13 env: null 14 provideClusterInfo: false Testing Access To test the access to your EKS cluster using Okta authentication, execute any kubectl command. For example, run kubectl get svc to retrieve information about the cluster\u0026rsquo;s services. If the command executes successfully, it means you have authenticated successfully with Okta as the IDP.\nConclusion In this blog post, we have learned how to authenticate to an AWS EKS cluster using Okta as the IDP. By leveraging Okta\u0026rsquo;s secure authentication capabilities and the flexibility of AWS EKS, you can ensure robust access control and streamlined management of your Kubernetes clusters. Implementing Okta as the IDP enhances the security and ease of use for developers and administrators working with your EKS infrastructure.\nNote:\u0026nbsp; Code mentioned above is here\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/aws-eks-okta-auth/","summary":"Introduction Authenticating to AWS EKS (Elastic Kubernetes Service) using AWS IAM (Identity and Access Management) credentials is a crucial aspect of managing and securing these clusters. However, IAM is not the only option when it comes to authenticating with these clusters. You can also configure additional IDP (identity providers) for this purpose. In this post today, I\u0026rsquo;ll explain the steps necessary to authenticate with Okta.\nPre-requisities Before diving into the specifics, please ensure you have the following -","title":"AWS EKS Okta Auth"},{"content":"Introduction Terraform is one of the most popular IAC tools out there. Although, it is quite simple to grasp and use, the complexity rises quickly once you introduce multi-account and multi-region deployments.\nThis post is inspired by one such problem I had to tackle recently.\nProblem How to create and manage multi-account and multi-region AWS resources using Terraform?\nA lot of organizations now seggregate their AWS accounts on the basis of environments (development, test, production etc.). At the same time, due to DR considerations, the production workloads also need to span across regions. This results in a multi-dimensional array of combinations between accounts and regions.\nTo top it all, the cicd tool (most often) runs in a separate dedicated account (say, operator).\nSolution Terraform workspaces to the rescue! Well, almost.\nTerraform workspaces have been available for quite sometime now, and don\u0026rsquo;t need much of an introduction. A small clarification here is in order though. When I mention workspaces, I am referring to the cli workspace and NOT the Terraform cloud workspace.\nWorkspaces make it fairly easy to deploy the same configuration over and over again by simply overriding the variable values. This keeps the configurations minimal, but, at the same time provides enough flexibility to create multiple environments that are isolated from each other.\nIn this particular case, I use the same names for the workspaces as the environments - development and production. The benefit of this will become clear when you look at the configurations and cicd commands.\nAs you already know, to target different AWS accounts and regions, the provider block is what we need to focus on. This is the configuration which dictates which account and region Terraform runs the API commands against.\nSince Terraform does not support dynamic provider blocks (yet!), it is clear a bunch of explicit provider blocks are necessary to interact with different accounts and regions.\nSome optimization is still possible though since the provider block does support specifying variables. Following is an example providers.tf file which enables working with multiple accounts and regions. Explanation follows after this snippet.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 provider \u0026#34;aws\u0026#34; { region = \u0026#34;eu-west-1\u0026#34; } provider \u0026#34;aws\u0026#34; { region = \u0026#34;eu-west-1\u0026#34; alias = \u0026#34;euw1\u0026#34; assume_role { role_arn = var.role_arn[terraform.workspace] } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-1\u0026#34; alias = \u0026#34;usw1\u0026#34; assume_role { role_arn = var.role_arn[terraform.workspace] } } The first provider block is the default one, and is configured partially. AWS credentials are provided for it via environment variables. This is the provider which allows Terraform to use an S3 bucket, and Dynamodb table in the cicd tool account (operator) for state files and locking respectively.\nNext, 2 additional provider blocks are configured, each pointing to a specific region. Notice the alias being set for these providers, as this is necessary to use them later on in the configuration. Also note that the role_arn is not specified in these, rather it is being derived from a variable role_arn of type map(string). This variable is essentially where the magic happens. Depending on the current workspace, it provides the role_arn to Terraform from either the development account or production account. This can be easily extended to include more accounts and workspaces.\nLooking at the auto loading tfvars file below, should make it clearer. Note that these roles are pre-existing, and created as part of the cicd setup.\n1 2 3 4 role_arn = { development = \u0026#34;arn:aws:iam::123456789012:role/TFRole\u0026#34; production = \u0026#34;arn:aws:iam::123456789013:role/TFRole\u0026#34; } With the above configs, we are now ready to create and manage resources across AWS accounts and regions.\nBelow is a dummy file which can be quickly used to test (provided ofcourse, your target accounts/regions have a vpc named MyVPC) the configuration. The alias I mentioned above comes in handy here when declaring the data block. You can also use alias for resources and modules. More details here\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 variable \u0026#34;role_arn\u0026#34; { description = \u0026#34;Map of role_arn to use in each account\u0026#34; } data \u0026#34;aws_vpc\u0026#34; \u0026#34;eu\u0026#34; { provider = aws.euw1 filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;MyVPC\u0026#34;] } } data \u0026#34;aws_vpc\u0026#34; \u0026#34;us\u0026#34; { provider = aws.usw1 filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;MyVPC\u0026#34;] } } output \u0026#34;eu\u0026#34; { value = data.aws_vpc.eu.arn } output \u0026#34;us\u0026#34; { value = data.aws_vpc.us.arn } Finally, in the cicd pipeline, it\u0026rsquo;s very simple to target the specific environment using workspaces. The following few cli commands should do it. I only show this for the development environment, but I am sure, you get the drift!\n1 2 3 4 5 6 7 $ export WORKSPACE=development $ terraform select workspace $WORKSPACE || terraform new workspace $WORKSPACE # This commands loads an additional variable file named development.tfvars # Any environment specific customization can be put into this file $ terraform plan -input=false -var-file $WORKSPACE.tfvars $ terraform apply -input=false -auto-approve -var-file $WORKSPACE.tfvars The visualization of the above setup would probably look something like the diagram included at the begining of this post. Have a look at it now, it would probably make more sense.\nConclusion This is a simple pattern to manage resources with Terraform across AWS accounts and regions. It can be extended further to cater to environment specific customizations etc. This should atleast give you some ideas and help in getting started.\nIf you use a different simpler pattern, I would love to hear more about it. Please feel free to leave a link in the comments below.\nReferences (1) Configuration#alias Multiple Provider Configurations\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/terraform-multiaccount-multiregion/","summary":"Introduction Terraform is one of the most popular IAC tools out there. Although, it is quite simple to grasp and use, the complexity rises quickly once you introduce multi-account and multi-region deployments.\nThis post is inspired by one such problem I had to tackle recently.\nProblem How to create and manage multi-account and multi-region AWS resources using Terraform?\nA lot of organizations now seggregate their AWS accounts on the basis of environments (development, test, production etc.","title":"Terraform, multi-account and multi-region workloads"},{"content":"Introduction Bitbucket Pipelines is often the ci-cd tool of choice for organisations which are already entrenched into the Atlassian suite of products. No one is thrilled at the prospect, but hey, you got to play the hand you have been dealt. So, here we go.\nProblem One of the key problems with establishing a CI/CD infrastructure flow is that of access control. Specifically for Bitbucket Pipelines, in the past you would need to provide access via repository variables. These would include a access_key and secret_key which would grant it the necessary permission to do the changes as part of the Terraform run.\nSolution Bitbucket Pipelines now support a OIDC role based access flow for AWS, so you don\u0026rsquo;t need to manage a robot user and it\u0026rsquo;s key rotation. In this post, I\u0026rsquo;ll desribe how I went about enabling this flow in our project.\nStep 1 - Setting up the basics in AWS First up, you need to create the basics for use with Bitbucket Pipeline. I use a Cloudformation template to deploy the resources necessary for terraform run like S3 and Dynamodb. To this template we will add the required OIDC provider and an IAM role as well.\nI use the following template, you can modify it suitably for any further customizations necessary\n1AWSTemplateFormatVersion: 2010-09-09 2Description: Basic resources for Terraform 3Resources: 4 TFBucket: 5 Type: AWS::S3::Bucket 6 Properties: 7 AccessControl: Private 8 BucketEncryption: 9 ServerSideEncryptionConfiguration: 10 - ServerSideEncryptionByDefault: 11 SSEAlgorithm: AES256 12 PublicAccessBlockConfiguration: 13 BlockPublicAcls: true 14 BlockPublicPolicy: true 15 IgnorePublicAcls: true 16 RestrictPublicBuckets: true 17 VersioningConfiguration: 18 Status: Enabled 19 Tags: 20 - Key: Purpose 21 Value: \u0026#34;Terraform state file remote storage\u0026#34; 22 TFDynamoDBTable: 23 Type: AWS::DynamoDB::Table 24 Properties: 25 BillingMode: PAY_PER_REQUEST 26 SSESpecification: 27 SSEEnabled: true 28 TableName: terraform-remote-state 29 AttributeDefinitions: 30 - AttributeName: LockID 31 AttributeType: S 32 KeySchema: 33 - AttributeName: LockID 34 KeyType: HASH 35 Tags: 36 - Key: Purpose 37 Value: \u0026#34;Terraform state file remote storage\u0026#34; 38 BBOidc: 39 Type: AWS::IAM::OIDCProvider 40 Properties: 41 ClientIdList: 42 - \u0026#39;AUDIENCE\u0026#39; 43 Tags: 44 - Key: Purpose 45 Value: \u0026#34;Bitbucket pipelines to assume IAM role\u0026#34; 46 ThumbprintList: 47 - \u0026#39;THUMBPRINT\u0026#39; 48 Url: \u0026#39;IDENTITY PROVIDER URL\u0026#39; 49 TFRole: 50 Type: AWS::IAM::Role 51 Properties: 52 AssumeRolePolicyDocument: 53 Version: \u0026#34;2012-10-17\u0026#34; 54 Statement: 55 - Effect: Allow 56 Principal: 57 Federated: 58 - !Ref BBOidc 59 Action: 60 - \u0026#39;sts:AssumeRoleWithWebIdentity\u0026#39; 61 62 RoleName: terraform-iam-role 63 Tags: 64 - Key: Purpose 65 Value: \u0026#34;Access for Bitbucket Pipelines\u0026#34; 66 TFPolicy: 67 Type: AWS::IAM::Policy 68 Properties: 69 PolicyName: terraform-base-policy 70 Roles: 71 - !Ref TFRole 72 PolicyDocument: 73 Version: \u0026#39;2012-10-17\u0026#39; 74 Statement: 75 - Effect: Allow 76 Action: 77 - \u0026#39;s3:ListBucket\u0026#39; 78 - \u0026#39;s3:GetObject\u0026#39; 79 - \u0026#39;s3:PutObject\u0026#39; 80 - \u0026#39;s3:PutObjectAcl\u0026#39; 81 Resource: 82 - !Sub \u0026#39;arn:aws:s3:::${TFBucket}\u0026#39; 83 - !Sub \u0026#39;arn:aws:s3:::${TFBucket}/*\u0026#39; 84 - Effect: Allow 85 Action: 86 - \u0026#39;dynamodb:GetItem\u0026#39; 87 - \u0026#39;dynamodb:PutItem\u0026#39; 88 - \u0026#39;dynamodb:DeleteItem\u0026#39; 89 Resource: 90 - !Sub \u0026#39;arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/${TFDynamoDBTable}\u0026#39; 91 CBPolicy: 92 Type: AWS::IAM::Policy 93 Properties: 94 PolicyName: codebuild-access-policy 95 Roles: 96 - !Ref TFRole 97 PolicyDocument: 98 Version: \u0026#39;2012-10-17\u0026#39; 99 Statement: 100 - Effect: Allow 101 Action: 102 - \u0026#39;codebuild:CreateProject\u0026#39; 103 - \u0026#39;codebuild:DeleteProject\u0026#39; 104 - \u0026#39;codebuild:UpdateProject\u0026#39; 105 - \u0026#39;codebuild:CreateWebhook\u0026#39; 106 - \u0026#39;codebuild:DeleteWebhook\u0026#39; 107 - \u0026#39;codebuild:UpdateWebhook\u0026#39; 108 - \u0026#39;codebuild:BatchGetProjects\u0026#39; 109 Resource: 110 - !Sub \u0026#39;arn:aws:codebuild:${AWS::Region}:${AWS::AccountId}:project/*\u0026#39; 111 - Effect: Allow 112 Action: 113 - \u0026#39;codebuild:ImportSourceCredentials\u0026#39; 114 - \u0026#39;codebuild:DeleteSourceCredentials\u0026#39; 115 - \u0026#39;codebuild:ListProjects\u0026#39; 116 - \u0026#39;codebuild:ListCuratedEnvironmentImages\u0026#39; 117 Resource: \u0026#39;*\u0026#39; 118 IAMPolicy: 119 Type: AWS::IAM::Policy 120 Properties: 121 PolicyName: iam-access-policy 122 Roles: 123 - !Ref TFRole 124 PolicyDocument: 125 Version: \u0026#39;2012-10-17\u0026#39; 126 Statement: 127 - Effect: Allow 128 Action: 129 - \u0026#39;iam:List*\u0026#39; 130 - \u0026#39;iam:PassRole\u0026#39; 131 - \u0026#39;iam:GetRole*\u0026#39; 132 Resource: 133 - !Sub \u0026#39;arn:aws:iam::${AWS::AccountId}:role/*\u0026#39; 134 S3Policy: 135 Type: AWS::IAM::Policy 136 Properties: 137 PolicyName: s3-access-policy 138 Roles: 139 - !Ref TFRole 140 PolicyDocument: 141 Version: \u0026#39;2012-10-17\u0026#39; 142 Statement: 143 - Effect: Allow 144 Action: 145 - \u0026#39;s3:List*\u0026#39; 146 - \u0026#39;s3:GetObject*\u0026#39; 147 - \u0026#39;s3:PutObject\u0026#39; 148 - \u0026#39;s3:PutObjectAcl\u0026#39; 149 Resource: 150 - \u0026#39;arn:aws:s3:::bucket\u0026#39; 151 - \u0026#39;arn:aws:s3:::bucket/*\u0026#39; As you can see, the template creates -\nAn S3 bucket which will be used to save the terraform state A Dynamodb table for terraform state locking An OIDC provider for Bitbucket pipelines to use IAM Role and policies to provide necessary access to Bitbucket pipelines The policies CBPolicy, IAMPolicy, S3Policy provide appropriate permissions to create and manage a codebuild project. These will ofcourse vary, depending on what you plan to manage with Terraform. For the BBOidc resource, replace the values AUDIENCE, IDENTITY PROVIDER URL and THUMBPRINT. These can be obtained from Bitbucket under Repository settings. Details are here Once this template is deployed, we have the necessary basics setup to start setting up the pipeline on Bitbucket. Bear in mind though, this is more of an academic exercise. If you plan to roll this out in production, you should probably look at securing this role further. By defining conditions in the role\u0026rsquo;s IAM trust policy to tie down the scope to a particular repo or workspace, as well as possibly restricting it by IPs.\nStep 2 - Terraform Configs I will just share the provider block to keep it short, you can build up on this to create any necessary AWS resources. Be sure to modify the Cloudformation stack above for the proper permissions.\n1# Backend config for remote_state 2terraform { 3 backend \u0026#34;s3\u0026#34; { 4 bucket = \u0026#34;BUCKET_FROM_CFTEMPLATE\u0026#34; 5 key = \u0026#34;DYNAMODB_KEY\u0026#34; 6 region = \u0026#34;REPLACE_WITH_REGION_TO_USE\u0026#34; 7 encrypt = true 8 dynamodb_table = \u0026#34;DYNAMODB_FROM_CFTEMPLATE\u0026#34; 9 } 10 required_providers { 11 aws = { 12 source = \u0026#34;hashicorp/aws\u0026#34; 13 version = \u0026#34;~\u0026gt; 3.0\u0026#34; 14 } 15 } 16} 17# Configure aws provider 18provider \u0026#34;aws\u0026#34; { 19 region = \u0026#34;REPLACE_WITH_REGION_TO_USE\u0026#34; 20} You will need to replace BUCKET_FROM_CFTEMPLATE, DYNAMODB_FROM_CFTEMPLATE, DYNAMODB_KEY, REPLACE_WITH_REGION_TO_USE. The Dynamodb and S3 bucket are created in Step 1, just copy over the values from there.\nStep 3 - Configure Bitbucket Pipeline Right, now to the heart of the matter - configuring Bitbucket pipeline. It is fairly simple, you just need to add a file bibucket-pipelines.yml in the same Bitbucket repo as your terraform code. The one I used is included below.\n1image: hashicorp/terraform:1.0.7 2definitions: 3 scripts: 4 - script: \u0026amp;aws-context 5 export AWS_REGION=REPLACE_WITH_REGION_TO_USE; 6 export AWS_WEB_IDENTITY_TOKEN_FILE=$(pwd)/web-identity-token; 7 export AWS_ROLE_SESSION_NAME=build-session; 8 export AWS_ROLE_ARN=REPLACE_WITH_ROLE_ARN_TO_USE; 9 echo $BITBUCKET_STEP_OIDC_TOKEN \u0026gt; $(pwd)/web-identity-token 10 steps: 11 - step: \u0026amp;validate 12 name: Validate Terraform config 13 oidc: true 14 script: 15 - terraform init -backend=false 16 - terraform validate 17 - step: \u0026amp;plan 18 name: Terraform Plan 19 oidc: true 20 script: 21 - *aws-context 22 - terraform init 23 - terraform plan -input=false -out=tfplan.out 24 artifacts: 25 - tfplan.out 26 - step: \u0026amp;apply 27 name: Terraform Apply 28 oidc: true 29 trigger: manual 30 script: 31 - *aws-context 32 - terraform init 33 - terraform apply -input=false -auto-approve tfplan.out 34pipelines: 35 branches: 36 main: 37 - step: *validate 38 - step: *plan 39 - step: *apply 40 default: 41 - step: *validate There is a lot going on in there, so I\u0026rsquo;ll try to break it down a bit.\nFirst up are the definitions - Resources here can be resued in your pipeline using yaml anchor. So this is a good place to declare stuff you know you will be calling often. In this case, I have declared a script which sets up the environment variables necessary for the OIDC role flow to work. Yes, you need to attach this to EVERY step which needs access to AWS via the OIDC role.\nSecond, notice the flag oidc: true that is applied to all the steps that need access to AWS resources. This possibly tells Bitbucket to kick off some magic in the backend and populate all the environment variables we set up above.\nThird, I use artifacts to pass on the plan output from one step to the next. This is resued when you run the apply step.\nFourth, I have a flag trigger: manual on the terraform apply step. This ensures the apply step is only triggered after a careful review of the plan step above. But you can always remove it to trigger the apply as soon as plan succeeds.\nAnd that\u0026rsquo;s it really! Now everytime you commit a change to the repo, you should see a pipeline run triggered.\nConclusion I hope this post proves useful to setup a OIDC role based access control for Bitbucket pipelines. There are a plethora of customization options available for them including running the builds on self hosted runners. So feel free to go through their docs to find out more, I will include the useful links below.\nReferences (3) Configure Your Pipeline\u0026nbsp; Integrate Pipelines With Resource Servers Using Oidc\u0026nbsp; Deploy on Aws Using Bitbucket Pipelines Openid Connect\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/terraform-bitbucket-pipeline-oidc/","summary":"Introduction Bitbucket Pipelines is often the ci-cd tool of choice for organisations which are already entrenched into the Atlassian suite of products. No one is thrilled at the prospect, but hey, you got to play the hand you have been dealt. So, here we go.\nProblem One of the key problems with establishing a CI/CD infrastructure flow is that of access control. Specifically for Bitbucket Pipelines, in the past you would need to provide access via repository variables.","title":"Terraform, Bitbucket pipelines and OIDC"},{"content":"Introduction Prometheus Blackbox Exporter is a popular plugin to test http endpoints. It offers a range of configuration options which can be tweaked to suit any use case. I recently had to implement monitoring for some APIs, each expecting a different content body.\nProblem By design, Prometheus blackbox exporter is not expected to work as a \u0026ldquo;proxy\u0026rdquo;, so you cannot pass the body of a request via relabelling in Prometheus configs.\nLet\u0026rsquo;s say you want to monitor 2 APIs each of which expect you to provide a different payload. For this academic exercise, we can use https://httpbin.org/post as the API endpoint.\nSolution On a high level then, this needs 2 different steps -\nAdd a new module in blackbox exporter configs Add a new job in prometheus config to use the module created under 1. Step 1 - Blackbox Exporter The blackbox.yaml configuration file with 2 new modules for each post request. Notice we also use the property fail_if_body_not_matches_regexp, which will fail our test if the response doesn\u0026rsquo;t have the text we expect.\n1modules: 2 http_2xx: 3 prober: http 4 timeout: 5s 5 http: 6 valid_http_versions: [\u0026#34;HTTP/1.1\u0026#34;, \u0026#34;HTTP/2.0\u0026#34;] 7 valid_status_codes: [] 8 method: GET 9 preferred_ip_protocol: \u0026#34;ip4\u0026#34; 10 ip_protocol_fallback: false 11 post_one_2xx: 12 prober: http 13 timeout: 5s 14 http: 15 valid_http_versions: [\u0026#34;HTTP/1.1\u0026#34;, \u0026#34;HTTP/2.0\u0026#34;] 16 valid_status_codes: [] 17 method: POST 18 headers: 19 content-type: application/json 20 body: \u0026#39;{\u0026#34;data\u0026#34;: \u0026#34;one\u0026#34;}\u0026#39; 21 preferred_ip_protocol: \u0026#34;ip4\u0026#34; 22 ip_protocol_fallback: false 23 fail_if_body_not_matches_regexp: [\u0026#34;.*one.*\u0026#34;] 24 post_two_2xx: 25 prober: http 26 timeout: 5s 27 http: 28 valid_http_versions: [\u0026#34;HTTP/1.1\u0026#34;, \u0026#34;HTTP/2.0\u0026#34;] 29 valid_status_codes: [] 30 method: POST 31 headers: 32 content-type: application/json 33 body: \u0026#39;{\u0026#34;data\u0026#34;: \u0026#34;two\u0026#34;}\u0026#39; 34 preferred_ip_protocol: \u0026#34;ip4\u0026#34; 35 ip_protocol_fallback: false 36 fail_if_body_not_matches_regexp: [\u0026#34;.*two.*\u0026#34;] Let\u0026rsquo;s run a blackbox exporter container with the above config and see if this works.\n1$ docker run --rm -d -p 9115:9115 --name blackbox_exporter -v `pwd`:/config prom/blackbox-exporter:master --config.file=/config/blackbox.yaml Blackbox exporter should now be available on http://localhost:9115/. The endpoint of interest to us is http://localhost:9115/probe. Hit the probe endpoint either through curl or on the browser\n1$ curl -s http://localhost:9115/probe?target=https://httpbin.org/post\u0026amp;module=post_one_2xx You should get a page with quite a few metrics listed. Look for the probe_failed_due_to_regex 0 and probe_http_status_code 200 metrics, and if they are there, it means our config worked! \u0026#x1f44f;\nThe probe hit https://httpbin.org/post with the data as defined in post_one_2xx module, AND matched the regex as defined under the module property fail_if_body_not_matches_regexp.\nIf you were to switch, or change the regex pattern, so that the response body would not match it, probe_failed_due_to_regex 1 will be returned instead.\nNow that our blackbox exporter config is ready, let\u0026rsquo;s add the necessary config on Prometheus.\nStep 2 The prometheus.yaml configuration file which points to the blackbox exporter we have running from Step 1.\n1global: 2 scrape_interval: 1m 3scrape_configs: 4 - job_name: blackbox 5 metrics_path: /metrics 6 static_configs: 7 - targets: 8 - host.docker.internal:9115 9 - job_name: blackbox-http 10 metrics_path: /probe 11 scrape_interval: 5m 12 params: 13 module: [http_2xx] 14 static_configs: 15 - targets: 16 - https://httpbin.org 17 relabel_configs: 18 - source_labels: [__address__] 19 target_label: __param_target 20 - source_labels: [__param_target] 21 target_label: instance 22 - target_label: __address__ 23 replacement: host.docker.internal:9115 24 - job_name: blackbox-http-post-one 25 metrics_path: /probe 26 scrape_interval: 5m 27 params: 28 module: [post_one_2xx] 29 static_configs: 30 - targets: 31 - https://httpbin.org/post 32 relabel_configs: 33 - source_labels: [__address__] 34 target_label: __param_target 35 - source_labels: [__param_target] 36 target_label: instance 37 - target_label: __address__ 38 replacement: host.docker.internal:9115 39 - job_name: blackbox-http-post-two 40 metrics_path: /probe 41 scrape_interval: 5m 42 params: 43 module: [post_two_2xx] 44 static_configs: 45 - targets: 46 - https://httpbin.org/post 47 relabel_configs: 48 - source_labels: [__address__] 49 target_label: __param_target 50 - source_labels: [__param_target] 51 target_label: instance 52 - target_label: __address__ 53 replacement: host.docker.internal:9115 Let\u0026rsquo;s run a prometheus container with the above config and see all of this in action.\n1$ docker run --rm -d -p 9090:9090 --name prometheus -v `pwd`:/config prom/prometheus:latest --config.file=/config/prometheus.yaml Prometheus should now be available on http://localhost:9090/. Navigate to the targets page, to see it has picked up our blackbox exporter targets.\nOnce you have this integration going and Prometheus is scraping the Blackbox exporter, it is quite trivial to add more endpoints. Except ofcourse, when you want to monitor another API POST call. In that case, you need to start at step 1 again. Groundhog Day, eh? \u0026#x1f643;\nConclusion That\u0026rsquo;s it!\nThis was a short post about using Prometheus Blackbox exporter to monitor different APIs.\nHope this post proves useful. \u0026#x1f44d;\nReferences (2) Blackbox Exporter\u0026nbsp; Multi Target Exporter\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/prometheus-blackbox-monitor-post-api/","summary":"Introduction Prometheus Blackbox Exporter is a popular plugin to test http endpoints. It offers a range of configuration options which can be tweaked to suit any use case. I recently had to implement monitoring for some APIs, each expecting a different content body.\nProblem By design, Prometheus blackbox exporter is not expected to work as a \u0026ldquo;proxy\u0026rdquo;, so you cannot pass the body of a request via relabelling in Prometheus configs.","title":"Prometheus Blackbox Exporter and POST calls"},{"content":"Introduction S3 is one of the core services offered by AWS, and more often than not, fulfills a critical storage need in most architectures. Over time, AWS has made it incredibly easy to hook it up with other services, further increasing its adoption. As the Google trends below indicate, S3 is possibly THE most widely used AWS service, even edging out the likes of Lambda, EC2, IAM etc. It may not be a 100% accurate inference, but I am sure, directionally, it is not too far off.\nWith such huge adoption, security becomes a critical issue. AWS provides a number of tools to help reduce bucket misconfigurations. But, they are still rampant, and cause data breaches quite often. You can read about some recent examples here and here. For more, take a look at this neat list, going back several years.\nIf anything, these breaches show us, how crucial it is to safeguard the S3 bucket. There are a lot of strategies to do this, like bucket policies, IAM policies, encryption, config rules etc. Refer to this best practice guide published by AWS, to get more details.\nLet\u0026rsquo;s now look at one approach, which can help keep S3 buckets secure. It still allows specific actions (read/write) on specific data, but for a limited time.\nProblem How to provide temporary access to your bucket?\nMaybe, you need to temporarily share a file in your private S3 bucket with a third party, which doesn\u0026rsquo;t have any AWS access.\nOr, you need to temporarily allow access to the third part to upload a file to your private S3 bucket.\nThese cases can easily be extended to cover similar use cases like apps trying to download from or upload to S3.\nSolution For providing temporary access to S3 buckets, you can use what AWS calls Presigned URLs. These are essentially temporary urls which you can share with anyone who needs access to your bucket/object. Using these temporary urls, the recepient can then complete the desired action (read/write) in the stipulated time. Remember, since these urls are tied to the particular object and are temporary, (expiring after a set duration) there is low risk of exploits using them.\nHow to create a presigned url? A presigned url for an object in a bucket can only be created by an IAM entity which has access to it. So, somebody with only a read access on your bucket will not be able to create a presigned url to allow an upload. This is important to mention, since, if you are creating these urls programatically in an app, the role assigned to this app must allow the same action.\nAWS SDK (Go/Python/JS etc.) provides methods to generate these presigned urls. You can even use AWS CLI to generate a presigned url, if you want to give read-only (download) access.\nUsing Python Boto3 Note: this is a basic example with the bare minimum steps\nLet\u0026rsquo;s create a presigned upload url using Python boto3 generate_presigned_post method, which will be valid for the 10 mins.\n1import boto3 2 3s3_client = boto3.client(\u0026#39;s3\u0026#39;, region_name=\u0026#39;us-east-1\u0026#39;) 4resp = s3_client.generate_presigned_post(\u0026#34;mybucket\u0026#34;, \u0026#34;path/to/file.txt\u0026#34;, ExpiresIn=600) 5print(resp) Similarly, to create a presigned url to download a file, the appropriate boto3 method to use would be generate_presigned_url.\nOnce you execute the above code, you will receive a json response as follows\n1{ 2 \u0026#34;url\u0026#34;: \u0026#34;https://mybucket.s3.amazonaws.com/\u0026#34;, 3 \u0026#34;fields\u0026#34;: { 4 \u0026#34;key\u0026#34;: \u0026#34;path/to/file.txt\u0026#34;, 5 \u0026#34;AWSAccessKeyId\u0026#34;: \u0026#34;access_key\u0026#34;, 6 \u0026#34;x-amz-security-token\u0026#34;: \u0026#34;security_token\u0026#34;, 7 \u0026#34;policy\u0026#34;: \u0026#34;base64_encoded_policy\u0026#34;, 8 \u0026#34;signature\u0026#34;: \u0026#34;signature\u0026#34; 9 } 10} Now, you can share this json with the third party to allow them to upload this file to your bucket. They will not be able to do anything on any other object in the bucket or even any other action on the same object.\nValidating the presigned URL The simplest way to validate this url is to upload a file by the same name as specified under \u0026ldquo;key\u0026rdquo; above. Let\u0026rsquo;s do that using curl.\nFor uploading a file, use the form fields with the -F flag. The -v flag will allow you to inspect the response headers Ref. Include all the fields as received in the json above. Remember to include the file only at the end, otherwise you may receive an error.\nThe final curl command will look like this (assuming the file to upload file.txt exists in the local working directory) -\n1$ curl -v \u0026#34;https://mybucket.s3.amazonaws.com/\u0026#34; \\ 2-F \u0026#34;key=path/to/file.txt\u0026#34; \\ 3-F \u0026#34;AWSAccessKeyId=access_key\u0026#34; \\ 4-F \u0026#34;x-amz-security-token=security_token\u0026#34; \\ 5-F \u0026#34;policy=base64_encoded_policy\u0026#34; \\ 6-F \u0026#34;signature=signature\u0026#34; \\ 7-F \u0026#34;file=@file.txt\u0026#34; The response to this call will NOT be 200 OK. You will instead get a 204 Content not modified response. Nothing to worry though, as this means the transfer has been successful. Check your bucket to verify the file is now available.\nConclusion Presigned URLs are the recommended way to grant temporary access to your bucket and objects. It is a great way to offload the overheads associated with file upload/download to Amazon. This is an incredibly useful design pattern for serverless applications as they can simply pass on the presigned url to the client (whether it\u0026rsquo;s a mobile app or web app), and then clients deal with S3 directly to upload/download the file.\nIt can also come in handy while working with people outside your organisation (maybe vendors), to share information/data which you cannot send/share by other means.\nHopefully, this quick walkthrough was helpful in explaining the usefulness of presigned urls. I have also linked some useful pages on the topic below.\nReferences (4) Using Presigned Url\u0026nbsp; S3 Presigned Urls\u0026nbsp; S3#S3.Client.generate Presigned Post\u0026nbsp; S3#S3.Client.generate Presigned Url\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/s3-presigned-urls/","summary":"Introduction S3 is one of the core services offered by AWS, and more often than not, fulfills a critical storage need in most architectures. Over time, AWS has made it incredibly easy to hook it up with other services, further increasing its adoption. As the Google trends below indicate, S3 is possibly THE most widely used AWS service, even edging out the likes of Lambda, EC2, IAM etc. It may not be a 100% accurate inference, but I am sure, directionally, it is not too far off.","title":"S3 Presigned URLs"},{"content":"Introduction Fastly is a popular CDN based on the open-source Varnish. Since it supports VCL, a lot of custom \u0026ldquo;logic\u0026rdquo; to handle incoming requests can be added, right at the edge. It also provides a host of geolocation identification options.\nIn this post, we\u0026rsquo;ll build on the service we created in a previous post Fastly Meta Service to implement a simple API which returns details about a particular public IP.\nThis service can be nice value add, whether for use by machines (json output helps!) or by non-technical folks to gather useful info tech teams often need to solve/debug \u0026ldquo;Does not work from my home. I get an error, please fix!\u0026rdquo; issues.\nSolution The service will rely entirely on Fastly\u0026rsquo;s infrastructure and will not interact with the configured origin at all! Sadly, you cannot launch a service without any origin at this point, so you would need to add some dummy host. httpbin.org maybe a good choice here!\nSuch an isolated request flow is possible, thanks to the synthetic responses that can be served directly from Fastly using the vcl constructs.\nThe request flow will look like this (no requests are sent to the origin) - Geolocation Database Fastly uses 2 vendors to fetch the geolocation details - Maxmind and Digital Element. This Fastly docs page lists out the different variables that are available for use. Broadly these variables are available under 2 distinct namespaces, each provided by a different vendor. client.geo.* namespace variables are provided by Digital Element, while geoip.* namespace variables are provided by Maxmind.\nNote that Fastly has deprecated Maxmind, so any future implementations should only be using the client.geo.* variables as outlined in this post.\nAPI Endpoints We will build the following endpoints\nRequest\nGET / Response\n1{ 2 \u0026#34;date\u0026#34;: \u0026#34;Timestamp in GMT\u0026#34;, 3 \u0026#34;ipv6\u0026#34;: \u0026#34;true|false\u0026#34;, 4 \u0026#34;public_ip\u0026#34;: \u0026#34;public ip\u0026#34;, 5 \u0026#34;de_city\u0026#34;: \u0026#34;city from DE database\u0026#34;, 6 \u0026#34;de_country\u0026#34;: \u0026#34;country from DE database\u0026#34;, 7 \u0026#34;de_continent\u0026#34;: \u0026#34;continent from DE database\u0026#34;, 8 \u0026#34;de_proxy\u0026#34; : \u0026#34;proxy type and description from DE database\u0026#34;, 9 \u0026#34;mx_city\u0026#34;: \u0026#34;city from Maxmind database\u0026#34;, 10 \u0026#34;mx_country\u0026#34;: \u0026#34;city from Maxmind database\u0026#34;, 11 \u0026#34;mx_continent\u0026#34;: \u0026#34;countinent from Maxmind database\u0026#34; 12} Request\nGET /?ip=ip_v4_add or ip_v6_add Response\n1{ 2 \u0026#34;date\u0026#34;: \u0026#34;Timestamp in GMT\u0026#34;, 3 \u0026#34;ipv6\u0026#34;: \u0026#34;true|false\u0026#34;, 4 \u0026#34;public_ip\u0026#34;: \u0026#34;provided ip_v4_add or ip_v6_add\u0026#34;, 5 \u0026#34;de_city\u0026#34;: \u0026#34;city from DE database\u0026#34;, 6 \u0026#34;de_country\u0026#34;: \u0026#34;country from DE database\u0026#34;, 7 \u0026#34;de_continent\u0026#34;: \u0026#34;continent from DE database\u0026#34;, 8 \u0026#34;de_proxy\u0026#34; : \u0026#34;proxy type and description from DE database\u0026#34;, 9 \u0026#34;mx_city\u0026#34;: \u0026#34;city from Maxmind database\u0026#34;, 10 \u0026#34;mx_country\u0026#34;: \u0026#34;city from Maxmind database\u0026#34;, 11 \u0026#34;mx_continent\u0026#34;: \u0026#34;countinent from Maxmind database\u0026#34; 12} Request\nGET /random Response\n301 Redirect to / VCL Snippets We will modify the vcl snippets recv and error created previously to extend the service and provide more details.\nUpdate the recv snippet with this code -\n1if (!req.url.path ~ \u0026#34;^/$\u0026#34; || !req.http.Fastly-SSL){ 2 error 618; 3} 4 5set req.url = querystring.filter_except(req.url, \u0026#34;ip\u0026#34;); 6 7if (fastly.ff.visits_this_service == 0 \u0026amp;\u0026amp; req.restarts == 0) { 8 set req.http.Fastly-Client-IP = client.ip; 9 set req.http.ipv6 = \u0026#34;false\u0026#34;; 10 if (req.is_ipv6){ 11 set req.http.ipv6 = \u0026#34;true\u0026#34;; 12 } 13} 14 15if (req.url.qs ~ \u0026#34;ip\u0026#34;){ 16 set req.http.Fastly-Client-IP = querystring.get(req.url, \u0026#34;ip\u0026#34;); 17 set client.geo.ip_override = req.http.Fastly-Client-IP; 18 set geoip.ip_override = req.http.Fastly-Client-IP; 19} 20 21error 620; Update the error snippet with this code -\n1if (obj.status == 618) { 2 set obj.status = 301; 3 set obj.response = \u0026#34;Moved Permanently\u0026#34;; 4 set obj.http.Location = \u0026#34;https://\u0026#34; req.http.host \u0026#34;/\u0026#34;; 5 set obj.http.cache-control = \u0026#34;private, no-store, no-cache, max-age=0\u0026#34;; 6 return (deliver); 7} 8 9if (obj.status == 620) { 10 set obj.status = 200; 11 set obj.response = \u0026#34;OK\u0026#34;; 12 set obj.http.Content-Type = \u0026#34;application/json\u0026#34;; 13 set obj.http.cache-control = \u0026#34;private, no-store, no-cache, max-age=0\u0026#34;; 14 synthetic 15{\u0026#34;{ 16 \u0026#34;date\u0026#34;: \u0026#34;\u0026#34;} now {\u0026#34;\u0026#34;, 17 \u0026#34;ipv6\u0026#34;: \u0026#34;\u0026#34;} req.http.ipv6 {\u0026#34;\u0026#34;, 18 \u0026#34;public_ip\u0026#34;: \u0026#34;\u0026#34;} req.http.Fastly-Client-IP {\u0026#34;\u0026#34;, 19 \u0026#34;de_city\u0026#34;: \u0026#34;\u0026#34;} client.geo.city {\u0026#34;\u0026#34;, 20 \u0026#34;de_country\u0026#34;: \u0026#34;\u0026#34;} client.geo.country_code {\u0026#34;\u0026#34;, 21 \u0026#34;de_continent\u0026#34;: \u0026#34;\u0026#34;} client.geo.continent_code {\u0026#34;\u0026#34;, 22 \u0026#34;de_proxy\u0026#34; : \u0026#34;\u0026#34;} client.geo.proxy_type \u0026#34; - \u0026#34; client.geo.proxy_description {\u0026#34;\u0026#34;, 23 \u0026#34;mx_city\u0026#34;: \u0026#34;\u0026#34;} geoip.city {\u0026#34;\u0026#34;, 24 \u0026#34;mx_country\u0026#34;: \u0026#34;\u0026#34;} geoip.country_code {\u0026#34;\u0026#34;, 25 \u0026#34;mx_continent\u0026#34;: \u0026#34;\u0026#34;} geoip.continent_code {\u0026#34;\u0026#34; 26}\u0026#34;}; 27 return (deliver); 28} You can see/test this flow in a fiddle here\nSnippets also committed to the repo\nDeployment The service, configurations and vcls can be deployed to Fastly using Terraform. The required Terraform code is similar to what we used in the last post and is available here\nOutputs Once the service is ready, a simple request via curl or browser will return the details like below. I am using the test domain, hence need to use the -k flag (for more uses of curl, you can check this post). For a production level deployment, you would obviously map a custom domain to the service and also use SSL (Fastly provided or self procured).\nLet\u0026rsquo;s check a few curl responses.\nProton Public IP If you are connected to the ProtonVPN free NL server, this is what you should see. Note the difference of output between the Maxmind and DE outputs. Maxmind says it\u0026rsquo;s a US IP!\n1$ curl -k https://meta.dane-example.com.global.prod.fastly.net/ 2 3{ 4 \u0026#34;date\u0026#34;: \u0026#34;Fri, 05 Mar 2021 16:23:53 GMT\u0026#34;, 5 \u0026#34;ipv6\u0026#34;: \u0026#34;false\u0026#34;, 6 \u0026#34;public_ip\u0026#34;: \u0026#34;34.66.60.237\u0026#34;, 7 \u0026#34;de_city\u0026#34;: \u0026#34;HUISSEN\u0026#34;, 8 \u0026#34;de_country\u0026#34;: \u0026#34;NL\u0026#34;, 9 \u0026#34;de_continent\u0026#34;: \u0026#34;EU\u0026#34;, 10 \u0026#34;de_proxy\u0026#34; : \u0026#34;HOSTING - VPN\u0026#34;, 11 \u0026#34;mx_city\u0026#34;: \u0026#34;COUNCIL BLUFFS\u0026#34;, 12 \u0026#34;mx_country\u0026#34;: \u0026#34;US\u0026#34;, 13 \u0026#34;mx_continent\u0026#34;: \u0026#34;NA\u0026#34; 14} Google Public IP Let\u0026rsquo;s inspect a public IP, maybe the Google DNS? This time both vendors show the correct country, but Maxmind is off the mark on city.\n1$ curl -k https://meta.dane-example.com.global.prod.fastly.net/?ip=8.8.8.8 2 3{ 4 \u0026#34;date\u0026#34;: \u0026#34;Fri, 05 Mar 2021 16:23:53 GMT\u0026#34;, 5 \u0026#34;ipv6\u0026#34;: \u0026#34;false\u0026#34;, 6 \u0026#34;public_ip\u0026#34;: \u0026#34;8.8.8.8\u0026#34;, 7 \u0026#34;de_city\u0026#34;: \u0026#34;MOUNTAIN VIEW\u0026#34;, 8 \u0026#34;de_country\u0026#34;: \u0026#34;US\u0026#34;, 9 \u0026#34;de_continent\u0026#34;: \u0026#34;NA\u0026#34;, 10 \u0026#34;de_proxy\u0026#34; : \u0026#34;ANONYMOUS - ?\u0026#34;, 11 \u0026#34;mx_city\u0026#34;: \u0026#34;NEW YORK\u0026#34;, 12 \u0026#34;mx_country\u0026#34;: \u0026#34;US\u0026#34;, 13 \u0026#34;mx_continent\u0026#34;: \u0026#34;NA\u0026#34; 14} Public IPv6 Let\u0026rsquo;s try the IPv6 address now, if that works? Google DNS again. Maxmind doesn\u0026rsquo;t have any details about IPv6, DE does.\n1$ curl -k https://meta.dane-example.com.global.prod.fastly.net/?ip=2001:4860:4860::8888 2 3{ 4 \u0026#34;date\u0026#34;: \u0026#34;Fri, 05 Mar 2021 16:23:53 GMT\u0026#34;, 5 \u0026#34;ipv6\u0026#34;: \u0026#34;false\u0026#34;, 6 \u0026#34;public_ip\u0026#34;: \u0026#34;2001:4860:4860::8888\u0026#34;, 7 \u0026#34;de_city\u0026#34;: \u0026#34;MOUNTAIN VIEW\u0026#34;, 8 \u0026#34;de_country\u0026#34;: \u0026#34;US\u0026#34;, 9 \u0026#34;de_continent\u0026#34;: \u0026#34;NA\u0026#34;, 10 \u0026#34;de_proxy\u0026#34; : \u0026#34;? - ?\u0026#34;, 11 \u0026#34;mx_city\u0026#34;: \u0026#34;\u0026#34;, 12 \u0026#34;mx_country\u0026#34;: \u0026#34;\u0026#34;, 13 \u0026#34;mx_continent\u0026#34;: \u0026#34;\u0026#34; 14} Private IP What will it return for a private IP? Maxmind is trumped again, however, DE does tell you it\u0026rsquo;s a private IP. Nice.\n1$ curl -k https://meta.dane-example.com.global.prod.fastly.net/?ip=192.168.1.1 2 3{ 4 \u0026#34;date\u0026#34;: \u0026#34;Fri, 05 Mar 2021 16:23:53 GMT\u0026#34;, 5 \u0026#34;ipv6\u0026#34;: \u0026#34;false\u0026#34;, 6 \u0026#34;public_ip\u0026#34;: \u0026#34;192.168.1.1\u0026#34;, 7 \u0026#34;de_city\u0026#34;: \u0026#34;PRIVATE\u0026#34;, 8 \u0026#34;de_country\u0026#34;: \u0026#34;**\u0026#34;, 9 \u0026#34;de_continent\u0026#34;: \u0026#34;**\u0026#34;, 10 \u0026#34;de_proxy\u0026#34; : \u0026#34;? - ?\u0026#34;, 11 \u0026#34;mx_city\u0026#34;: \u0026#34;\u0026#34;, 12 \u0026#34;mx_country\u0026#34;: \u0026#34;\u0026#34;, 13 \u0026#34;mx_continent\u0026#34;: \u0026#34;\u0026#34; 14} What if someone hits a random path?\nSince the body doesn\u0026rsquo;t return anything for redirects, we will either have to follow redirect using -L or inspect the headers to confirm the response.\nLet\u0026rsquo;s do the latter.\n1$ curl -k -I https://meta.dane-example.com.global.prod.fastly.net/random 2 3HTTP/2 301 Moved Permanently 4accept-ranges: bytes 5cache-control: private, no-store, no-cache, max-age=0 6content-length: 0 7date: Fri, 05 Mar 2021 16:23:53 GMT 8location: https://meta.dane-example.com.global.prod.fastly.net/ 9retry-after: 0 Conclusion We enhanced the previous basic version considerably, by returning output from both vendors as well as making the response more machine friendly. It is possible to further enhance this service to put out even more information, as needed. It removes any reliance on 3rd party services which may or may not be hosted in secure environments.\nThe simpler service with only txt output is desribed in a previous post - Fastly Meta Service.\nNote:\u0026nbsp; Code mentioned above is here\u0026nbsp; References (1) Geolocation\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/fastly-geo-service/","summary":"Introduction Fastly is a popular CDN based on the open-source Varnish. Since it supports VCL, a lot of custom \u0026ldquo;logic\u0026rdquo; to handle incoming requests can be added, right at the edge. It also provides a host of geolocation identification options.\nIn this post, we\u0026rsquo;ll build on the service we created in a previous post Fastly Meta Service to implement a simple API which returns details about a particular public IP.","title":"Fastly Geolocation Service"},{"content":"Introduction Amazon Web Services (AWS) offers certifications in 4 broad categories - Foundational, Associate, Professional and Speciality. I recently cleared the Solutions Architect Associate certification. This time, wanted to attempt a \u0026ldquo;Speciality\u0026rdquo; certification. And what better topic to delve into than Cloud security. As much of a boon cloud is, it\u0026rsquo;s Achilies\u0026rsquo; heel remains unsecure workloads. Although AWS provides a ton of tools to mitigate and potentially limit security incidents, but they still do happen and often cost not just in monetary terms but also reduced customer confidence.\nI decided to take this assessment to see where I stand and if my head is in the right place especially w.r.t securing the Cloud workloads. After about a week\u0026rsquo;s on and off preparation, I was able to pass the exam and earn my badge. \u0026#x1f44f;\nI\u0026rsquo;ll divide this post into 5 sections - exam format, exam topics, my preparation, additional resources and tips.\nLet\u0026rsquo;s discuss the exam format first.\nWhat is the exam like? The exam duration is roughly 3 hours (180 mins), and you need to answer 60-odd questions. The number of questions can vary as per AWS, with some questions being not used in scoring. Questions are multiple-choice types with a mix of single answer or multiple answers. Multiple answers can appear in 2 scenarios. A problem that is solved in multiple steps or a problem that can be solved in multiple ways. I managed to finish answering all questions and reviewing a few in under 90 minutes. So don\u0026rsquo;t worry about the duration, its generous. And I managed to score \u0026gt;900.\nYou are expected to login atleast 30 mins prior to your scheduled time and a proctor will inspect your surrounding before setting up the exam. The 180 min window starts from when you launch the exam.\nThe result will be shown on the screen as soon as the test ends. You can expect a detailed scorecard in your inbox within the next day, though they mention it can take upto 5 days.\nWhat kind of questions to expect? Here are the exam objectives as per AWS.\nBroadly, the exam tests you on 5 topics (decreasing weightage) -\nInfrastructure Security (26%) Data Protection (22%) Identity and Access Management (20%) Logging and Monitoring (20%) Incident Response (12%) How did I prepare? There is no better preparation than hands-on experience.\nJust what AWS says in their exam guide, and it is quite true. Yes, you could slog it out and try to clear this exam, but you will need some form of hands-on training to be successful.\nWorking with AWS on a daily basisc, I already had a good understanding of the major services, security practices, recommended architectures etc. I started by going through the exam guide and sample questions to understand what kind of knowledge AWS expects the test takers to have.\nI then browsed through the FAQ section of some crucial services under the \u0026ldquo;Security\u0026rdquo; umbrella. For example, KMS, IAM, CloudTrail etc. The FAQ pages are an excellent resource to develop an understanding of the services, and their limitations. Knowing limitatons or where a particular service does not apply can help eliminate choices. For more detailed information, I also looked through the documentation.\nBetween the FAQ and Documentation - everything gets covered in terms of what can be asked in the exam. If you already have some experience with AWS, chances are these will probably be like reaffirming your knowledge. If you don\u0026rsquo;t have practical experience previously, my recommendation is to try out at least the major services in a free-tier account before attempting the exam.\nAWS Security Blog is a great source to get familier with Security best practices as well as some recommended architectures to automate incident responses etc.\nI did not buy any courses or prep materials from the usual suspects as I did not feel the need after the above preparation. In total, I devoted around 5 days of prep time. 2 to 4 hours on each day.\nAnything else? AWS\u0026rsquo;s re:Invent videos are a great source for a deep dive into any service. A small caveat here is to supplement them with the documentation. Over the years, even though most concepts remain the same, service limits and capabilities keep changing.\nTake the AWS Security Training Quizes here.\nAWS Training Portal courses for the major services like IAM, KMS etc.\nThis is a good starting point which covers most of the curcial services this certification delves into. This is a good course to understand how should you secure webapps This is for security basics This one is must take for deep dive on IAM! Complete the exam readiness course by AWS here. It would give you an idea if you need to prepare better in some areas.\nAWS provides a test exam at a nominal fee, which you can use to test your understanding before the final exam. This can give you a feedback in time, if you need to improve. I did not take it though.\nTips! I recommend to prepare these topics really well as a majority of the questions will touch upon these in some form.\nKMS - Study about the different keys, their rotation, access control, cross account, cross region access, where and how to use data keys, permissions needed to encrypt/decrypt etc. This will have maximum questions after infrastruture security.\nS3 - It may not immediately strike as a \u0026ldquo;security\u0026rdquo; service but it plays a crucial role as a data store and protecting data at rest. Be absolutely clear about controlling object access, bucket policies, encryption options etc. It also plays a key role in incident management, audit (Cloudtrail logs) etc. Be thorough with your understandng of this service.\nIAM - Study about the different policies (resource/identity) and their use-cases. Be familier with the policy syntax as well, there maybe a few questions which will ask you to choose the most appropriate policy given a requirement. Topics like Federation, SSO, Identity providers, STS are all important to know about.\nVPC - The network layer is incredibly important again. Things like NACL, routes, NAT gateways, flow logs etc are essential to creating secure architectures as well as during incident response. Establing secure routes for workloads using VPN, DirectConnect, VPC endpoints etc. is essential to having a secure deployment.\nCompute - EC2, ALB, external security appliances etc. One of the rare occassions, where AWS actually recommends some thrid party tools. How and why, what additional configs should be used - one needs to be aware of the details.\nMonitoring \u0026amp; Audit - Cloudwatch, Cloudtrail, Config, Trusted Advisor, Inspector etc. are some of the services which can fall under this category. Be clear about when and how would you use Cloudtrail vs Cloudwatch. Prepare well for these service including any customizations that may be needed to get the desired solution, like - centralizing logs on Cloudwatch using an agent on every EC2. Basically, what AWS offers out-of-the-box, and what you need to configure specifically.\nOthers - There are a whole host of services which touch upon the security sphere in some way of the other - Macie, GuardDuty, WAF, CloudTrail, Secrets Manager, Artifacts, ACM etc. Be sure about the purpose of each and maybe more impotantly, what one cannot do with them!\nI noted down some points while preparing for the exam, to brush up my knowledge, and as a quick reference. This is in no way exhaustive, but merely what I felt worth keeping in mind. Refer to this note.\nConclusion To sum up, if you have been working with AWS for a year or 2, with some focus on securing the accounts, it should not take you more than a week\u0026rsquo;s study to be prepared. But, I still recommend supplementing your practical knowledge with some of the resources I described above.\nIf you are just starting out, I suggest to get some hands-on experience first with the basic services discussed above before attempting the exam. Also, familiarise yourself with the shared cloud security model. I cannot stress this enough, practical experience is absolutely essential to acing this exam.\nHope you find this post helpful.\nGood Luck for your exam! \u0026#x1f44d;\nReferences (7) Certified Security Specialty\u0026nbsp; AWS Security Best Practices.pdf\u0026nbsp; AWS Certified Security Speciality Sample Questions.pdf\u0026nbsp; AWS Certified Security Specialty Exam Guide.pdf\u0026nbsp; Aws Federated Authentication With Active Directory Federation Services Ad Fs\u0026nbsp; LearningLibrary\u0026nbsp; ELearning\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/certified-security-speciality/","summary":"Introduction Amazon Web Services (AWS) offers certifications in 4 broad categories - Foundational, Associate, Professional and Speciality. I recently cleared the Solutions Architect Associate certification. This time, wanted to attempt a \u0026ldquo;Speciality\u0026rdquo; certification. And what better topic to delve into than Cloud security. As much of a boon cloud is, it\u0026rsquo;s Achilies\u0026rsquo; heel remains unsecure workloads. Although AWS provides a ton of tools to mitigate and potentially limit security incidents, but they still do happen and often cost not just in monetary terms but also reduced customer confidence.","title":"AWS Certified Security Specialty (SCS-C01) - My Experience"},{"content":"Introduction Those who do not learn from history are doomed to repeat it!\nThis is incredibly apt for the software development world as well. And the act of keeping a tab on a project\u0026rsquo;s Git history has split the world in half, quite literally.\nOn one side, there are people who want the history to be a snashot of what exactly happened in a repo, and a non-linear history is not much of a concern to them. The workflow for such a history is quite simple and doesn\u0026rsquo;t need any more than a working knowledge of git.\nOn the other side, there are people who prefer to have a clean linear history - free of merge commits, multiple topic commits of feature branches etc. This workflow relies heavily on git rebase, and thus requires a higher level of proficiency than using just the basic git commands.\nIrrespective of which camp you fall into, working in a large team, there are high chances the project\u0026rsquo;s history does not look like you want it to. But, since most of the magic that affects history happens on local, you can still stick to the preferred camp! \u0026#x1f604;\nLet\u0026rsquo;s go over the different kinds of git history and required workflows.\nNon-linear Git history Let\u0026rsquo;s look at what the non-linear history is, and what contibutes to it. If your project history looks anything like the following simplified image you have a non-linear history. The biggest contibutors to a non-linear history are merge commits. Merge commits are created when git does a 3-way merge between the HEADs of source and target branches. The final outcome is a 3rd commit on the target branch. This can be seen in the figure above when the blue \u0026amp; green (feature) branches merge into black (master).\nMerge commits are introduced not just while merging a feature PR into master, but also when you merge master back into a feature. This is often required in active projets where master gets updated frequently.\nAs seen here, the git log will show every commit right down to the ones made on the feature branch. It also shows the PR merge events as well as master merge events quite clearly.\nWhat does the workflow look like? To achieve a non-linear history, there isn\u0026rsquo;t anything special one needs to do. The below basic high-level steps will eventually result in a non-linear history.\n1# 1. Branch off master 2$ git checkout -b featureA 3 4# 2. Make the changes 5 6# 3. Commit the changes 7$ git add . 8$ git commit -m \u0026#34;commit 1 on featureA\u0026#34; 9 10# Repeat 2, 3 while all changes are completed. 11 12# 4. While this change was underway, master was updated. 13# The latest changes from master need to be merged into the feature branch. 14# Pull the latest changes from master. 15$ git merge origin/master 16 17# 6. Push to origin and raise PR to master. 18$ git push 19# or 20$ git push --set-upstream origin featureA 21 22# 7. Finally, merge the PR. As is evident, it is pretty simple and is often the workflow of choice for teams with basic git understanding.\nLinear Git history Now, let\u0026rsquo;s look at a linear git history and how can it be achieved. If you project\u0026rsquo;s history looks like the following image, you have a linear history. The biggest benefit of a linear history is that it\u0026rsquo;s easy to follow the project\u0026rsquo;s progress. This also depends heavily on the teams sticking to a \u0026ldquo;one feature one commit\u0026rdquo; philosophy.\nWhat does the workflow look like? To achieve a linear history, everyone in the team needs to follow the below high-level steps.\n1# 1. Branch off master 2$ git checkout -b featureC 3 4# 2. Make the changes 5 6# 3. Commit the changes 7$ git add . 8$ git commit -m \u0026#34;commit 1 on featureC\u0026#34; 9 10# Repeat 2, 3 while all changes are completed. 11 12# 4. Rebase all commits on local branch (Ref 1 below) 13$ git rebase -i HEAD~3 14 15# 5. While this change was underway, master was updated. 16# The latest changes from master need to be merged into the feature branch. 17# Pull the latest changes from master (Ref 2 below) 18$ git rebase -i origin/master 19 20# 6. Push to origin and raise PR to master. 21# If the local branch has already been pushed to remote, 22# use the \u0026#39;--force-with-lease\u0026#39; flag. 23$ git push 24# or 25$ git push --force-with-lease 26 27# 7. Finally, merge the PR. Ref 1: Above git rebase -i command launches an interactive UI to select and choose the commits to work with - There are several options to complete the rebase, most common ones being squash (commit messages from individual commits are added to the final commit message) and fixup (commit messages are discarded) Ref 2: After this completes successfully, the history will start to look like this - \u0026#x1f449; Note: rebase is a poweful command, keep these things in mind to avoid unforseen issues -\nDon\u0026rsquo;t rebase commits already pushed to master. Try to run it on local commits as much as possible. Don\u0026rsquo;t rebase commits which are part of a PR as it will mean rework for reviewers. Use --force-with-lease instead of -f to avoid wiping out other\u0026rsquo;s changes already pushed to remote. Conclusion There is no one size fits all in this case of Git history. There are pros \u0026amp; cons of each approach. While I personally strive to keep the history clean by following the second workflow, I have no issues in working with teams which feel comfortable with the first workflow. I still manage to keep my end of the project clean in such a case.\nHope this post proves useful. \u0026#x1f44d;\nNote:\u0026nbsp; Code mentioned above is here\u0026nbsp; References (4) Git Rebase\u0026nbsp; Git Merge\u0026nbsp; Git Branching Rebasing\u0026nbsp; Git Rebase\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/git-history/","summary":"Introduction Those who do not learn from history are doomed to repeat it!\nThis is incredibly apt for the software development world as well. And the act of keeping a tab on a project\u0026rsquo;s Git history has split the world in half, quite literally.\nOn one side, there are people who want the history to be a snashot of what exactly happened in a repo, and a non-linear history is not much of a concern to them.","title":"Git History Wars"},{"content":"Introduction A consistent convention, whether it is for naming branches or adding commit messages is absolutely essential for a team. Following a common theme or standard ensures the readability and maintainability of the project. It also makes the lives of people who will come after you a lot easier.\nProblem Standards or conventions can be set, however, without oversight there are always chances of people overlooking them. Remember this famous comic? \u0026#x1f605;\nSolution Git provides scripts out of the box, which run on certain events. These can be run locally or on the git server. Having these hooks locally makes them easier to edit or implement, but they are easy to bypass.\nLocal hooks\npre-commit prepare-commit-msg commit-msg post-commit Server hooks\npre-receive update post-receive To establish an oversight mechanism for commit messages, use the commit_msg hook. It runs after a local commit, and fails the commit if the script exits with a non-zero code.\nLocal hooks can only serve as a guidance and should not be relied upon for enforcements. Server hooks are a better choice for enforcing rules. Local hooks reside in the folder .git/hooks/ and are not checked into source control.\nLet\u0026rsquo;s assume we want every team member to add commit messages in the following format -\n[ISSUE-000] Short description in 50 chars or less To nudge the team to adhere to this standard, distribute the commit-msg file in a separate directory in your source control, like hooks.\n1#!/bin/sh 2 3commit_regex=\u0026#39;^(\\[ISSUE-[0-9]+\\] [A-Z][a-zA-Z0-9 -]{1,30})$\u0026#39; 4commit_error=\u0026#39;Error! Not as per format - 5 [ISSUE-000] Task description 6\u0026#39; 7if ! grep -qE \u0026#34;$commit_regex\u0026#34; \u0026#34;$1\u0026#34;; then 8 echo \u0026#34;$commit_error\u0026#34; \u0026gt;\u0026amp;2 9 exit 1 10fi Add appropriate instructions in the README.md of the project on how to enable it. High level steps will be -\nAdd a symlink or simply copy the files from hooks directory to .git/hooks/. Make sure the files under .git/hooks/ are set to executable. Test by adding a random commit you expect to fail - Success! \u0026#x1f44f;\nCaveats Easily disabled by the user! They can simply delete the file from the .git/hook/ directory, or Use --no-verify cli option with git commit to bypasses this local hook Use server side hooks to enforce compliance or other checks. Local hooks work with Gitbash for Windows, so no extra config is required for Windows. Conclusion Git local hooks provide an excellent option to automatically execute scripts on certain events. For example, run lint after checkout, or do a code scan before commit, etc. Local hooks can be useful to setup oversight, to make sure you don\u0026rsquo;t stray from the set standards or conventions. They, however, are not for setting up guardrails or enforcement as they reside on each user\u0026rsquo;s system and are completely under their control.\nAdding a check for commit message is only a small part of the overall functionality that Git hooks provide. Check the documentation to know more and explore what else can they do.\nHope this post proves useful! \u0026#x1f44d;\nNote:\u0026nbsp; Code mentioned above is here\u0026nbsp; References (1) Customizing Git Git Hooks\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/git-hooks/","summary":"Introduction A consistent convention, whether it is for naming branches or adding commit messages is absolutely essential for a team. Following a common theme or standard ensures the readability and maintainability of the project. It also makes the lives of people who will come after you a lot easier.\nProblem Standards or conventions can be set, however, without oversight there are always chances of people overlooking them. Remember this famous comic?","title":"Git Hooks"},{"content":"Introduction Elasticsearch is an open-source search solution which is quite popular for logs analysis. It allows data from various different sources to be available and searchable at a centralized location.\nIn this post, we will see how to ingest logs from S3 into Elasticsearch using AWS Lambda.\nArchitecture The stack will look like the following once launched. Logs are written to the S3 bucket. Our ingestion lambda is then triggered based on these events. The logs for this Lambda is written to AWS Cloudwatch, and alarms are also created to notify the relevant team if there are failures. Once the Lambda is able to read and process the log file from S3, it pushes them to the Elasticsearch cluster using the /bulk_api.\nIngestion Elasticsearch offers a lot of options to ingest data - Beats, Logstash, language specific clients and a generic REST API.\nThe REST API offers 2 distinct endpoints for indexing data - single document or bulk. The single document API is useful to index a small number of documents. In case, the number of documents to index is large it is often benefitial to use the bulk API.\nFor the purpose of this Lambda, we will use the _bulk REST API to index the data.\nBulk Ingest REST API The bulk API for ingesting docs into Elasticsearch offers a couple of subtle variations as follows -\n1POST /_bulk 2{ \u0026#34;index\u0026#34; : { \u0026#34;_index\u0026#34; : \u0026#34;test1\u0026#34; } } 3{ _document1_ } 4{ _document2_ } 5{ \u0026#34;index\u0026#34; : { \u0026#34;_index\u0026#34; : \u0026#34;test1\u0026#34; } } 6{ _document3_ } OR\n1POST /index/_doc/_bulk 2{ \u0026#34;index\u0026#34; : {}} 3{ _document1_ } 4{ _document2_ } 5{ _document3_ } Do note, that the document which is submitted as part of the bulk API is not a valid json. Each line of the document needs to be a valid json.\nS3 Logs The logs being pushed into S3 need to conform to the following format -\nValid json string on each new line S3 path - s3://bucket/service/date/logfile-sequence.log Lambda The Lambda is triggered on every S3 put object event. It then reads the contents of the file and prepares a bulk ingestion doc as pecified in the section above.\nA new index is created daily during ingestion. Index name pattern is service-date. Example - httpd-2020.01.01\nIngest Pipelines As I wrote in an earlier post, ingest pipelines are quite easy to setup. They can step-in if you have modest data transformation needs, and do not want (or need) a full blown logstash setup. Pipeline can be specified as a query parameter during ingestion.\n1POST /index/_bulk?pipeline=master_pipeline 2{ \u0026#34;index\u0026#34; : {}} 3{ _document1_ } 4{ _document2_ } 5{ _document3_ } Source code Once the repo is cloned, you can run the below steps before deploying the stack.\nCreate and activate virtual environment\n1$ python3 -m venv env 2$ source env/bin/activate Install dependencies\n1$ (env) pip install -r requirements.txt Format, lint, run tests, check coverage reports etc.\n1$ (env) black src/*.py 2$ (env) flake8 3$ (env) pytest 4$ (env) coverage run -m pytest 5$ (env) coverage html Deploying the Solution We will use Serverless framework to deploy all the resources as depicted in the architecture diagram above. Modify configs as per your environment - ES base url, account number etc, and then follow the steps below to create this stack.\nExport the AWS credentials as environment variables. Either access/secret keys or the aws cli profile\nDeploy/Update the service to AWS\n1$ sls deploy To cleanup and remove the resources, just run\n1$ sls remove Conclusion That\u0026rsquo;s it!\nThis was a simple function to index documents to an Elasticsearch cluster. Feel free to customize the function to work in your environment.\nHope this post proves useful. \u0026#x1f44d;\nNote:\u0026nbsp; Code mentioned above is here\u0026nbsp; References (2) Ingest Apis\u0026nbsp; Docs Bulk\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/index-elasticsearch/","summary":"Introduction Elasticsearch is an open-source search solution which is quite popular for logs analysis. It allows data from various different sources to be available and searchable at a centralized location.\nIn this post, we will see how to ingest logs from S3 into Elasticsearch using AWS Lambda.\nArchitecture The stack will look like the following once launched. Logs are written to the S3 bucket. Our ingestion lambda is then triggered based on these events.","title":"Index Logs to Elasticsearch"},{"content":"Introduction In the previous post, we launched a Cloud Function in GCP.\nToday, let\u0026rsquo;s look at creating a Kubernetes cluster and deploying Grafana on it. Google calls it\u0026rsquo;s Kubernetes offering GKE - Google Kubernetes Engine. It is a fully managed service which considerably reduces the operational overhead of maintaining and running a production grade cluster.\nExercise We will keep this fairly simple. It will involve\nEnable required googleapis to work with - container in this case. Launch the GKE cluster using gcloud with a single node in the same zone. Deploy the latest Grafana image to the newly launched cluster using kubectl. Make the deployment accessible over the internet. Configure Google Cloud Monitoring as a data source in Grafana. Configure a dashboard. Cleanup - delete all resources. Steps Let\u0026rsquo;s go through the steps to complete the aforementioned tasks.\n1. Enable container API As last time, the first step of working with any service in GCP is to enable it\u0026rsquo;s APIs - container for this post.\n1$ gcloud services enable container.googleapis.com In case, you are not sure of the exact name of the api, use this command to fetch the entire list\n1$ gcloud services list --available 2. Launch the GKE cluster The simplest way to launch a GKE cluster is via gcloud. It takes care of a number of defaults for you. The full list of options can be seen here.\n1$ gcloud container clusters create tools-k8s --zone us-central1-a --num-nodes=1 We define --num-nodes=1 to limit the pool nodes to 1 (default is 3), --zone us-central1-a to define the zone for the cluster and nodes.\nYou could also modify the node type (default is n1-standard-1). To get a list of all the valid and available node types in a particular zone, use this command (by defualt it lists this info for every zone) -\n1$ gcloud compute machine-types list --filter=\u0026#34;zone: (us-central1-a)\u0026#34; The create command will run for a few mins and by the end of it, you should have a running K8S cluster ready to accept deployments. You can list the cluster to verify (I have modified the MASTER_IP).\n1$ gcloud container clusters list 2NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS 3tools-k8s us-central1-a 1.16.15-gke.4901 192.168.1.1 n1-standard-1 1.16.15-gke.4901 1 RUNNING 3. Deploy Grafana We can now deploy the latest Grafana image on this cluster using the cli utility kubectl\n1$ kubectl create deployment grafana --image=docker.io/grafana/grafana 2deployment.apps/grafana created Verify that new pods are launched and the deployment is successful -\n1$ kubectl get pods 2NAME READY STATUS RESTARTS AGE 3grafana-7fb845f455-27tpl 1/1 Running 0 50s 4 5$ kubectl get deployment 6NAME READY UP-TO-DATE AVAILABLE AGE 7grafana 1/1 1 1 1m30s 4. Enable public access to Grafana The deployment is successful, however, we can\u0026rsquo;t access it just yet. Now we have to create a service which will expose this deployment to the world, outside the K8S cluster.\n1$ kubectl expose deployment grafana --name=grafana-service --type=loadBalancer --port 80 --target-port 3000 Verify the new service is deployed. It may take a few mins to provide you the public external IP (I have modified the external-IP).\n1$ kubectl get service 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3grafana-service LoadBalancer 192.168.1.2 192.168.1.10 80:30447/TCP 2m Now you can simply hit the external IP shown in the command above to access Grafana.\n5. Configure datasource in Grafana Login using the default Grafana creds (admin:admin) and navigate to the datasources section. Here, click add on Google Cloud Monitoring. On the config screen, select \u0026ldquo;Service Account\u0026rdquo; for authentication. Click \u0026ldquo;Save \u0026amp; Test\u0026rdquo;.\nWe are running Grafana on GKE, using a service account which has full access to monitoring API so no additional steps are needed. If you get an error, verify that the service account has read access to monitoring API.\n6. Configure a dashboard in Grafana Now that we have a datasource added, let\u0026rsquo;s configure a dashboard in Grafana. We will plot the successful/failed/crashed invocations of the cloud funtion we created in the previous post. It is pretty straight forward and queries are exactly the same as what you would confiure in the Google monitoring console.\nHere is the final result -\n7. Cleanup Delete the service. It may take a few mins.\n1$ kubectl delete service grafana-service 2service \u0026#34;grafana-service\u0026#34; deleted Delete the cluster. Enter \u0026lsquo;Y\u0026rsquo; at the prompt, this will also take a few mins.\n1$ gcloud container clusters delete tools-k8s --zone us-central1-a 2The following clusters will be deleted. 3- [tools-k8s] in [us-central1-a] 4 5Do you want to continue (Y/n)? Y 6 7Deleting cluster tools-k8s...done. 8Deleted [https://container.googleapis.com/v1/projects/tools-202011/zones/us-central1-a/clusters/tools-k8s]. Conclusion GKE is a fully managed Kubernetes offering from Google Cloud which makes it incredibly simple to deploy your app to the cloud. Although this was a very basic example, it can be a good first step to start your GKE journey. We will delve into more complex setups in the future.\nHope this post proves useful, and sets you on the path to exploring more. \u0026#x1f44d;\nReferences (2) Docs\u0026nbsp; Installation\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/day2-kubernetes-engine/","summary":"Introduction In the previous post, we launched a Cloud Function in GCP.\nToday, let\u0026rsquo;s look at creating a Kubernetes cluster and deploying Grafana on it. Google calls it\u0026rsquo;s Kubernetes offering GKE - Google Kubernetes Engine. It is a fully managed service which considerably reduces the operational overhead of maintaining and running a production grade cluster.\nExercise We will keep this fairly simple. It will involve\nEnable required googleapis to work with - container in this case.","title":"Day 2 with GCP - Google Kubernetes Engine"},{"content":"Introduction In their own words, Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform, offering over 175 fully featured services from data centers globally. Millions of customers—including the fastest-growing startups, largest enterprises, and leading government agencies—are using AWS to lower costs, become more agile, and innovate faster.\nI had always read about their certification paths, but never got around to dig deeper. Until last week. Decided to have a go at it, to validate my knowledge and experience. After about a week\u0026rsquo;s preparation I was able to pass the exam and earn my badge. \u0026#x1f44f;\nI\u0026rsquo;ll divide this post into 5 sections - exam format, exam topics, my preparation, additional resources and tips.\nLet\u0026rsquo;s discuss the exam format first.\nWhat is the exam like? The exam duration is roughly 2 hours (130 mins), and you need to answer 60-odd questions. The number of questions can vary as per AWS, with some questions being not used in scoring. Questions are multiple-choice types with a mix of single answer or multiple answers. Multiple answers can appear in 2 scenarios. A problem that is solved in multiple steps or a problem that can be solved in multiple ways.\nYou are expected to login atleast 30 mins prior to your scheduled time and a proctor will inspect your surrounding before setting up the exam. The 130 min window starts from when you launch the exam.\nThe result will be shown on the screen as soon as the test ends. You can expect a detailed scorecard in your inbox within the next day, though they mention it can take upto 5 days.\nWhat kind of questions to expect? As per AWS, the exam objectives are as follows -\nEffectively demonstrate knowledge of how to architect and deploy secure and robust applications on AWS technologies. Define a solution using architectural design principles based on customer requirements. Provide implementation guidance based on best practices to the organization throughout the life cycle of the project. How did I prepare? There is no better preparation than hands-on experience.\nJust what AWS says in their exam guide, and it is quite true. Yes, you could slog it out and try to clear this exam, but you will need some form of hands-on training to be successful.\nI had good exposure to AWS, thanks to my job profile. As a result, I had a good understanding of the major services, security practices, recommended architectures etc. I started by going through the exam guide and sample questions to understand what kind of knowledge AWS expects the test takers to have.\nI then browsed through the FAQ section of services I did not feel absolutely confident about. For example, AWS Direct Connect, AWS Snowball, AWS Storage Gateway etc. The FAQ pages are an excellent resource to develop an understanding of the service, and where does it best fit. For more detailed information, I also looked through the documentation.\nBetween the FAQ and Documentation - everything gets covered in terms of what can be asked in the exam. If you already have some experience with AWS, chances are these will be like reaffirming your knowledge. If you don\u0026rsquo;t have practical experience previously, my recommendation would be to try out at least the major services in a free-tier account before attempting the exam.\nAWS Whitepapers are a great source to get a holistic understanding of how to approach a problem and design a \u0026ldquo;well-architected\u0026rdquo; system. If not all, be sure to go through the Well Architected section to understand what makes a architecture \u0026ldquo;well-architected\u0026rdquo; in AWS speak.\nI did not buy any courses or prep materials from the usual suspects as I did not feel the need after the above preparation. In total, I must have devoted around 7 days of prep time. 4 hours or more on 3-4 days, and 2 hours or less on others.\nAnything else? AWS\u0026rsquo;s re:Invent videos are a great source for a deep dive into any service. A small caveat here is to supplement them with the documentation. Over the years, even though most concepts remain the same, service limits and capabilities keep changing. Take the AWS Training Quizes here. There are about 6 to 7 episodes of 20 mins each where the panelists go over 4 or 5 questions. AWS Training Portal videos for specific services you want to deep dive into. For example, this is for S3 storage classes. Complete the exam readiness course by AWS here. It would give you an idea if you need to prepare better in some areas. AWS provides a test exam at a nominal fee, which you can use to test your understanding before the final exam. This can give you a feedback in time, if you need to improve. I did not take it though. This is an excellent video (10 hr+) from FreeCodeCamp that covers almost everything and is completely free! It\u0026rsquo;s a great resource to brush up your understanding. The cource is by Andrew Brown from ExamPro.co. Cheatsheets by Andrew Brown here are again excellent to do last minute revisions, and are completely free. Tips! I recommend to prepare these topics really well as a majority of the questions will touch upon these in some form.\nS3 - Study about S3, Glacier, lifecycle policies, security, controlling access, bucket policies, replication, storage classes etc. This is one of the most important sections as S3 has a huge variety of applications. Be thorough with your understandng of this service. IAM - Study about the different policies (resource/identity) and their use-cases. Be familier with the policy syntax as well, there maybe a few questions which will ask you to choose the most appropriate policy given a requirement. VPC - The network layer is incredibly important again. Things like NACL, gateway endpoints, EIP, routes, NAT gateways, flow logs etc are essential to creating robust, secure, fault-tolerant architectures. Compute - EC2, ECS, Lambda or EKS. This is again a big one. You need to have good understanding about the instance types, capacity reserations, placement groups, auto-scaling policies, Load balancers (\u0026amp; their applications) etc. Storage - EBS, EFS, Storage gateways etc. Questions might ask about the most suitable storage type for a requirement, so you must be aware of which product offers what. AWS often puts options in questions that are simply not possible, so it\u0026rsquo;s just as important to know what a service can\u0026rsquo;t do. Database - RDS, DynamoDB, Aurora, RedShift etc. Be very clear about the functions, uses and capabilities of replicas. How are they configured, what kind of auto-scaling each of these offer etc. It may get tricky with RDS MySQL and Aurora at times, but if you have a thorough grasp of Aurora you will be able to select the answers by eliminating wrong choices. Security - CloudTrail, KMS, IAM, Parameter Store, Secrets Manager etc. Explore the different options available to secure the workloads in cloud. It may not necessarily be about 1 service in particular, but rather a combination of services. Monitoring and automation - Cloudwatch, Cloudformation, Systems Manager etc. Although it\u0026rsquo;s a small sub-section, you would still get questions about monitoring, alerting etc. So prepare well for these areas including what AWS offers out-of-the-box, and what you need to configure specifically. Conclusion To sum up, if you have been working with AWS for a year or 2, I recommend supplementing your practical knowledge with some of the resources I described above. It should not take you more than a week\u0026rsquo;s study to be prepared.\nIf you are just starting out, I would suggest to get some hands-on experience first with the basic services like VPC, EC2, S3, IAM etc. before attempting the exam. I cannot stress this enough, practical experience is absolutely essential to acing this exam.\nHope you find this post helpful.\nGood Luck for your exam! \u0026#x1f44d;\nReferences (1) Certified Solutions Architect Associate\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/certified-solutions-architect/","summary":"Introduction In their own words, Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform, offering over 175 fully featured services from data centers globally. Millions of customers—including the fastest-growing startups, largest enterprises, and leading government agencies—are using AWS to lower costs, become more agile, and innovate faster.\nI had always read about their certification paths, but never got around to dig deeper. Until last week. Decided to have a go at it, to validate my knowledge and experience.","title":"AWS Certified Solutions Architect (SAA-C02) - My Experience"},{"content":"Introduction Terraform has evolved and established itself as the defacto tool for implementing IAC (Infrastructure As Code) across cloud and other service providers. The Google trends graph is a testament to it\u0026rsquo;s growing popularity.\nI have been using it for the last few years for my projects and it\u0026rsquo;s a great fit in most situations.\nI had always read about their certification, but never got around to dig deeper. Until last week. Decided to have a go at it, to validate what I knew about it, and find out, what I didn\u0026rsquo;t. After about a day\u0026rsquo;s preparation I was able to pass the exam and earn my badge. \u0026#x1f44f;\nI\u0026rsquo;ll divide this post into 5 sections - exam format, exam topics, my preparation, additional resources and tips.\nLet\u0026rsquo;s discuss the exam format first.\nWhat is the exam like? The exam duration is 60 mins, and you are expected to answer 60-odd questions which are a mix of multiple-choice, true-false, fill-in-the-blanks kinds. The fill-in-the-blank questions were new to me but I quite liked them.\nYou are expected to login atleast 15 mins prior to your scheduled time and a proctor will inspect your surrounding before setting up the exam. The 60 min window starts from when you launch the exam. So don\u0026rsquo;t worry if your login process takes a bit longer. Mine lasted for around 20 mins in total.\nThe result will be shown on the screen as soon as the test ends. You can expect a detailed scorecard in your inbox within the next hour.\nWhat kind of questions to expect? As per HashiCorp, the exam objectives are as follows -\nUnderstand infrastructure as code (IaC) concepts Understand Terraform\u0026rsquo;s purpose (vs other IaC tools) Understand Terraform basics Use the Terraform CLI Interact with Terraform modules Navigate Terraform workflow Implement and maintain state Read, generate, and modify configuration Understand Terraform Cloud and Enterprise capabilities How did I prepare? IF you have been using Terraform regularly, the best place to start is the Exam Review Guide. It lists out all the concepts you need to know along with the documentation and tutorial links that you can go through. I reviewed the documentation and tutorial links to double check my understanding.\nOnce I was done with the exam guide, I had a look at the Sample Questions.\nNext, I followed one of their getting started guides (AWS), to go through the steps and workflow. This helped brush up the knowledge of terraform cli and associated commands.\nI then browsed through the documentation to go over topics I did not feel confident about. In my case, it was workspaces.\nI did not buy any courses or prep materials from the usual suspects as I did not feel the need after the above preparation.\nAnything else? HashiCorp\u0026rsquo;s YouTube channel is a great resource to understand and get a feel of it if you haven\u0026rsquo;t worked with it before. This is an excellent video from FreeCodeCamp that covers most of the basic stuff. Start with the Study Guide if you are new to Terraform. Tips! I recommend to prepare these topics really well as a majority of the questions will touch upon these in some form.\nVariables - You should know about how to declare them, use in expressions, order of precedence etc. Docs link\nModules - This is one of the most important topics to know. You should know when, how, where to use them, as well as how to refer their outputs/variables/state etc. Docs link\nExpressions - This is the other important topic to know. Since terraform 0.12 a variety of expressions are now available for use like for_each, for, dynamic_blocks etc. Be sure to read through it and understand the different requirements for each. Docs link\nState - You should understand it thoroughly - why is it necessary, how to use it, manipulate it, what commands affect it etc. Docs link\nTerraform CLI - All the different commands like terraform init, terraform fmt, terraform taint, terraform plan, terraform apply, terraform show etc. including their uses and options. Docs link\nTerraform Cloud and Enterprise - Mentioning it here as you will get questions about them. You may not be using it or even need it while working with the open-source version, but don\u0026rsquo;t miss this! Docs link\nConclusion To sum up, if you have been working with Terraform in some way or form, it would take you about a day\u0026rsquo;s study to be prepared.\nIf you are just starting out, I would suggest to get some hands-on experience with Terraform first before attempting the exam.\nGood Luck! \u0026#x1f44d;\nReferences (2) Terraform Associate\u0026nbsp; 360049382552\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/certified-terraform-associate/","summary":"Introduction Terraform has evolved and established itself as the defacto tool for implementing IAC (Infrastructure As Code) across cloud and other service providers. The Google trends graph is a testament to it\u0026rsquo;s growing popularity.\nI have been using it for the last few years for my projects and it\u0026rsquo;s a great fit in most situations.\nI had always read about their certification, but never got around to dig deeper. Until last week.","title":"Hashicorp Certified Terraform Associate - My Experience"},{"content":"Introduction In the previous post, we completed a few basic things like - creating a free tier GCP account, a new project, service role, launched an instance etc.\nToday, let\u0026rsquo;s look at creating a cloud function. These are similar to what AWS refers as Lambda functions.\nExercise We will keep this fairly simple. It will involve\nEnable required googleapis to work with - like cloudfunctions, iam etc. Set up additional permissions for service-account Create a sample function Create the terraform config necessary to deploy the function Make the function endpoint public, to allow unauthenticated users to invoke it. Cleanup - delete all resources. Steps Let\u0026rsquo;s go through the steps to complete the aforementioned tasks.\n1. Enable necessary APIs As last time, the first step of working with any service in GCP is to enable it\u0026rsquo;s APIs.\n1$ gcloud services enable cloudfunctions.googleapis.com 2$ gcloud services enable cloudbuild.googleapis.com In case, you are not sure of the exact name of the api, use this command to fetch the entire list\n1$ gcloud services list --available 2. Attach permissions to the service account Allow access on cloudfunctions and iam by attaching roles to the service account\n1$ gcloud projects add-iam-policy-binding tools-202011 --member serviceAccount:tools-service-account@tools-202011.iam.gserviceaccount.com --role roles/iam.serviceAccountUser 2 3$ gcloud projects add-iam-policy-binding tools-202011 --member serviceAccount:tools-service-account@tools-202011.iam.gserviceaccount.com --role roles/cloudfunctions.admin 3. Sample function We will deploy a python3.7 function, which will return either \u0026ldquo;Hello World!\u0026rdquo; or \u0026ldquo;Hello (arg)!\u0026rdquo; depending on the request.\nSource code is here\n3. Terraform Config Creating the terraform config is pretty similar to how you\u0026rsquo;d do it for AWS Lambda. We need a function resource, a bucket object resource which contains the source code and a zip resource which generates the zip at runtime.\nFor GCP, an additional config item is creating a new user and allowing it to invoke the function. This is necessary to allow unautheticated requests to invoke the function.\nThe terraform template is here\n8. Run terraform With all the prerequisites completed, we can now setup terraform to work with GCP.\nAs last time, we need to provide credentials to terraform. Export the environment variable GOOGLE_APPLICATION_CREDENTIALS=tools-service-account.json.\nNow we can run the terraform commands to deploy our function.\n1$ terraform init 2 3$ terraform apply Type Yes when prompted. The function is deployed and the url to access it is displayed as output.\nNext, validate the function.\n9. Validate We can validate what resources have been creating either from the console or gcloud cli. Use these command to verify the resources are created as expected -\nCloud Function created\n1$ gcloud functions list 2NAME STATUS TRIGGER REGION 3greet ACTIVE HTTP Trigger us-central1 Storage bucket contains zip and terraform state file\n1gsutil ls -r gs://tools-202011/* 2gs://tools-202011/fsource/: 3gs://tools-202011/fsource/greet_2011192345.zip 4 5gs://tools-202011/terraform/: 6 7gs://tools-202011/terraform/day1-state/: 8gs://tools-202011/terraform/day1-state/default.tfstate Finally, let\u0026rsquo;s access the function endpoint to validate functionality.\nUsing Curl (curl cheatsheet)\n1$ curl https://us-central1-tools-202011.cloudfunctions.net/greet?name=sam 2Hello Sam! 3 4$ curl https://us-central1-tools-202011.cloudfunctions.net/greet 5Hello World! 6 7$ curl https://us-central1-tools-202011.cloudfunctions.net/greet?demo=1 8Hello World! Using Browser 10. Cleanup Remove all the resources created so far.\n1$ terraform destroy --auto-approve Conclusion Serverless architectures are incredibly efficient and relatively maintenance free. Due to some of their inherent benefits, they have become extremely common, popular and even preffered for a lot of solutions.\nThus, a good understanding of services like \u0026ldquo;Cloud Functions\u0026rdquo; is crucial. Hope this post proves useful, and sets you on the path to exploring more. \u0026#x1f44d;\nNote:\u0026nbsp; Code mentioned above is here\u0026nbsp; References (4) Functions\u0026nbsp; Http\u0026nbsp; 5889\u0026nbsp; 1938\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/day1-gcp-cloud-functions/","summary":"Introduction In the previous post, we completed a few basic things like - creating a free tier GCP account, a new project, service role, launched an instance etc.\nToday, let\u0026rsquo;s look at creating a cloud function. These are similar to what AWS refers as Lambda functions.\nExercise We will keep this fairly simple. It will involve\nEnable required googleapis to work with - like cloudfunctions, iam etc. Set up additional permissions for service-account Create a sample function Create the terraform config necessary to deploy the function Make the function endpoint public, to allow unauthenticated users to invoke it.","title":"Day 1 with GCP - Cloud Functions"},{"content":"Introduction GCP or Google Cloud Platform is quickly emerging as a major force in the public cloud space. It is at no.3 right now by marketshare(7%) behind AWS(32%) and Azure (19%)1. As per the 2020 Magic Quadrant report, it is very close to bridging the gap with the other 2 leaders2.\nIn this series, We will setup a free tier GCP account and explore some of the services by creating some basic projects.\nThe first step is to enroll yourself for the free GCP account (with $300 credit - 90 days). You do need a credit card to sign-up but Google assures, it won\u0026rsquo;t be charged until you enable the automatic billing in the console (a safety net of sorts?). You can use the free tier services without this turned on, however, there are some services which will ask you to enable this billing thing before proceeding.\nGCP Basics This https://cloud.google.com/docs/overview is a good starting point to undrstanding more about the layout of a typical GCP account and its different sections. If you are familier with any of the other clouds (AWS or Azure), these pages https://cloud.google.com/docs/compare/aws, https://cloud.google.com/docs/compare/azure can help you understand the corresponding GCP service offerings.\nFirst Exercise We will keep this fairly simple. It will involve\nSetting up a free account Configuring and understanding gcloud cli Enabling some googleapis to work with - like compute, iam etc. Setting up a new project Setting up permissions and service-accounts Creating a storage bucket with versioning Creating a new instance in the default network topology Cleanup - deleting all resources. gcloud is the cli to work with GCP. It can be run either directly on the browser or locally after installing Cloud SDK.\nGCP has an extra concept of \u0026ldquo;enabling/disabling\u0026rdquo; APIs. This is a prerequisite before you can use any service.\nSteps Now we will go through the steps to complete the aforementioned tasks. I am not including the tasks for creating a account, and adding your billing information here.\n1. Create a new project In GCP, resources are logically separated under projects. Think of it as a namespace. Resources in one project don\u0026rsquo;t have access to resources in other projects by default. There are ways to allow this, but that\u0026rsquo;s for a later post.\nRemember to assign a unique project-id\n1$ gcloud projects create tools-202011 2. Associate the new project to an active billing account 1$ gcloud alpha billing projects link tools-202011 --billing-account 01132C-383548-8543A6 3. Create a new service account This will be used by Terraform to provision resources. A separate service account ensures that it\u0026rsquo;s permissions can be tightly controlled.\nNote the service account email full string, will be needed in the later steps.\n1$ gcloud iam service-accounts create tools-service-account 2$ gcloud iam service-accounts list 3DISPLAY NAME EMAIL DISABLED 4tools-service-account@tools-202011.iam.gserviceaccount.com False 4. Create a key for the service account 1$ gcloud iam service-accounts keys create --iam-account tools-service-account@tools-202011.iam.gserviceaccount.com tools-service-account.json 5. Attach permissions to the service account Allow access on compute and storage googleapis by attaching existing roles.\n1$ gcloud projects add-iam-policy-binding tools-202011 --member serviceAccount:tools-service-account@tools-202011.iam.gserviceaccount.com --role roles/storage.admin 2 3$ gcloud projects add-iam-policy-binding tools-202011 --member serviceAccount:tools-service-account@tools-202011.iam.gserviceaccount.com --role roles/compute.admin 6. Enable necessary APIs This will need to be enabled before we can use these services and it\u0026rsquo;s APIs.\n1$ gcloud services enable cloudresourcemanager.googleapis.com 2$ gcloud services enable cloudbilling.googleapis.com 3$ gcloud services enable iam.googleapis.com 4$ gcloud services enable compute.googleapis.com 5$ gcloud services enable serviceusage.googleapis.com 7. Create Storage Bucket This will be used for terraform remote state, so enable versioning as well. For working with the storage buckets the cli to use is gsutil\n1$ gsutil mb -p tools-202011 gs://tools-202011 2$ gsutil versioning set on gs://tools-202011 8. Run terraform With all the prerequisites completed, we can now setup terraform to work with GCP.\nDownload the latest terraform installer from here, the basic terraform template from here\nNext, we need to provide credentials to terraform. Export the environment variable GOOGLE_APPLICATION_CREDENTIALS=tools-service-account.json. The json provided here is created in the step-4 above with the service account.\nNow we can run the terraform commands to launch our instance.\n1$ terraform init 2$ terraform apply Type Yes when prompted. The instance will be ready in a few mins. Next, validate the resources that are created so far.\n9. Validate We can validate what resources have been creating either from the console or gcloud cli. Use these command to verify the resources are created as expected -\nInstance and disk created\n1$ gcloud compute instances list --filter=\u0026#34;zone:us-central1-a\u0026#34; 2NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS 3tf-compute-1 us-central1-a f1-micro 10.128.0.21 104.154.265.8 RUNNING 4 5$ gcloud compute disks list --filter=\u0026#34;zone:us-central1-a\u0026#34; 6NAME LOCATION LOCATION_SCOPE SIZE_GB TYPE STATUS 7tf-compute-1 us-central1-a zone 10 pd-standard READY Storage bucket and terraform state file created\n1$ gsutil ls -r gs://tools-202011/* 2gs://tools-202011/terraform/: 3 4gs://tools-202011/terraform/day0-state/: 5gs://tools-202011/terraform/day0-state/default.tfstate Finally, let\u0026rsquo;s SSH to this new instance using gcloud console\n1$ gcloud compute ssh tf-compute-1 --zone us-central1-a Note: If gcloud can\u0026rsquo;t find a ssh-keypair in your home directory it will prompt you to create one first, and then connect to the instance using it.\nGCP handles the SSH keypairs a bit differently in the sense that, they are tied to a project. The keys are picked up from the instance metadata. You can inspect the available keys by requesting this from within an instance\n1$ curl http://metadata.google.internal/computeMetadata/v1/project/attributes/ssh-keys -H\u0026#34;Metadata-Flavor: Google\u0026#34; 2abiydv:ssh-rsa AAAAB3Nza......... 3...........abiydv@tf-compute-1:~$ 10. Cleanup Remove all the resources created so far.\n1$ terraform destroy --auto-approve Conclusion This was a simple exercise suitable to develop a basic understanding of GCP - creating a free tier account, understanding the layout, some key services like iam, storage, compute etc, using/configuring the cli - gcloud and finally, using terraform with GCP.\nIn the next few posts, we will build on this and also explore other services, use-cases etc. \u0026#x1f44d;\nNote:\u0026nbsp; Code mentioned above is here\u0026nbsp; References (4) Gcp Free Tier\u0026nbsp; Gcloud\u0026nbsp; Managing Gcp Projects With Terraform\u0026nbsp; Connecting to Instance\u0026nbsp; https://www.canalys.com/newsroom/worldwide-cloud-market-q320\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://cloud.google.com/gartner-cloud-infrastructure-as-a-service\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://abiydv.github.io/posts/day0-gcp/","summary":"Introduction GCP or Google Cloud Platform is quickly emerging as a major force in the public cloud space. It is at no.3 right now by marketshare(7%) behind AWS(32%) and Azure (19%)1. As per the 2020 Magic Quadrant report, it is very close to bridging the gap with the other 2 leaders2.\nIn this series, We will setup a free tier GCP account and explore some of the services by creating some basic projects.","title":"Day 0 with GCP (Google Cloud)"},{"content":"Introduction Fastly is a popular CDN based on the open-source Varnish. Since it supports VCL, a lot of custom \u0026ldquo;logic\u0026rdquo; to handle incoming requests can be added, right at the edge. This improves the user experience as well as frees up the overhead on origin servers.\nWe\u0026rsquo;ll implement a simple service which will return details about users accessing it. Details like public IP, user-agent, location etc. It can be extended to put out more information as needed.\nSolution The service will rely entirely on Fastly\u0026rsquo;s VCL and will not depend on the origin at all! This is possible, thanks to synthetic responses that can be served from Fastly.\nThe request flow will be like this (no requests are sent to the origin) - VCL Snippets To make this work, we just need 2 VCL snippets - recv and error.\nAdd the following code in recv snippet -\n1unset req.http.Cookie; 2 3if (req.url ~ \u0026#34;^/$\u0026#34; ){ 4 set req.http.synthetic_resp = \u0026#34;Public IP: \u0026#34; + req.http.Fastly-Client-IP; 5 error 620; 6} 7 8if (req.url ~ \u0026#34;^/more$\u0026#34; ){ 9 set req.http.synthetic_resp = \u0026#34;Public IP: \u0026#34; + req.http.Fastly-Client-IP + \u0026#34; | City: \u0026#34; + std.toupper(client.geo.city) + \u0026#34; | Country: \u0026#34; + client.geo.country_code + \u0026#34; | User-Agent: \u0026#34; + req.http.user-agent + \u0026#34; | FServer: \u0026#34; + server.hostname + \u0026#34; | FDC: \u0026#34; + server.datacenter; 10 error 620; 11} 12 13if (req.url ~ \u0026#34;^/ip$\u0026#34; ){ 14 set req.http.synthetic_resp = req.http.Fastly-Client-IP; 15 error 620; 16} 17 18set req.http.synthetic_resp = \u0026#34;Not Found!\u0026#34;; 19error 644; Add the following code in error snippet -\n1if (obj.status == 620) { 2 set obj.status = 200; 3 set obj.response = \u0026#34;OK\u0026#34;; 4 set obj.http.Content-Type = \u0026#34;text/html\u0026#34;; 5 set obj.http.cache-control = \u0026#34;private, no-store, no-cache, max-age=0\u0026#34;; 6 synthetic req.http.synthetic_resp; 7 return (deliver); 8} 9 10if (obj.status == 644) { 11 set obj.status = 404; 12 set obj.response = \u0026#34;Not Found\u0026#34;; 13 set obj.http.Content-Type = \u0026#34;text/html\u0026#34;; 14 set obj.http.cache-control = \u0026#34;private, no-store, no-cache, max-age=0\u0026#34;; 15 synthetic req.http.synthetic_resp; 16 return (deliver); 17} Both snippets and other service configurations are available here\nDeployment The service, configurations and vcls can be quickly deployed to Fastly using Terraform. The required code is here\nTests Once the service is ready, verify the responses either via curl or browser.\nCurl responses (masked some bits)\n1$ curl http://meta.dane-example.com.global.prod.fastly.net 2Public IP: 192.168.1.1 3 4$ curl http://meta.dane-example.com.global.prod.fastly.net/more 5Public IP: 192.168.1.1 | City: CITY | Country: GB | User-Agent: curl/7.73.0 | FServer: cache-lhr7341 | FDC: LHR 6 7$ curl http://meta.dane-example.com.global.prod.fastly.net/ip 8192.168.1.1 9 10$ curl http://meta.dane-example.com.global.prod.fastly.net/random 11Not Found! Conclusion This service can be nice value add, and also help non-technical folks gather all the information needed for whitelisting or debugging issues. It removes any reliance on 3rd party services which may or may not be hosted in secure environments. It is possible to further enhance this service to put out even more information, as needed.\nNote:\u0026nbsp; Code mentioned above is here\u0026nbsp; References (1) About Vcl Snippets\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/fastly-meta-service/","summary":"Introduction Fastly is a popular CDN based on the open-source Varnish. Since it supports VCL, a lot of custom \u0026ldquo;logic\u0026rdquo; to handle incoming requests can be added, right at the edge. This improves the user experience as well as frees up the overhead on origin servers.\nWe\u0026rsquo;ll implement a simple service which will return details about users accessing it. Details like public IP, user-agent, location etc. It can be extended to put out more information as needed.","title":"Fastly Meta Service"},{"content":"Introduction GitHub Actions are event-driven (new commits, pull requests, tags etc.). Based on the config, it runs a series of commands after a specified event has occurred. For example, every time someone creates a pull request for a repository, it can automatically run a set of tests, coverage and code quality checks.\nIt makes it so much easier to create a cohesive \u0026ldquo;GitOps\u0026rdquo; flow as you can now see everything in a single place - without creating too much custom plumbing.\nIn this example, we will enable it on a repo with sample python project and use it to run linting, tests and coverage on pull-requests. This is the repo we will use - https://github.com/abiydv/python-github-actions\nConfiguration To enable Github actions, we simply add this file under .github/workflows/main.yml\n1name: pr-checks 2 3on: 4 pull_request: 5 branches: [master] 6 7jobs: 8 checks: 9 runs-on: ubuntu-latest 10 steps: 11 - uses: actions/checkout@v2 12 - name: Set up Python 3.8 13 uses: actions/setup-python@v2 14 with: 15 python-version: 3.8 16 - name: install dependencies 17 run: | 18 python -m pip install --upgrade pip 19 pip install -r requirements.txt 20 - name: lint 21 run: | 22 flake8 src/*.py --count --select=E9,F63,F7,F82 --show-source --statistics 23 flake8 src/*.py --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics 24 - name: tests with pytest 25 run: | 26 pytest 27 - name: coverage 28 run: | 29 coverage run -m pytest 30 coverage report --fail-under=50 Some important sections of this file are -\nFilter events - This block filters the event(s) for which this action will execute. In this case, it will trigger whenever a PR is raised for the master branch.\n3on: 4 pull_request: 5 branches: [master] Docker Image - The docker image you run this action on.\n9runs-on: ubuntu-latest Code checkout - Checkout the code before running anything else\n10steps: 11- uses: actions/checkout@v2 Rest of the steps are the usual lint, pytests, coverage test commands etc. The only thing to take care of while adding such commands is to make sure they exit with a non-zero code incase of any failures.\nActions in Action A new PR is created on the master branch from the bug/coverage branch. This causes the Github action to trigger. While it is running, a yellow dot denotes the in-progress status.\nSince the action fails in this case (due to coverage being less than 50%), a red cross against the commit id denotes the failure. \u0026#x274c;\nTo see what exactly caused the check to fail, we can navigate to the actions, and then the failed step. This brings up the detailed logs screen to show the output.\nService Limits There are service and usage limits on Github actions for free tier accounts. Details here.\nConclusion While this example shows how to setup a sample CI/CD flow for a basic python application, the high level steps are pretty much the same for most languages.\nGithub actions makes it extremely convinient to run checks and builds right from the \u0026ldquo;codebase\u0026rdquo; with minimal configuration.\nOver the subsequent posts, we will explore more about how to extend this functionality.\nHope the post proves useful and helps you along on your Github Actions adoption journey \u0026#x1f44d;\nNote:\u0026nbsp; Code mentioned above is here\u0026nbsp; References (1) Introduction to Github Actions\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/python-github-actions/","summary":"Introduction GitHub Actions are event-driven (new commits, pull requests, tags etc.). Based on the config, it runs a series of commands after a specified event has occurred. For example, every time someone creates a pull request for a repository, it can automatically run a set of tests, coverage and code quality checks.\nIt makes it so much easier to create a cohesive \u0026ldquo;GitOps\u0026rdquo; flow as you can now see everything in a single place - without creating too much custom plumbing.","title":"Github Actions for Python"},{"content":"Introduction Cloud Manager, part of the Adobe Managed Services, is a continuous integration and continuous delivery (CI/CD) framework that lets organisations deploy code to their AEM environments. This is only applicable for organisations which use Adobe Managed Services to host their AEM installations with Adobe.\nThe pipeline runs through a standard set of steps as shown below. Output of each step is available via an API through Adobe I/O, which is like a central place to manage all Adobe APIs.\nProblem Short of creating an Adobe account, getting added to the AMS project and then keeping an eye on the notifications in the console - there is no other way of knowing what\u0026rsquo;s happening with a particular Cloud Manager pipeline, with out of the box solutions/integrations that Adobe provides.\nWe need to build something up if we want to send notifications to a larger group who don\u0026rsquo;t have or need to have a Adobe account.\nSolution The Cloud Manager API enables us to interact with it in a fully programmatic fashion. This allows for integration of the Cloud Manager Continuous Integration / Continuous Delivery pipeline with other systems. Using this we will setup a workflow such that every new step, pipeline execution will send out a notification on MS Teams and Email.\nAdobe I/O Cloud Manager Integration The starting point of this setup is the Adobe I/O console.\nTo complete these steps, you need to either the System Administrator role or be an assigned API Developer for your organization.\nAll requests to the Cloud Manager API are authenticated using an access token retrieved using JWT.\nTo create an API Integration:\nLogin to Adobe I/O console https://console.adobe.io/. Click the Create new project button. Click the Add to Project button on top left and select API from the drop-down menu. Under the Experience Cloud section select Cloud Manager and then click Next. Generate or Upload a key pair. Click either Generate keypair or Next. Select one of the Product Profiles to assign the integration to a specific Cloud Manager role. It will control the level of access this API will have on Cloud Manager. Click Save configured API. This will create your client and the page will list Client ID (sometimes referred to as an API Key), Organization ID, Technical Account ID and Client Secret. These values will be needed when we setup the AWS Lambda.\nThis completes the initial step on Adobe I/O, we will return to it once our webhook is ready\nWebhook - AWS API Gateway + AWS Lambda The Lambda and serverless configs can be cloned from this repo\nWe need to setup a few things before we can deploy our lambda.\nPrerequisites AWS CLI (preferably v2) Serverless framework Adobe I/O Cloudmanager project details MS Teams incoming webhook AWS Account with - Custom domain on Route 53 and ACM cert for the domain SES configured to send emails S3 bucket to use for serverless template Adobe I/O keys saved in AWS SSM Once the above are in-place, the subsequent steps are as follows -\nSetup Add the appropriate configuration properties in src/config.py\nCreate and activate virtual environment\n1python3 -m venv env 2source env/bin/activate Install dependencies\n1(env) pip3 install -r requirements.txt (Optional) Running lint, test, coverage reports\n1(env) pylint src/*.py --rcfile=setup.cfg 2(env) pytest 3(env) coverage run -m pytest 4(env) coverage html Export the credentials as environment variables. Either the access/secret keys or the aws cli profile\n1export AWS_PROFILE=mfa Deloy Deploy/Update the service to AWS 1sls deploy Once the deployment is completed, the custom domain API gateway will be ready to use. In case of this example, it\u0026rsquo;ll be https://example.com/abobeio\nNow we return back to Adobe I/O console to complete Event registration.\nAdobe I/O Cloud Manager Integration - contd. Login to Adobe I/O console https://console.adobe.io/.\nSelect the previously created project.\nClick the Add to Project button and select \u0026ldquo;Event\u0026rdquo; from the drop-down menu.\nUnder the Experience Cloud section select Cloud Manager and then click Next.\nSelect the events you want to receive on the webhook. Available options are -\nPipeline Execution Started/Ended Pipeline Execution Step Started/Waiting/Ended Next screen shows the JWT created earlier, Click Next.\nProvide the webhook URL - The API endpoint we created above https://example.com/adobeio\nClick Save configured events\nAdobe I/O will trigger a challenge request and if it returns 200, the status of the webhook will be changed to ACTIVE\nNote - If you cannot host a Public API in your environment due to security implication, consider using the Journaling API\nConclusion This is a simple integration with Cloud Manager API using Adobe IO which sends notifications to MS Teams and Email.\nIt can be further extended to cater to many other use cases as well, like -\nStarting the Cloud Manager CI/CD pipeline from an external non-git system. Cancelling an existing execution before triggering a new one. Executing additional steps before the standard Cloud Manager production deployment. Triggering additional activities after the a pipeline execution is complete or a specific step has been completed, like CDN cache invalidation, creating issue reports in bug tracking systems on pipeline failures, saving run stats to a monitoring time-series db like prometheus etc. Once the inital setup is ready, the possibilities are endless. \u0026#x1f44d;\nNote:\u0026nbsp; Code mentioned above is here\u0026nbsp; References (2) Api\u0026nbsp; Introduction\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/adobe-io-cloudmanager-api/","summary":"Introduction Cloud Manager, part of the Adobe Managed Services, is a continuous integration and continuous delivery (CI/CD) framework that lets organisations deploy code to their AEM environments. This is only applicable for organisations which use Adobe Managed Services to host their AEM installations with Adobe.\nThe pipeline runs through a standard set of steps as shown below. Output of each step is available via an API through Adobe I/O, which is like a central place to manage all Adobe APIs.","title":"Adobe IO and Cloud Manager API"},{"content":"Introduction Fastly is a popular CDN based on the open-source Varnish. Since it supports VCL, a lot of custom \u0026ldquo;logic\u0026rdquo; to handle incoming requests can be added, right at the edge. This improves the user experience as well as reduces the overhead on origin servers.\nLet\u0026rsquo;s take an example of redirects. Traditionally, they are applied on webservers and often involve not-so-straightforward syntax for complex rules (multiple checks for various headers, cookies, referrers, urls etc.). All of these are fairly straight forward to achieve in the VCL. Thanks to a lot of customization by Fastly on default VCL, you even have various functions OOTB for use. It makes redirects one of the easiest to offload to Fastly edge. This is what we will look at in this post.\nProblem As the rules begin to increase on edge, you need to make changes reliably and also need to test before pushing them off to the live environment. You can create multiple services and deploy changes first on a lower environment, test it and then promote to live. As for automating these tests, you can use anything - even a simple curl command (have covered some of it here)\nWhile curl maybe fine to test a few redirects, it\u0026rsquo;s not ideal if you want to run through hundreds of them, and inspect various different sections of the response. A lot of plumbing would be needed in shell to make it work. Ideally, we need programatic control and constructs.\nSolution Enter behave. It is a BDD1 framework which uses tests written in a natural language style, backed up by Python code.\nThe tests can easily be executed in any CI/CD pipeline as an initial step of deploying a live service. The overall flow can be as follows with AWS Codepipeline and Codebuild - Any commits on github triggers the AWS Codepipeline. First step in the pipeline deploys a test service with the same vcl. Second step runs the behave tests against this test service and destroy the temporary service afterwards. If the tests are successful, the pipeline then runs a tarraform plan against the live service. Waits for a manual approval. A member of the team can review the plan output and approve it. After approval, it deploys the new vcl to the live service. I will cover the Fastly CI/CD in detail in a separate post later.\nLet\u0026rsquo;s look at an example now.\nFeature Test Cases This defines the functionality that we need to implement and can be written by someone from the business, QA or PM. I am going to use the following structure and layout for this project.\n1+--tests/ 2 +--features/ 3 | +-- steps/ 4 | | +-- steps.py 5 | +-- redirect.feature Consider the first test. It has a feature redirect and within this, there are 2 scenarios. One, to test when actual users hit the site. Second, to test when some crawlers hit the site.\n1Feature: redirect 2 3 Scenario: 4 Given I am a human 5 And I visit https://fastly-bdd-example.com.global.prod.fastly.net/status/200 6 Then Response is redirect 7 And Response reason is REDIRECT 8 And Status code is 302 9 And Redirected url is https://fastly-bdd-example.com/gateway 10 11 Scenario: 12 Given I am a googlebot 13 And I visit https://fastly-bdd-example.com.global.prod.fastly.net/status/200 14 Then Status code is 200 15 And Response reason is OK VCL Now that we have the feature described above, we need to implement this using VCLs. We are going to use vcl snippets which are quicker to setup and use. These can be deployed by the CI/CD pipeline or if you are just testing, from the Fastly console as well.\nrecv vcl snippet\n1if (req.http.User-Agent ~ \u0026#34;googlebot\u0026#34;){ 2 return (lookup); 3} 4 5if (req.url ~ \u0026#34;^/status/200\u0026#34;) { 6 error 902; 7} error vcl snippet\n1if (obj.status == 902) { 2 set obj.status = 302; 3 set obj.response = \u0026#34;REDIRECT\u0026#34;; 4 set obj.http.Location = \u0026#34;https://\u0026#34; req.http.host \u0026#34;/gateway\u0026#34;; 5 return (deliver); 6} Step Files The backend code for running the test is implemented in the file steps.py. You can split this across different files as your tests grow.\n1from behave import * 2import requests 3 4@given(u\u0026#39;I am a {useragent}\u0026#39;) 5def step_impl(context,useragent): 6 if \u0026#34;bot\u0026#34; in useragent: 7 context.useragent = useragent 8 else: 9 context.useragent = \u0026#34;pybehave\u0026#34; 10 11@given(u\u0026#39;I visit {url}\u0026#39;) 12def step_impl(context,url): 13 headers = {\u0026#39;User-Agent\u0026#39;: context.useragent} 14 context.resp = requests.get(url, headers=headers, allow_redirects=False, verify=False ) 15 print(\u0026#34;response_headers :\u0026#34;+str(context.resp.headers)) 16 17@then(u\u0026#39;Response is redirect\u0026#39;) 18def step_impl(context): 19 assert context.resp.is_redirect 20 21@then(u\u0026#39;Response reason is {resp_reason}\u0026#39;) 22def step_impl(context,resp_reason): 23 print(\u0026#34;response_reason : \u0026#34;+context.resp.reason) 24 assert context.resp.reason == resp_reason 25 26@then(u\u0026#39;Status code is {resp_status}\u0026#39;) 27def step_impl(context,resp_status): 28 print(\u0026#34;response_status : \u0026#34;+str(context.resp.status_code)) 29 assert context.resp.status_code == int(resp_status) 30 31@then(u\u0026#39;Redirected url is {resp_url}\u0026#39;) 32def step_impl(context,resp_url): 33 print(\u0026#34;response_url : \u0026#34;+context.resp.headers[\u0026#39;Location\u0026#39;]) 34 assert context.resp.headers[\u0026#39;Location\u0026#39;] == resp_url Executing Tests Once you have the above files ready in your project, it is very simple to run the tests. You can use a lot of command line options as per your need. These are described in detail here\n1$ behave 2. 3. 41 feature passed, 0 failed, 0 skipped 51 scenario passed, 0 failed, 0 skipped 66 steps passed, 0 failed, 0 skipped, 0 undefined 7Took 0m0.051s To generate test reports in the junit format use the --junit command-line option. The so generated reports can then be used by any existing junit visualization frameworks you have, to create visualizations.\nConclusion Apart from the obvious benefits of BDD, this workflow ensures issues are caught early and the deployments are error free. A side benefit of deploying it via a CI/CD pipeline is that it also syntax checks the actual VCL snippets before they make it to the live service. Since Fastly\u0026rsquo;s vcl implementation is quite different from open source Varnish - it is near impossible to syntax check quickly using a varnish container.\nNote:\u0026nbsp; Code mentioned above is here\u0026nbsp; References (2) Behave\u0026nbsp; About Vcl Snippets\u0026nbsp; BDD or Behavior-driven development is an agile software development technique that encourages collaboration between developers, QA and non-technical or business participants in a software project\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://abiydv.github.io/posts/fastly-bdd/","summary":"Introduction Fastly is a popular CDN based on the open-source Varnish. Since it supports VCL, a lot of custom \u0026ldquo;logic\u0026rdquo; to handle incoming requests can be added, right at the edge. This improves the user experience as well as reduces the overhead on origin servers.\nLet\u0026rsquo;s take an example of redirects. Traditionally, they are applied on webservers and often involve not-so-straightforward syntax for complex rules (multiple checks for various headers, cookies, referrers, urls etc.","title":"Fastly BDD"},{"content":"Introduction One of the basic but crucial things to secure cloud account(s) is to enable MFA on your accounts. Apart from instance keys, leaked IAM credentials are also a huge contributer to the various breaches we keep hearing about.\nEnabling MFA for all your users by default is a great first step. This also means restircting access to services for non-MFA authenticated users. But this can cause some discomfort.\nProblem The issue arrises mainly for aws cli use. Due to the restricted policy, regular secret key \u0026amp; access key don\u0026rsquo;t work. A new temporary credential needs to be requested everytime you want to work with the aws cli.\nHow can we address this issue?\nSolution(s) Basics Assuming you already have a default named profile setup like so,\n1[default] 2aws_access_key_id=SOMETHING 3aws_secret_access_key=SOMEKEY 4region=us-east-1 You can use this command to get the temporary credentials for use\n1aws sts get-session-token --duration-seconds 900 \\ 2 --serial-number arn:aws:iam::aws_account_id:mfa/iam_user \\ 3 --profile=default --token-code=mfa_token Response\n1{ 2 \u0026#34;Credentials\u0026#34;: { 3 \u0026#34;SecretAccessKey\u0026#34;: \u0026#34;secret_key\u0026#34;, 4 \u0026#34;SessionToken\u0026#34;: \u0026#34;temporary_token\u0026#34;, 5 \u0026#34;Expiration\u0026#34;: \u0026#34;expiration_datetime\u0026#34;, 6 \u0026#34;AccessKeyId\u0026#34;: \u0026#34;access_key\u0026#34; 7 } 8} Now you can either export this as environment variable or create a new named profile to use aws cli. But that\u0026rsquo;s a lot of work, right? \u0026#x1f612;\nA Shortcut Fret not, we will reduce a few steps.\nThe above commands and steps can easily be packaged into a script. I have added such a script here but some groundwork is needed for initial setup.\nIdentify a backup directory like /home/users/iam_user/backup Install jq and append it\u0026rsquo;s location to PATH for the script to use Download the script under /tmp and update the variables as per your envrionment 1# This is your home directory on the local system 2home=\u0026#34;/home/users/iam_user\u0026#34; 3 4# temporary path the script will use 5base=\u0026#34;/tmp\u0026#34; 6 7# validity of the token, this is the max (36 hours) 8validity=\u0026#34;129600\u0026#34; 9 10# aws account id 11account=\u0026#34;123456890\u0026#34; 12 13# your iam user on aws account 14iam_user=\u0026#34;iam_user\u0026#34; Once done, for subsequent runs, only the following steps are needed.\nGenerate a new token by running the script and providing the MFA token as the first and only argument\n1sh login.sh 123456 Response\n1export AWS_PROFILE=mfa Run the above output (export AWS_PROFILE=mfa) on the terminal, and start using the new profile!\nI am feeling lazy! Switching to a directory, typing out the script name is still too much work! \u0026#x1f612;\nLet\u0026rsquo;s reduce some more steps.\nAdd this to your .bash_profile 1echo \u0026#34;export AWS_PROFILE=mfa\u0026#34; \u0026gt;\u0026gt; ~/.bash_profile Add an alias to your .bash_profile. Now you can run it from anywhere on the system 1echo \u0026#34;alias awsmfalogin=\u0026#39;sh /tmp/login.sh\u0026#39;\u0026#34; \u0026gt;\u0026gt; ~/.bash_profile Simply invoke the script as below, and start using the aws-cli 1awsmfalogin 123456 You are now authenticated and authorized in ONE command! \u0026#x1f44f;\nConclusion MFA is a necessary precaution to keep cloud accounts secure, but it also introduces some additional steps every few hours during the work day. Hopefully, this quick snippet will help reclaim some of that time \u0026#x1f44d;\nNote:\u0026nbsp; Code mentioned above is here\u0026nbsp; References (1) Authenticate Mfa Cli\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/aws-cli-mfa-login/","summary":"Introduction One of the basic but crucial things to secure cloud account(s) is to enable MFA on your accounts. Apart from instance keys, leaked IAM credentials are also a huge contributer to the various breaches we keep hearing about.\nEnabling MFA for all your users by default is a great first step. This also means restircting access to services for non-MFA authenticated users. But this can cause some discomfort.\nProblem The issue arrises mainly for aws cli use.","title":"AWS CLI MFA Login"},{"content":"Introduction Curl is an invaluable tool for anyone who needs to debug webapps fequently. Given it\u0026rsquo;s vast amount of options, it is easy to miss some of it\u0026rsquo;s simplest yet powerful uses.\nListing here some options that I use everyday. Use the topics below to navigate direct to what you are looking for.\nI\u0026rsquo;ll keep updating it as and when I discover something new! \u0026#x1f918;\nInspect Response headers The simplest way to do that is by using the -I option -\n1$ curl -I https://httpbin.org/ip 2 3HTTP/2 200 4date: Thu, 12 Nov 2019 19:08:13 GMT 5content-type: application/json 6content-length: 32 7server: gunicorn/19.9.0 8access-control-allow-origin: * 9access-control-allow-credentials: true The above works in most cases, but if your webapp/endpoint doesn\u0026rsquo;t support the HEAD method or behaves differently to it compared to the usual GET method, this will dissappoint you.\nTo inspect the headers with a GET method, following options can be helpful.\nSimplest option is to use -i (don\u0026rsquo;t confuse it with -I described above), this includes response headers with the response body\n1$ curl -D - https://httpbin.org/ip -s -o /dev/null 2HTTP/2 200 3date: Wed, 28 Aug 2024 13:24:44 GMT 4content-type: application/json 5content-length: 32 6server: gunicorn/19.9.0 7access-control-allow-origin: * 8access-control-allow-credentials: true 9 10{ 11 \u0026#34;origin\u0026#34;: \u0026#34;x.x.x.x\u0026#34; 12} A slightly more verbose way of doing the same thing is using the -D flag\n1$ curl -D - https://httpbin.org/ip -s -o /dev/null 2 3HTTP/2 200 4date: Thu, 12 Nov 2019 19:10:39 GMT 5content-type: application/json 6content-length: 32 7server: gunicorn/19.9.0 8access-control-allow-origin: * 9access-control-allow-credentials: true OK, this works \u0026#x1f44d; but there are a lot of options. What do they mean?\n-D dump the headers to a file. - dump it to stdin instead of a file. -s suppress the progress update (like the following), helps to keep the output clean. 1% Total % Received % Xferd Average Speed Time Time Time Current 2 Dload Upload Total Spent Left Speed 30 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0HTTP/2 200 -o send any output to file -o /dev/nulldiscard the response completely. Request headers Use -v to inspect request headers. Check the request headers on the lines starting with \u0026gt;\n1$ curl -v https://httpbin.org/ip -s -o /dev/null 2 3* Trying 3.211.1.78:443... 4* Connected to httpbin.org (3.211.1.78) port 443 (#0) 5* Using HTTP2, server supports multi-use 6* Connection state changed (HTTP/2 confirmed) 7* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0 8} [5 bytes data] 9* Using Stream ID: 1 (easy handle 0x1da3b7de150) 10} [5 bytes data] 11\u0026gt; GET /ip HTTP/2 12\u0026gt; Host: httpbin.org 13\u0026gt; user-agent: curl/7.73.0 14\u0026gt; accept: */* Redirects Redirects are implemented for a variety of reasons. Redirect responses return a 30X response code and a location header which tells the requestor where to go next. The following request gives us these specific fields while filtering out the rest.\n1$ curl https://httpbin.org/status/302 -w \u0026#34;%{response_code} %{redirect_url}\u0026#34; -s -o /dev/null 2302 https://httpbin.org/redirect/1 OK, this is nice \u0026#x1f44c; but what does -w do?\n-w is write-out option and takes a number of variables. These variables need to be enclosed in a %{ } block. I have used 2 here. response_code and redirect_url which are, sort of, self explanatory. Full list of supported variables is here. What if you want to inspect redirects for multiple pages? Easy, follow the linux philosophy and stack your commands. Add the urls in a list and run a one liner loop. Let\u0026rsquo;s also format the output a bit while we are at it.\nhttps://httpbin.org/status/301 https://httpbin.org/status/302 https://httpbin.org/status/307 1$ while read url; do curl $url -w \u0026#34; %{response_code} | $url --\u0026gt; %{redirect_url} \\n\u0026#34; -s -o /dev/null; done \u0026lt; urls.txt 2 3301 | https://httpbin.org/status/301 --\u0026gt; https://httpbin.org/redirect/1 4302 | https://httpbin.org/status/302 --\u0026gt; https://httpbin.org/redirect/1 5307 | https://httpbin.org/status/307 --\u0026gt; https://httpbin.org/redirect/1 Customize Requests Inspecting requests is not all that curl is capable of. You can customize the outgoing requests to your heart\u0026rsquo;s liking. Adding a few common scenarios below.\nAuthorization To pass authentication headers to an API, simply add the required header using -H\n1$ curl \u0026#34;https://httpbin.org/headers\u0026#34; -H \u0026#34;Authorization: bearer secrettoken\u0026#34; 2 3{ 4 \u0026#34;headers\u0026#34;: { 5 \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, 6 \u0026#34;Authorization\u0026#34;: \u0026#34;bearer secrettoken\u0026#34;, 7 \u0026#34;Host\u0026#34;: \u0026#34;httpbin.org\u0026#34;, 8 \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/8.7.1\u0026#34;, 9 \u0026#34;X-Amzn-Trace-Id\u0026#34;: \u0026#34;Root=1-66cf0a02-7f81812758215deb6aba0be3\u0026#34; 10 } 11} Note: You can also read the auth header from a file as well, see example below in Custom headers section.\nModify User-Agent By default, curl uses a user-agent request header like, user-agent: curl/7.73.0\nIf you need to send a custom user-agent, simply use the -A option. Check the User-Agent key value in the returned json response.\n1$ curl https://httpbin.org/headers -A \u0026#34;bob-the-builder\u0026#34; 2 3{ 4 \u0026#34;headers\u0026#34;: { 5 \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, 6 \u0026#34;Host\u0026#34;: \u0026#34;httpbin.org\u0026#34;, 7 \u0026#34;User-Agent\u0026#34;: \u0026#34;bob-the-builder\u0026#34;, 8 \u0026#34;X-Amzn-Trace-Id\u0026#34;: \u0026#34;Root=1-5fadb05b-490211b54febef30592912b5\u0026#34; 9 } 10} Custom headers To send custom headers with a request, you can use the -H option. Check the headers section in the returned json response.\n1$ curl https://httpbin.org/headers -H \u0026#34;CustomHeader1: value1\u0026#34; -H \u0026#34;CustomHeader2: value2\u0026#34; 2 3{ 4 \u0026#34;headers\u0026#34;: { 5 \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, 6 \u0026#34;Customheader1\u0026#34;: \u0026#34;value1\u0026#34;, 7 \u0026#34;Customheader2\u0026#34;: \u0026#34;value2\u0026#34;, 8 \u0026#34;Host\u0026#34;: \u0026#34;httpbin.org\u0026#34;, 9 \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/7.73.0\u0026#34;, 10 \u0026#34;X-Amzn-Trace-Id\u0026#34;: \u0026#34;Root=1-5fadb190-26af70e1691093c76d180401\u0026#34; 11 } 12} Headers can also be read from a file, like shown below. We pass the auth header and content type header from a file headers.txt\n1$ cat headers.txt 2Authorization: bearer secrettoken 3Content-Type: application/json 1$ curl \u0026#34;https://httpbin.org/headers\u0026#34; -H \u0026#34;@headers.txt\u0026#34; 2 3{ 4 \u0026#34;headers\u0026#34;: { 5 \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, 6 \u0026#34;Authorization\u0026#34;: \u0026#34;bearer secrettoken\u0026#34;, 7 \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, 8 \u0026#34;Host\u0026#34;: \u0026#34;httpbin.org\u0026#34;, 9 \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/8.7.1\u0026#34;, 10 \u0026#34;X-Amzn-Trace-Id\u0026#34;: \u0026#34;Root=1-66cf0b27-3e65ca6a3dc1e4db5f2008d8\u0026#34; 11 } 12} Cookies To send cookies with a request, you can use the -b option. Cookies can either be provided inline, or read from a file.\nCheck the cookies field in the json response below.\nSend the cookie values inline\n1$ curl https://httpbin.org/cookies -b \u0026#34;country=GB; language=EN; debug=1\u0026#34; 2 3{ 4 \u0026#34;cookies\u0026#34;: { 5 \u0026#34;country\u0026#34;: \u0026#34;GB\u0026#34;, 6 \u0026#34;debug\u0026#34;: \u0026#34;1\u0026#34;, 7 \u0026#34;language\u0026#34;: \u0026#34;EN\u0026#34; 8 } 9} Send the cookie values from a file\n1$ curl https://httpbin.org/cookies -b cookies.txt 2 3{ 4 \u0026#34;cookies\u0026#34;: { 5 \u0026#34;country\u0026#34;: \u0026#34;GB\u0026#34;, 6 \u0026#34;debug\u0026#34;: \u0026#34;2\u0026#34;, 7 \u0026#34;language\u0026#34;: \u0026#34;EN\u0026#34;, 8 \u0026#34;source\u0026#34;: \u0026#34;file\u0026#34; 9 } 10} Note The cookie.txt file needs to be in the following format for curl to read and parse the cookies\n1httpbin.org\tFALSE\t/\tFALSE\t0\tcountry GB 2httpbin.org\tFALSE\t/\tFALSE\t0\tlanguage EN 3httpbin.org\tFALSE\t/\tFALSE\t0\tsource file 4httpbin.org\tFALSE\t/\tFALSE\t0\tdebug 2 Reuse cookies To save cookies received in a response, you can use the -c option. Cookies can then be easily used in subsequent requests using the -b option described above.\n1$ curl https://httpbin.org/cookies/set/locale/en_GB -c cookies.txt 2 3\u0026lt;!DOCTYPE HTML PUBLIC \u0026#34;-//W3C//DTD HTML 3.2 Final//EN\u0026#34;\u0026gt; 4\u0026lt;title\u0026gt;Redirecting...\u0026lt;/title\u0026gt; 5\u0026lt;h1\u0026gt;Redirecting...\u0026lt;/h1\u0026gt; 6\u0026lt;p\u0026gt;You should be redirected automatically to target URL: \u0026lt;a href=\u0026#34;/cookies\u0026#34;\u0026gt;/cookies\u0026lt;/a\u0026gt;. If not click the link. Let\u0026rsquo;s inspect the file, if cookie is saved. The contents of the file -\n1# Netscape HTTP Cookie File 2# https://curl.haxx.se/docs/http-cookies.html 3# This file was generated by libcurl! Edit at your own risk. 4 5httpbin.org FALSE / FALSE 0 locale en_GB Chaining the above steps, we can make a request like this. -L causes curl to continue following the redirect.\n1$ curl https://httpbin.org/cookies/set/locale/en_GB -c cookies.txt -b cookies.txt -L 2 3{ 4 \u0026#34;cookies\u0026#34;: { 5 \u0026#34;locale\u0026#34;: \u0026#34;en_GB\u0026#34; 6 } 7} POST data The option to use in this case is -d. When you use this option, Content-Type: application/x-www-form-urlencoded request header is automatically set, so you don\u0026rsquo;t need to specify it explicitly. You can specify the data to be posted immediately after this flag. A sample request and response is added below. Check the response json\u0026rsquo;s form field.\n1$ curl \u0026#34;https://httpbin.org/post\u0026#34; -d \u0026#34;firstname=John\u0026amp;lastname=Doe\u0026#34; 2 3{ 4 \u0026#34;args\u0026#34;: {}, 5 \u0026#34;data\u0026#34;: \u0026#34;\u0026#34;, 6 \u0026#34;files\u0026#34;: {}, 7 \u0026#34;form\u0026#34;: { 8 \u0026#34;firstname\u0026#34;: \u0026#34;John\u0026#34;, 9 \u0026#34;lastname\u0026#34;: \u0026#34;Doe\u0026#34; 10 }, 11 \u0026#34;headers\u0026#34;: { 12 \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, 13 \u0026#34;Content-Length\u0026#34;: \u0026#34;27\u0026#34;, 14 \u0026#34;Content-Type\u0026#34;: \u0026#34;application/x-www-form-urlencoded\u0026#34;, 15 \u0026#34;Host\u0026#34;: \u0026#34;httpbin.org\u0026#34;, 16 \u0026#34;User-Agent\u0026#34;: \u0026#34;curl\u0026#34;, 17 \u0026#34;X-Amzn-Trace-Id\u0026#34;: \u0026#34;Root=1-60ec27f5-4d461f2d3bbf20ed16291ed3\u0026#34; 18 }, 19 \u0026#34;json\u0026#34;: null, 20 \u0026#34;url\u0026#34;: \u0026#34;https://httpbin.org/post\u0026#34; 21} You could also hide the data you are submitting from the command line or history by reading it off a file like so -\n1$ curl \u0026#34;https://httpbin.org/post\u0026#34; -d \u0026#34;@sensitive-data.txt\u0026#34; POST multifield form It\u0026rsquo;s quite straightforward to submit a multifield form using curl. The option to use is -F. You can add as many fields as required by adding multiple -F flags. When you use this option, the Content-Type: multipart/form-data request header is automatically set, so you don\u0026rsquo;t need to specify it explicitly.\nHere is one such request and response -\n1$ curl -F \u0026#34;firstname=John\u0026#34; -F \u0026#34;lastname=Doe\u0026#34; \u0026#34;https://httpbin.org/post\u0026#34; 2 3{ 4 \u0026#34;args\u0026#34;: {}, 5 \u0026#34;data\u0026#34;: \u0026#34;\u0026#34;, 6 \u0026#34;files\u0026#34;: {}, 7 \u0026#34;form\u0026#34;: { 8 \u0026#34;firstname\u0026#34;: \u0026#34;John\u0026#34;, 9 \u0026#34;lastname\u0026#34;: \u0026#34;Doe\u0026#34; 10 }, 11 \u0026#34;headers\u0026#34;: { 12 \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, 13 \u0026#34;Content-Length\u0026#34;: \u0026#34;248\u0026#34;, 14 \u0026#34;Content-Type\u0026#34;: \u0026#34;multipart/form-data; boundary=------------------------e41227aceee312a6\u0026#34;, 15 \u0026#34;Host\u0026#34;: \u0026#34;httpbin.org\u0026#34;, 16 \u0026#34;User-Agent\u0026#34;: \u0026#34;curl\u0026#34;, 17 \u0026#34;X-Amzn-Trace-Id\u0026#34;: \u0026#34;Root=1-60ec23d7-3a82be0162af35456010c778\u0026#34; 18 }, 19 \u0026#34;json\u0026#34;: null, 20 \u0026#34;url\u0026#34;: \u0026#34;https://httpbin.org/post\u0026#34; 21} File upload Similar to using the multifield form above, you can also upload a file using the -F flag. The only thing to be aware of is the filepath should be prefixed with a @ char. Check the field files in the response json.\n1$ curl -F \u0026#34;firstname=John\u0026#34; -F \u0026#34;lastname=Doe\u0026#34; \u0026#34;https://httpbin.org/post\u0026#34; -F \u0026#34;file=@file.txt\u0026#34; 2 3{ 4 \u0026#34;args\u0026#34;: {}, 5 \u0026#34;data\u0026#34;: \u0026#34;\u0026#34;, 6 \u0026#34;files\u0026#34;: { 7 \u0026#34;file\u0026#34;: \u0026#34;This is a test file\\n\u0026#34; 8 }, 9 \u0026#34;form\u0026#34;: { 10 \u0026#34;firstname\u0026#34;: \u0026#34;John\u0026#34;, 11 \u0026#34;lastname\u0026#34;: \u0026#34;Doe\u0026#34; 12 }, 13 \u0026#34;headers\u0026#34;: { 14 \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, 15 \u0026#34;Content-Length\u0026#34;: \u0026#34;408\u0026#34;, 16 \u0026#34;Content-Type\u0026#34;: \u0026#34;multipart/form-data; boundary=-----afd9220cdc8dae3d\u0026#34;, 17 \u0026#34;Host\u0026#34;: \u0026#34;httpbin.org\u0026#34;, 18 \u0026#34;User-Agent\u0026#34;: \u0026#34;curl\u0026#34;, 19 \u0026#34;X-Amzn-Trace-Id\u0026#34;: \u0026#34;Root=1-60ec24b5\u0026#34; 20 }, 21 \u0026#34;json\u0026#34;: null, 22 \u0026#34;url\u0026#34;: \u0026#34;https://httpbin.org/post\u0026#34; 23} Conclusion While this list is just a start, I hope it helps you use curl more effectively. As promised, I will try to keep this updated, so that over time, it may eventually become a lot more exhaustive. \u0026#x1f44d;\nReferences (2) Manpage\u0026nbsp; Ec.haxx.se\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/curl-cheatsheet/","summary":"Introduction Curl is an invaluable tool for anyone who needs to debug webapps fequently. Given it\u0026rsquo;s vast amount of options, it is easy to miss some of it\u0026rsquo;s simplest yet powerful uses.\nListing here some options that I use everyday. Use the topics below to navigate direct to what you are looking for.\nI\u0026rsquo;ll keep updating it as and when I discover something new! \u0026#x1f918;\nInspect Response headers The simplest way to do that is by using the -I option -","title":"Curl Cheatsheet"},{"content":"Introduction Elasticsearch is an open-source search solution which is quite popular for centralzed logs ingestion. It allows logs from various different sources to be available and searchable at a centralized location.\nIngesting data from various sources though, creates a problem. How do you normalize the incoming data, split it into some common fields, add or remove metadata etc? To perform all of this (and more), Logstash is the go-to solution. It can manipulate, transform incoming data before pushing it off to Elasticsearch for indexing.\nKibana is the tool of choice to search and visualize data indexed in Elasticsearch. These 3 together, are often referred as the ELK stack.\nProblem AWS now has a managed Elasticsearch offering which is a fantastic option for small teams with limited capacity to manage a self hosted Elasticsearch solution on EC2/ECS/EKS. There is a downside though. It only offers E_K from the ELK stack. Yes, there is no L (Logstash) out of the box. The only option is to self host a Logstash install - which kind of defeats the purpose of using a managed service.\nSolution Enter Elasticsearch ingest pipelines. These are not a complete replacement of Logstash, but they can do the data transformation part quite easily, if that\u0026rsquo;s the only thing you use Logstash for. Best part being, they run on the same cluster as Elasticsearch.\nIn my project, logs are being aggregated from a number of different sources - java app logs, httpd logs, systemd logs, external system logs etc. While most of these are ingested using filebeat agents, external logs arriving in S3 are ingested via Lambda.\nUsing ingest pipelines, I can split the data in various fields by applying different grok patterns but indexing them back in the same index. At a high level, this is what the whole setup looks like.\nIngest Pipeline(s) At its core, an ingest pipeline is a series of processors that are executed in order, to process/transform data. In this case, there are multiple ingest pipelines. The main pipeline accepts all incoming data, and based on some condition, will then invoke the sub-pipelines.\nThe some condition here is the value of the field logpattern. This will become more clear as we look at the configuration of each component in this whole setup.\nLet\u0026rsquo;s start with creating the pipelines first. Then we\u0026rsquo;ll look at the entrypoints - filebeat and lambda - which read the logs and forward it to the Elasticsearch cluster.\nMain Pipeline Configuration Broadly, all pipelines have the same structure - description and a bunch of processors. In the case of master_pipeline, I have used the following -\ndrop - To prevent the document from getting indexed on some condition. set - To add a new field, value can either be static or derived from other fields. remove - To delete a field from the document before it is indexed. pipeline - Trigger other ingest pipelines for further steps. 1{ 2 \u0026#34;description\u0026#34; : \u0026#34;main pipeline\u0026#34;, 3 \u0026#34;processors\u0026#34; : [ 4 { 5 \u0026#34;drop\u0026#34; : { 6 \u0026#34;if\u0026#34; : \u0026#34;ctx.message.toLowerCase().contains(\u0026#39;some unwanted data\u0026#39;)\u0026#34; 7 } 8 }, 9 { 10 \u0026#34;set\u0026#34; : { 11 \u0026#34;field\u0026#34; : \u0026#34;hostname\u0026#34;, 12 \u0026#34;value\u0026#34; : \u0026#34;{{host.name}}\u0026#34; 13 } 14 }, 15 { 16 \u0026#34;remove\u0026#34; : { 17 \u0026#34;field\u0026#34; : [ 18 \u0026#34;host.containerized\u0026#34;, 19 \u0026#34;host.architecture\u0026#34;, 20 \u0026#34;host.hostname\u0026#34;, 21 \u0026#34;host.name\u0026#34;, 22 \u0026#34;host.os.codename\u0026#34;, 23 ] 24 } 25 }, 26 { 27 \u0026#34;pipeline\u0026#34; : { 28 \u0026#34;if\u0026#34; : \u0026#34;ctx.logpattern == \u0026#39;java_log\u0026#39;\u0026#34;, 29 \u0026#34;name\u0026#34; : \u0026#34;java_log_pipeline\u0026#34; 30 } 31 }, 32 { 33 \u0026#34;pipeline\u0026#34; : { 34 \u0026#34;if\u0026#34; : \u0026#34;ctx.logpattern == \u0026#39;httpd_log\u0026#39;\u0026#34;, 35 \u0026#34;name\u0026#34; : \u0026#34;httpd_log_pipeline\u0026#34; 36 } 37 }, 38 { 39 \u0026#34;pipeline\u0026#34; : { 40 \u0026#34;if\u0026#34; : \u0026#34;ctx.logpattern == \u0026#39;system_log\u0026#39;\u0026#34;, 41 \u0026#34;name\u0026#34; : \u0026#34;system_log_pipeline\u0026#34; 42 } 43 } 44 { 45 \u0026#34;pipeline\u0026#34; : { 46 \u0026#34;if\u0026#34; : \u0026#34;ctx.logpattern == \u0026#39;external_log\u0026#39;\u0026#34;, 47 \u0026#34;name\u0026#34; : \u0026#34;external_log_pipeline\u0026#34; 48 } 49 } 50 ], 51 \u0026#34;on_failure\u0026#34; : [ 52 { 53 \u0026#34;set\u0026#34; : { 54 \u0026#34;field\u0026#34; : \u0026#34;Error\u0026#34;, 55 \u0026#34;value\u0026#34; : \u0026#34;{{_ingest.on_failure_message}}\u0026#34; 56 } 57 } 58 ] 59 } Other pipelines also have the same structure, but different processors. The most important one being the grok processor. It is responsible for splitting the log entry into sub-fields which can then be searched or aggregated. Following is an example for the httpd_logs_pipeline.\n1{ 2 \u0026#34;description\u0026#34; : \u0026#34;httpd logs\u0026#34;, 3 \u0026#34;processors\u0026#34; : [ 4 { 5 \u0026#34;gsub\u0026#34; : { 6 \u0026#34;field\u0026#34; : \u0026#34;message\u0026#34;, 7 \u0026#34;pattern\u0026#34; : \u0026#34;\\\u0026#34;\u0026#34;, 8 \u0026#34;replacement\u0026#34; : \u0026#34;\u0026#34; 9 } 10 }, 11 { 12 \u0026#34;grok\u0026#34; : { 13 \u0026#34;field\u0026#34; : \u0026#34;message\u0026#34;, 14 \u0026#34;patterns\u0026#34; : [ 15 \u0026#34;^%{IPV4:ipv4} - %{USER:username} %{HTTPDATE:datetime} %{PROG:method} %{URIPATHPARAM:request_uri} %{EMAILLOCALPART:http_version} %{NUMBER:http_status_code} %{PROG:pid} (%{NOTSPACE:request_by}|-) %{JAVALOGMESSAGE:useragent}$\u0026#34; 16 ] 17 } 18 }, 19 { 20 \u0026#34;set\u0026#34; : { 21 \u0026#34;field\u0026#34; : \u0026#34;details\u0026#34;, 22 \u0026#34;value\u0026#34; : \u0026#34;{{ipv4}} - {{username}} {{method}} {{request_uri}} {{http_version}} {{http_status_code}} {{pid}} {{request_by}} {{useragent}}\u0026#34; 23 } 24 }, 25 { 26 \u0026#34;date\u0026#34; : { 27 \u0026#34;field\u0026#34; : \u0026#34;datetime\u0026#34;, 28 \u0026#34;formats\u0026#34; : [ 29 \u0026#34;dd/MMM/yyyy:HH:mm:ss Z\u0026#34;, 30 \u0026#34;ISO8601\u0026#34; 31 ], 32 \u0026#34;timezone\u0026#34; : \u0026#34;Europe/London\u0026#34; 33 } 34 }, 35 { 36 \u0026#34;uppercase\u0026#34; : { 37 \u0026#34;field\u0026#34; : \u0026#34;severity\u0026#34;, 38 \u0026#34;on_failure\u0026#34; : [ 39 { 40 \u0026#34;set\u0026#34; : { 41 \u0026#34;field\u0026#34; : \u0026#34;severity\u0026#34;, 42 \u0026#34;value\u0026#34; : \u0026#34;INFO\u0026#34; 43 } 44 } 45 ] 46 } 47 }, 48 { 49 \u0026#34;remove\u0026#34; : { 50 \u0026#34;field\u0026#34; : \u0026#34;datetime\u0026#34; 51 } 52 } 53 ] 54} Elasticsearch Configuration Once ingest pipeline jsons are ready, it\u0026rsquo;s quite simple to add these to the Elasticsearch cluster. Following command adds a new pipeline\n1$ curl -X POST https://elasticsearch.example.com/_ingest/pipeline/pipeline_name -d pipeline_definition.json Following command diplays currently configured pipelines on the Elasticsearch cluster.\n1$ curl https://elasticsearch.example.com/_ingest/pipeline Filebeat Configuration The source of logs are modified next to use the configured ingest pipelines.\nShown here are only a subset of all the propeties that might be required. Notice the field logpattern under fields - this ensures the new field is appended to every log entry. Further down, in the Elasticsearch section, index name and the pipeline name are specified.\n1- type: log 2 enabled: true 3 paths: 4 - /opt/app/server/logs/stdout.log 5 fields_under_root: true 6 fields: 7 logpattern: \u0026#34;java_logs\u0026#34; 8 role: \u0026#34;java\u0026#34; 9 multiline.pattern: ^\\d{4}-\\d{1,2}-\\d{1,2} 10 multiline.negate: true 11 multiline.match: after 12# Elasticsearch 13output.elasticsearch: 14 hosts: [ \u0026#34;https://elasticsearch.example.com:443\u0026#34; ] 15 index: \u0026#34;logs-%{+yyyy.MM.dd}\u0026#34; 16 pipeline: main_pipeline Lambda Configuration Second input source is Lambda, which uses the Elasticsearch bulk API to index the logs pushed to S3. In this case, similar to filebeat, a new field logpattern is added to each record. After that, during indexing, a new querystring with the pipeline name is added.\nPOST /index/_bulk?pipeline=master_pipeline Conclusion That\u0026rsquo;s it!\nNow all incoming data gets transformed first by a combnation of ingest pipelines, before getting indexed in Elasticsearch. \u0026#x1f44f;\nReferences (5) Pipeline\u0026nbsp; Ingest Processors\u0026nbsp; Ingest Apis\u0026nbsp; Filebeat Reference Yml\u0026nbsp; Docs Bulk\u0026nbsp; ","permalink":"https://abiydv.github.io/posts/elasticsearch-ingest-pipelines/","summary":"Introduction Elasticsearch is an open-source search solution which is quite popular for centralzed logs ingestion. It allows logs from various different sources to be available and searchable at a centralized location.\nIngesting data from various sources though, creates a problem. How do you normalize the incoming data, split it into some common fields, add or remove metadata etc? To perform all of this (and more), Logstash is the go-to solution. It can manipulate, transform incoming data before pushing it off to Elasticsearch for indexing.","title":"Elasticsearch Ingest Pipelines"}]